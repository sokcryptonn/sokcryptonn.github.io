<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">

<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1"/>
  <meta name="generator" content="distill" />

  <style type="text/css">
  /* Hide doc at startup (prevent jankiness while JS renders/transforms) */
  body {
    visibility: hidden;
  }
  </style>

 <!--radix_placeholder_import_source-->
 <!--/radix_placeholder_import_source-->

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css" data-origin="pandoc">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  { color: #00769e; background-color: #f1f3f5; }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span { color: #00769e; } /* Normal */
code span.al { color: #ad0000; } /* Alert */
code span.an { color: #5e5e5e; } /* Annotation */
code span.at { color: #657422; } /* Attribute */
code span.bn { color: #ad0000; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #00769e; } /* ControlFlow */
code span.ch { color: #20794d; } /* Char */
code span.cn { color: #8f5902; } /* Constant */
code span.co { color: #5e5e5e; } /* Comment */
code span.cv { color: #5e5e5e; font-style: italic; } /* CommentVar */
code span.do { color: #5e5e5e; font-style: italic; } /* Documentation */
code span.dt { color: #ad0000; } /* DataType */
code span.dv { color: #ad0000; } /* DecVal */
code span.er { color: #ad0000; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #ad0000; } /* Float */
code span.fu { color: #4758ab; } /* Function */
code span.im { } /* Import */
code span.in { color: #5e5e5e; } /* Information */
code span.kw { color: #00769e; } /* Keyword */
code span.op { color: #5e5e5e; } /* Operator */
code span.ot { color: #00769e; } /* Other */
code span.pp { color: #ad0000; } /* Preprocessor */
code span.sc { color: #5e5e5e; } /* SpecialChar */
code span.ss { color: #20794d; } /* SpecialString */
code span.st { color: #20794d; } /* String */
code span.va { color: #111111; } /* Variable */
code span.vs { color: #20794d; } /* VerbatimString */
code span.wa { color: #5e5e5e; font-style: italic; } /* Warning */
</style>

<style>
  div.csl-bib-body { }
  div.csl-entry {
    clear: both;
    }
  .hanging div.csl-entry {
    margin-left:2em;
    text-indent:-2em;
  }
  div.csl-left-margin {
    min-width:2em;
    float:left;
  }
  div.csl-right-inline {
    margin-left:2em;
    padding-left:1em;
  }
  div.csl-indent {
    margin-left: 2em;
  }
</style>

  <!--radix_placeholder_meta_tags-->
  <title>SoK: Cryptographic Neural-Network Computation</title>



  <!--  https://schema.org/Article -->
  <meta property="article:published" itemprop="datePublished" content="2023-05-26"/>
  <meta property="article:created" itemprop="dateCreated" content="2023-05-26"/>
  <meta name="article:author" content="Lucien K. L. Ng"/>
  <meta name="article:author" content="Sherman S. M Chow"/>

  <!--  https://developers.facebook.com/docs/sharing/webmasters#markup -->
  <meta property="og:title" content="SoK: Cryptographic Neural-Network Computation"/>
  <meta property="og:type" content="article"/>
  <meta property="og:locale" content="en_US"/>
  <meta property="og:site_name" content="SoK: Cryptographic Neural-Network Computation"/>

  <!--  https://dev.twitter.com/cards/types/summary -->
  <meta property="twitter:card" content="summary"/>
  <meta property="twitter:title" content="SoK: Cryptographic Neural-Network Computation"/>

  <!--  https://scholar.google.com/intl/en/scholar/inclusion.html#indexing -->
  <meta name="citation_title" content="SoK: Cryptographic Neural-Network Computation"/>
  <meta name="citation_fulltext_html_url" content="https://doi.ieeecomputersociety.org/10.1109/SP46215.2023.00198"/>
  <meta name="citation_doi" content="10.1109/SP46215.2023.00198"/>
  <meta name="citation_conference_title" content="2023 IEEE Symposium on Security and Privacy (SP)"/>
  <meta name="citation_publisher" content="IEEE Computer Society"/>
  <meta name="citation_online_date" content="2023/05/26"/>
  <meta name="citation_publication_date" content="2023/05/26"/>
  <meta name="citation_author" content="Lucien K. L. Ng"/>
  <meta name="citation_author_institution" content="Georgia Institute of Technology"/>
  <meta name="citation_author" content="Sherman S. M Chow"/>
  <meta name="citation_author_institution" content="Chinese University of Hong Kong"/>
  <!--/radix_placeholder_meta_tags-->
  
  <meta name="citation_reference" content="citation_title=SoK: Security and privacy in machine learning;citation_author=Nicolas Papernot;citation_author=Patrick D. McDaniel;citation_author=Arunesh Sinha;citation_author=Michael P. Wellman"/>
  <meta name="citation_reference" content="citation_title=Deep learning on private data;citation_volume=17;citation_author=M. Sadegh Riazi;citation_author=Bita Darvish Rouhani;citation_author=Farinaz Koushanfar"/>
  <meta name="citation_reference" content="citation_title=Explaining and harnessing adversarial examples;citation_author=Ian J. Goodfellow;citation_author=Jonathon Shlens;citation_author=Christian Szegedy"/>
  <meta name="citation_reference" content="citation_title=Stealing machine learning models via prediction APIs;citation_author=Florian Tramèr;citation_author=Fan Zhang;citation_author=Ari Juels;citation_author=Michael K. Reiter;citation_author=Thomas Ristenpart"/>
  <meta name="citation_reference" content="citation_title=A privacy-preserving protocol for neural-network-based computation;citation_author=Mauro Barni;citation_author=Claudio Orlandi;citation_author=Alessandro Piva"/>
  <meta name="citation_reference" content="citation_title=Oblivious neural network computing via homomorphic encryption;citation_author=Claudio Orlandi;citation_author=Alessandro Piva;citation_author=Mauro Barni"/>
  <meta name="citation_reference" content="citation_title=How to generate and exchange secrets;citation_author=Andrew Chi-Chih Yao"/>
  <meta name="citation_reference" content="citation_title=Generalized universal circuits for secure evaluation of private functions with application to data classification;citation_author=Ahmad-Reza Sadeghi;citation_author=Thomas Schneider"/>
  <meta name="citation_reference" content="citation_title=How to play any mental game or A completeness theorem for protocols with honest majority;citation_author=Oded Goldreich;citation_author=Silvio Micali;citation_author=Avi Wigderson"/>
  <meta name="citation_reference" content="citation_title=Fully homomorphic encryption using ideal lattices;citation_author=Craig Gentry"/>
  <meta name="citation_reference" content="citation_title=CryptoNets: Applying neural networks to encrypted data with high throughput and accuracy;citation_author=Ran Gilad-Bachrach;citation_author=Nathan Dowlin;citation_author=Kim Laine;citation_author=Kristin E. Lauter;citation_author=Michael Naehrig;citation_author=John Wernsing"/>
  <meta name="citation_reference" content="citation_title=Oblivious neural network predictions via MiniONN transformations;citation_author=Jian Liu;citation_author=Mika Juuti;citation_author=Yao Lu;citation_author=N. Asokan"/>
  <meta name="citation_reference" content="citation_title=SecureML: A system for scalable privacy-preserving machine learning;citation_author=Payman Mohassel;citation_author=Yupeng Zhang"/>
  <meta name="citation_reference" content="citation_title=A survey on deep learning techniques for privacy-preserving;citation_author=Harry Chandra Tanuwidjaja;citation_author=Rakyong Choi;citation_author=Kwangjo Kim"/>
  <meta name="citation_reference" content="citation_title=SoK: Cryptography for neural networks;citation_author=Monir Azraoui;citation_author=Muhammad Barham;citation_author=Beyza Bozdemir;citation_author=Sébastien Canard;citation_author=Eleonora Ciceri;citation_author=Orhan Ermis;citation_author=Ramy Masalha;citation_author=Marco Mosconi;citation_author=Melek Önen;citation_author=Marie Paindavoine;citation_author=Boris Rozenberg;citation_author=Bastien Vialla;citation_author=Sauro Vicini"/>
  <meta name="citation_reference" content="citation_title=Membership inference attacks against machine learning models;citation_author=Reza Shokri;citation_author=Marco Stronati;citation_author=Congzheng Song;citation_author=Vitaly Shmatikov"/>
  <meta name="citation_reference" content="citation_title=Model inversion attacks that exploit confidence information &amp; basic countermeasures;citation_author=Matt Fredrikson;citation_author=Somesh Jha;citation_author=Thomas Ristenpart"/>
  <meta name="citation_reference" content="citation_title=Federated machine learning: Concept and applications;citation_volume=10;citation_author=Qiang Yang;citation_author=Yang Liu;citation_author=Tianjian Chen;citation_author=Yongxin Tong"/>
  <meta name="citation_reference" content="citation_title=Privacy in deep learning: A survey;citation_publisher=arXiv;citation_author=Fatemehsadat Mireshghallah;citation_author=Mohammadkazem Taram;citation_author=Praneeth Vepakomma;citation_author=Abhishek Singh;citation_author=Ramesh Raskar;citation_author=Hadi Esmaeilzadeh"/>
  <meta name="citation_reference" content="citation_title=Helen: Maliciously secure coopetitive learning for linear models;citation_author=Wenting Zheng;citation_author=Raluca Ada Popa;citation_author=Joseph E. Gonzalez;citation_author=Ion Stoica"/>
  <meta name="citation_reference" content="citation_title=CrypTFlow: Secure TensorFlow Inference;citation_author=Nishant Kumar;citation_author=Mayank Rathee;citation_author=Nishanth Chandran;citation_author=Divya Gupta;citation_author=Aseem Rastogi;citation_author=Rahul Sharma"/>
  <meta name="citation_reference" content="citation_title=SiRnn: A math library for secure RNN inference;citation_author=Deevashwer Rathee;citation_author=Mayank Rathee;citation_author=Rahul-Kranti-Kiran Goli;citation_author=Divya Gupta;citation_author=Rahul Sharma;citation_author=Nishanth Chandran;citation_author=Aseem Rastogi"/>
  <meta name="citation_reference" content="citation_title=CryptGPU: Fast privacy-preserving machine learning on the GPU;citation_author=Sijun Tan;citation_author=Brian Knott;citation_author=Yuan Tian;citation_author=David J. Wu"/>
  <meta name="citation_reference" content="citation_title=SoK: Privacy-preserving computation techniques for deep learning;citation_author=José Cabrero-Holgueras;citation_author=Sergio Pastrana"/>
  <meta name="citation_reference" content="citation_title=Batch normalization: Accelerating deep network training by reducing internal covariate shift;citation_author=Sergey Ioffe;citation_author=Christian Szegedy"/>
  <meta name="citation_reference" content="citation_title=Caffe: Convolutional architecture for fast feature embedding;citation_author=Yangqing Jia;citation_author=Evan Shelhamer;citation_author=Jeff Donahue;citation_author=Sergey Karayev;citation_author=Jonathan Long;citation_author=Ross B. Girshick;citation_author=Sergio Guadarrama;citation_author=Trevor Darrell"/>
  <meta name="citation_reference" content="citation_title=TensorFlow: A system for large-scale machine learning;citation_author=Martı́n Abadi;citation_author=Paul Barham;citation_author=Jianmin Chen;citation_author=Zhifeng Chen;citation_author=Andy Davis;citation_author=Jeffrey Dean;citation_author=Matthieu Devin;citation_author=Sanjay Ghemawat;citation_author=Geoffrey Irving;citation_author=Michael Isard;citation_author=Manjunath Kudlur;citation_author=Josh Levenberg;citation_author=Rajat Monga;citation_author=Sherry Moore;citation_author=Derek Gordon Murray;citation_author=Benoit Steiner;citation_author=Paul A. Tucker;citation_author=Vijay Vasudevan;citation_author=Pete Warden;citation_author=Martin Wicke;citation_author=Yuan Yu;citation_author=Xiaoqiang Zheng"/>
  <meta name="citation_reference" content="citation_title=Slalom: Fast, verifiable and private execution of neural networks in trusted hardware;citation_author=Florian Tramèr;citation_author=Dan Boneh"/>
  <meta name="citation_reference" content="citation_title=Goten: GPU-outsourcing trusted execution of neural network training;citation_author=Lucien K. L. Ng;citation_author=Sherman S. M. Chow;citation_author=Anna P. Y. Woo;citation_author=Donald P. H. Wong;citation_author=Yongjun Zhao"/>
  <meta name="citation_reference" content="citation_title=DeepSecure: Scalable provably-secure deep learning;citation_author=Bita Darvish Rouhani;citation_author=M. Sadegh Riazi;citation_author=Farinaz Koushanfar"/>
  <meta name="citation_reference" content="citation_title=Efficient multiparty protocols using circuit randomization;citation_author=Donald Beaver"/>
  <meta name="citation_reference" content="citation_title=ABY - A framework for efficient mixed-protocol secure two-party computation;citation_author=Daniel Demmler;citation_author=Thomas Schneider;citation_author=Michael Zohner"/>
  <meta name="citation_reference" content="citation_title=Extending oblivious transfers efficiently;citation_author=Yuval Ishai;citation_author=Joe Kilian;citation_author=Kobbi Nissim;citation_author=Erez Petrank"/>
  <meta name="citation_reference" content="citation_title=Fully homomorphic encryption without modulus switching from classical GapSVP;citation_doi=10.1007/978-3-642-32009-5_50;citation_author=Zvika Brakerski"/>
  <meta name="citation_reference" content="citation_title=Somewhat practical fully homomorphic encryption;citation_publisher=Cryptology ePrint, 2012/144;citation_author=Junfeng Fan;citation_author=Frederik Vercauteren"/>
  <meta name="citation_reference" content="citation_title=(Leveled) Fully homomorphic encryption without bootstrapping;citation_volume=6;citation_author=Zvika Brakerski;citation_author=Craig Gentry;citation_author=Vinod Vaikuntanathan"/>
  <meta name="citation_reference" content="citation_title=Homomorphic encryption for arithmetic of approximate numbers;citation_doi=10.1007/978-3-319-70694-8\_15;citation_author=Jung Hee Cheon;citation_author=Andrey Kim;citation_author=Miran Kim;citation_author=Yong Soo Song"/>
  <meta name="citation_reference" content="citation_title=Using homomorphic encryption for large scale statistical analysis;citation_publisher=Stanford;citation_author=David J. Wu;citation_author=Jacob Haven"/>
  <meta name="citation_reference" content="citation_title=TFHE: Fast fully homomorphic encryption over the torus;citation_volume=33;citation_author=Ilaria Chillotti;citation_author=Nicolas Gama;citation_author=Mariya Georgieva;citation_author=Malika Izabachène"/>
  <meta name="citation_reference" content="citation_title=Fast homomorphic evaluation of deep discretized neural networks;citation_author=Florian Bourse;citation_author=Michele Minelli;citation_author=Matthias Minihold;citation_author=Pascal Paillier"/>
  <meta name="citation_reference" content="citation_title=TAPAS: Tricks to accelerate (encrypted) prediction as a service;citation_author=Amartya Sanyal;citation_author=Matt J. Kusner;citation_author=Adrià Gascón;citation_author=Varun Kanade"/>
  <meta name="citation_reference" content="citation_title=SHE: A fast and accurate deep neural network for encrypted data;citation_author=Qian Lou;citation_author=Lei Jiang"/>
  <meta name="citation_reference" content="citation_title=CHIMERA: Combining ring-LWE-based fully homomorphic encryption schemes;citation_author=Christina Boura;citation_author=Nicolas Gama;citation_author=Mariya Georgieva;citation_author=Dimitar Jetchev"/>
  <meta name="citation_reference" content="citation_title=&lt;span style=&quot;font-variant:small-caps;&quot;&gt;XONN&lt;/span&gt;: &lt;span style=&quot;font-variant:small-caps;&quot;&gt;XNOR&lt;/span&gt;-based oblivious deep neural network inference;citation_author=M. Sadegh Riazi;citation_author=Mohammad Samragh;citation_author=Hao Chen;citation_author=Kim Laine;citation_author=Kristin E. Lauter;citation_author=Farinaz Koushanfar"/>
  <meta name="citation_reference" content="citation_title=Garbled neural networks are practical;citation_publisher=PPML;citation_author=Marshall Ball;citation_author=Brent Carmer;citation_author=Tal Malkin;citation_author=Mike Rosulek;citation_author=Nichole Schimanski"/>
  <meta name="citation_reference" content="citation_title=GAZELLE: A low latency framework for secure neural network inference;citation_author=Chiraag Juvekar;citation_author=Vinod Vaikuntanathan;citation_author=Anantha Chandrakasan"/>
  <meta name="citation_reference" content="citation_title=Delphi: A cryptographic inference service for neural networks;citation_author=Pratyush Mishra;citation_author=Ryan Lehmkuhl;citation_author=Akshayaram Srinivasan;citation_author=Wenting Zheng;citation_author=Raluca Ada Popa"/>
  <meta name="citation_reference" content="citation_title=GForce: GPU-friendly oblivious and rapid neural network inference;citation_author=Lucien K. L. Ng;citation_author=Sherman S. M. Chow"/>
  <meta name="citation_reference" content="citation_title=Faster CryptoNets: Leveraging sparsity for real-world encrypted inference;citation_publisher=arXiv:1811.09953;citation_author=Edward Chou;citation_author=Josh Beal;citation_author=Daniel Levy;citation_author=Serena Yeung;citation_author=Albert Haque;citation_author=Fei-Fei Li"/>
  <meta name="citation_reference" content="citation_title=QUOTIENT: Two-party secure neural network training and prediction;citation_author=Nitin Agrawal;citation_author=Ali Shahin Shamsabadi;citation_author=Matt J. Kusner;citation_author=Adrià Gascón"/>
  <meta name="citation_reference" content="citation_title=Glyph: Fast and accurately training deep neural networks on encrypted data;citation_author=Qian Lou;citation_author=Bo Feng;citation_author=Geoffrey Charles Fox;citation_author=Lei Jiang"/>
  <meta name="citation_reference" content="citation_title=SecureNN: 3-party secure computation for neural network training;citation_author=Sameer Wagh;citation_author=Divya Gupta;citation_author=Nishanth Chandran"/>
  <meta name="citation_reference" content="citation_title=FALCON: Honest-majority maliciously secure framework for private deep learning;citation_author=Sameer Wagh;citation_author=Shruti Tople;citation_author=Fabrice Benhamouda;citation_author=Eyal Kushilevitz;citation_author=Prateek Mittal;citation_author=Tal Rabin"/>
  <meta name="citation_reference" content="citation_title=ABY^3: A mixed protocol framework for machine learning;citation_author=Payman Mohassel;citation_author=Peter Rindal"/>
  <meta name="citation_reference" content="citation_title=Privacy-preserving machine learning as a service;citation_author=Ehsan Hesamifard;citation_author=Hassan Takabi;citation_author=Mehdi Ghasemi;citation_author=Rebecca N. Wright"/>
  <meta name="citation_reference" content="citation_title=Long short-term memory;citation_volume=9;citation_author=Sepp Hochreiter;citation_author=Jürgen Schmidhuber"/>
  <meta name="citation_reference" content="citation_title=On the properties of neural machine translation: Encoder-decoder approaches;citation_author=Kyunghyun Cho;citation_author=Bart Merrienboer;citation_author=Dzmitry Bahdanau;citation_author=Yoshua Bengio"/>
  <meta name="citation_reference" content="citation_title=CrypTen: Secure multi-party computation meets machine learning;citation_author=Brian Knott;citation_author=Shobha Venkataraman;citation_author=Awni Y. Hannun;citation_author=Shubho Sengupta;citation_author=Mark Ibrahim;citation_author=Laurens Maaten"/>
  <meta name="citation_reference" content="citation_title=Secure computation with fixed-point numbers;citation_author=Octavian Catrina;citation_author=Amitabh Saxena"/>
  <meta name="citation_reference" content="citation_title=nGraph-HE: A graph compiler for deep learning on homomorphically encrypted data;citation_author=Fabian Boemer;citation_author=Yixing Lao;citation_author=Rosario Cammarota;citation_author=Casimir Wierzynski"/>
  <meta name="citation_reference" content="citation_title=Low latency privacy preserving inference;citation_author=Alon Brutzkus;citation_author=Ran Gilad-Bachrach;citation_author=Oren Elisha"/>
  <meta name="citation_reference" content="citation_title=FALCON: A Fourier transform based approach for fast and secure convolutional neural network predictions;citation_author=Shaohua Li;citation_author=Kaiping Xue;citation_author=Bin Zhu;citation_author=Chenkai Ding;citation_author=Xindi Gao;citation_author=David S. L. Wei;citation_author=Tao Wan"/>
  <meta name="citation_reference" content="citation_title=Secure outsourced matrix computation and application to neural networks;citation_author=Xiaoqian Jiang;citation_author=Miran Kim;citation_author=Kristin E. Lauter;citation_author=Yongsoo Song"/>
  <meta name="citation_reference" content="citation_title=EzPC: Programmable and efficient secure two-party computation for machine learning;citation_author=Nishanth Chandran;citation_author=Divya Gupta;citation_author=Aseem Rastogi;citation_author=Rahul Sharma;citation_author=Shardul Tripathi"/>
  <meta name="citation_reference" content="citation_title=Chameleon: A hybrid secure computation framework for machine learning applications;citation_author=M. Sadegh Riazi;citation_author=Christian Weinert;citation_author=Oleksandr Tkachenko;citation_author=Ebrahim M. Songhori;citation_author=Thomas Schneider;citation_author=Farinaz Koushanfar"/>
  <meta name="citation_reference" content="citation_title=Protocols for secure remote database access with approximate matching;citation_author=Wenliang Du;citation_author=Mikhail J. Atallah"/>
  <meta name="citation_reference" content="citation_title=FLASH: Fast and robust framework for privacy-preserving machine learning;citation_author=Megha Byali;citation_author=Harsh Chaudhari;citation_author=Arpita Patra;citation_author=Ajith Suresh"/>
  <meta name="citation_reference" content="citation_title=BLAZE: Blazing fast privacy-preserving machine learning;citation_author=Arpita Patra;citation_author=Ajith Suresh"/>
  <meta name="citation_reference" content="citation_title=SWIFT: Super-fast and robust privacy-preserving machine learning;citation_author=Nishat Koti;citation_author=Mahak Pancholi;citation_author=Arpita Patra;citation_author=Ajith Suresh"/>
  <meta name="citation_reference" content="citation_title=Fantastic four: Honest-majority four-party secure computation with malicious security;citation_author=Anders P. K. Dalskov;citation_author=Daniel Escudero;citation_author=Marcel Keller"/>
  <meta name="citation_reference" content="citation_title=Trident: Efficient 4PC framework for privacy preserving machine learning;citation_author=Rahul Rachuri;citation_author=Ajith Suresh"/>
  <meta name="citation_reference" content="citation_title=Training and inference with integers in deep neural networks;citation_author=Shuang Wu;citation_author=Guoqi Li;citation_author=Feng Chen;citation_author=Luping Shi"/>
  <meta name="citation_reference" content="citation_title=Towards the AlexNet moment for homomorphic encryption: HCNN, the first homomorphic CNN on encrypted data with GPUs;citation_volume=9;citation_author=Ahmad Al Badawi;citation_author=Chao Jin;citation_author=Jie Lin;citation_author=Chan Fook Mun;citation_author=Sim Jun Jie;citation_author=Benjamin Hong Meng Tan;citation_author=Xiao Nan;citation_author=Khin Mi Mi Aung;citation_author=Vijay Ramaseshan Chandrasekhar"/>
  <meta name="citation_reference" content="citation_title=PlaidML-HE: Acceleration of deep learning kernels to compute on encrypted data;citation_author=Huili Chen;citation_author=Rosario Cammarota;citation_author=Felipe Valencia;citation_author=Francesco Regazzoni"/>
  <meta name="citation_reference" content="citation_title=NASS: Optimizing secure inference via neural architecture search;citation_author=Song Bian;citation_author=Weiwen Jiang;citation_author=Qing Lu;citation_author=Yiyu Shi;citation_author=Takashi Sato"/>
  <meta name="citation_reference" content="citation_title=SafeNet: A secure, accurate and fast neural network inference;citation_author=Qian Lou;citation_author=Yilin Shen;citation_author=Hongxia Jin;citation_author=Lei Jiang"/>
  <meta name="citation_reference" content="citation_title=CryptoNAS: Private inference on a ReLU budget;citation_author=Zahra Ghodsi;citation_author=Akshaj Kumar Veldanda;citation_author=Brandon Reagen;citation_author=Siddharth Garg"/>
  <meta name="citation_reference" content="citation_title=DeepReDuce: ReLU reduction for fast private inference;citation_author=Nandan Kumar Jha;citation_author=Zahra Ghodsi;citation_author=Siddharth Garg;citation_author=Brandon Reagen"/>
  <meta name="citation_reference" content="citation_title=On the efficacy of knowledge distillation;citation_author=Jang Hyun Cho;citation_author=Bharath Hariharan"/>
  <meta name="citation_reference" content="citation_title=nGraph-HE2: A high-throughput framework for neural network inference on encrypted data;citation_author=Fabian Boemer;citation_author=Anamaria Costache;citation_author=Rosario Cammarota;citation_author=Casimir Wierzynski"/>
  <meta name="citation_reference" content="citation_title=SEALion: A framework for neural network inference on encrypted data;citation_publisher=arXiv:1904.12840;citation_author=Tim Elsloo;citation_author=Giorgio Patrini;citation_author=Hamish Ivey-Law"/>
  <meta name="citation_reference" content="citation_title=CHET: An optimizing compiler for fully-homomorphic neural-network inferencing;citation_author=Roshan Dathathri;citation_author=Olli Saarikivi;citation_author=Hao Chen;citation_author=Kim Laine;citation_author=Kristin E. Lauter;citation_author=Saeed Maleki;citation_author=Madanlal Musuvathi;citation_author=Todd Mytkowicz"/>
  <meta name="citation_reference" content="citation_title=Muse: Secure inference resilient to malicious clients;citation_author=Ryan Lehmkuhl;citation_author=Pratyush Mishra;citation_author=Akshayaram Srinivasan;citation_author=Raluca Ada Popa"/>
  <meta name="citation_reference" content="citation_title=Realizing private and practical pharmacological collaboration;citation_volume=362;citation_author=Brian Hie;citation_author=Hyunghoon Cho;citation_author=Bonnie Berger"/>
  <meta name="citation_reference" content="citation_title=Two-party computation model for privacy-preserving queries over distributed databases;citation_author=Sherman S. M. Chow;citation_author=Jie-Han Lee;citation_author=Lakshminarayanan Subramanian"/>
  <meta name="citation_reference" content="citation_title=CrypTFlow2: Practical 2-party secure inference;citation_author=Deevashwer Rathee;citation_author=Mayank Rathee;citation_author=Nishant Kumar;citation_author=Nishanth Chandran;citation_author=Divya Gupta;citation_author=Aseem Rastogi;citation_author=Rahul Sharma"/>
  <meta name="citation_reference" content="citation_title=Very deep convolutional networks for large-scale image recognition;citation_author=Karen Simonyan;citation_author=Andrew Zisserman"/>
  <meta name="citation_reference" content="citation_title=Deep residual learning for image recognition;citation_author=Kaiming He;citation_author=Xiangyu Zhang;citation_author=Shaoqing Ren;citation_author=Jian Sun"/>
  <meta name="citation_reference" content="citation_title=Pushing the communication barrier in secure computation using lookup tables;citation_author=Ghada Dessouky;citation_author=Farinaz Koushanfar;citation_author=Ahmad-Reza Sadeghi;citation_author=Thomas Schneider;citation_author=Shaza Zeitouni;citation_author=Michael Zohner"/>
  <meta name="citation_reference" content="citation_title=Piranha: A GPU platform for secure computation;citation_author=Jean-Luc Watson;citation_author=Sameer Wagh;citation_author=Raluca Popa"/>
  <meta name="citation_reference" content="citation_title=Adam in private: Secure and fast training of deep neural networks with adaptive moment estimation;citation_author=Nuttapong Attrapadung;citation_author=Koki Hamada;citation_author=Dai Ikarashi;citation_author=Ryo Kikuchi;citation_author=Takahiro Matsuda;citation_author=Ibuki Mishina;citation_author=Hiraku Morita;citation_author=Jacob C. N. Schuldt"/>
  <meta name="citation_reference" content="citation_title=COINN: Crypto/ML codesign for oblivious inference via neural networks;citation_author=Siam Umar Hussain;citation_author=Mojan Javaheripi;citation_author=Mohammad Samragh;citation_author=Farinaz Koushanfar"/>
  <meta name="citation_reference" content="citation_title=Calibrating noise to sensitivity in private data analysis;citation_author=Cynthia Dwork;citation_author=Frank McSherry;citation_author=Kobbi Nissim;citation_author=Adam D. Smith"/>
  <meta name="citation_reference" content="citation_title=Sanitizing sentence embeddings (and labels) for local differential privacy;citation_author=Minxin Du;citation_author=Xiang Yue;citation_author=Sherman S. M. Chow;citation_author=Huan Sun"/>
  <meta name="citation_reference" content="citation_title=DP-Forward: Fine-tuning and inference on language models with differential privacy in forward pass;citation_author=Minxin Du;citation_author=Xiang Yue;citation_author=Sherman S. M. Chow;citation_author=Tianhao Wang;citation_author=Chenyu Huang;citation_author=Huan Sun"/>
  <meta name="citation_reference" content="citation_title=One hot garbling;citation_author=David Heath;citation_author=Vladimir Kolesnikov"/>
  <!--radix_placeholder_rmarkdown_metadata-->

  <script type="text/json" id="radix-rmarkdown-metadata">
  {"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["title","date","author","conference","doi","citation_url","slug","bibliography","output"]}},"value":[{"type":"character","attributes":{},"value":["SoK: Cryptographic Neural-Network Computation"]},{"type":"character","attributes":{},"value":["May 26, 2023"]},{"type":"list","attributes":{},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["first_name","last_name","url","affiliation","affiliation_url","orcid_id"]}},"value":[{"type":"character","attributes":{},"value":["Lucien K. L."]},{"type":"character","attributes":{},"value":["Ng"]},{"type":"character","attributes":{},"value":["https://lucieno.github.io/"]},{"type":"character","attributes":{},"value":["Georgia Institute of Technology"]},{"type":"character","attributes":{},"value":["https://www.gatech.edu/"]},{"type":"character","attributes":{},"value":["0000-0003-3662-3237"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["first_name","last_name","url","affiliation","affiliation_url","orcid_id"]}},"value":[{"type":"character","attributes":{},"value":["Sherman S. M"]},{"type":"character","attributes":{},"value":["Chow"]},{"type":"character","attributes":{},"value":["https://staff.ie.cuhk.edu.hk/~smchow/"]},{"type":"character","attributes":{},"value":["Chinese University of Hong Kong"]},{"type":"character","attributes":{},"value":["https://www.cuhk.edu.hk/"]},{"type":"character","attributes":{},"value":["0000-0001-7306-453X"]}]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["title","publisher"]}},"value":[{"type":"character","attributes":{},"value":["2023 IEEE Symposium on Security and Privacy (SP)"]},{"type":"character","attributes":{},"value":["IEEE Computer Society"]}]},{"type":"character","attributes":{},"value":["10.1109/SP46215.2023.00198"]},{"type":"character","attributes":{},"value":["https://doi.ieeecomputersociety.org/10.1109/SP46215.2023.00198"]},{"type":"character","attributes":{},"value":["sp/NgC23"]},{"type":"character","attributes":{},"value":["references.bib"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["distill::distill_article"]}},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["toc","toc_depth","css"]}},"value":[{"type":"logical","attributes":{},"value":[true]},{"type":"integer","attributes":{},"value":[2]},{"type":"character","attributes":{},"value":["style.css"]}]}]}]}
  </script>
  <!--/radix_placeholder_rmarkdown_metadata-->
  <!--radix_placeholder_navigation_in_header-->
  <meta name="distill:offset" content=""/>

  <script type="application/javascript">

    window.headroom_prevent_pin = false;

    window.document.addEventListener("DOMContentLoaded", function (event) {

      // initialize headroom for banner
      var header = $('header').get(0);
      var headerHeight = header.offsetHeight;
      var headroom = new Headroom(header, {
        tolerance: 5,
        onPin : function() {
          if (window.headroom_prevent_pin) {
            window.headroom_prevent_pin = false;
            headroom.unpin();
          }
        }
      });
      headroom.init();
      if(window.location.hash)
        headroom.unpin();
      $(header).addClass('headroom--transition');

      // offset scroll location for banner on hash change
      // (see: https://github.com/WickyNilliams/headroom.js/issues/38)
      window.addEventListener("hashchange", function(event) {
        window.scrollTo(0, window.pageYOffset - (headerHeight + 25));
      });

      // responsive menu
      $('.distill-site-header').each(function(i, val) {
        var topnav = $(this);
        var toggle = topnav.find('.nav-toggle');
        toggle.on('click', function() {
          topnav.toggleClass('responsive');
        });
      });

      // nav dropdowns
      $('.nav-dropbtn').click(function(e) {
        $(this).next('.nav-dropdown-content').toggleClass('nav-dropdown-active');
        $(this).parent().siblings('.nav-dropdown')
           .children('.nav-dropdown-content').removeClass('nav-dropdown-active');
      });
      $("body").click(function(e){
        $('.nav-dropdown-content').removeClass('nav-dropdown-active');
      });
      $(".nav-dropdown").click(function(e){
        e.stopPropagation();
      });
    });
  </script>

  <style type="text/css">

  /* Theme (user-documented overrideables for nav appearance) */

  .distill-site-nav {
    color: rgba(255, 255, 255, 0.8);
    background-color: #0F2E3D;
    font-size: 15px;
    font-weight: 300;
  }

  .distill-site-nav a {
    color: inherit;
    text-decoration: none;
  }

  .distill-site-nav a:hover {
    color: white;
  }

  @media print {
    .distill-site-nav {
      display: none;
    }
  }

  .distill-site-header {

  }

  .distill-site-footer {

  }


  /* Site Header */

  .distill-site-header {
    width: 100%;
    box-sizing: border-box;
    z-index: 3;
  }

  .distill-site-header .nav-left {
    display: inline-block;
    margin-left: 8px;
  }

  @media screen and (max-width: 768px) {
    .distill-site-header .nav-left {
      margin-left: 0;
    }
  }


  .distill-site-header .nav-right {
    float: right;
    margin-right: 8px;
  }

  .distill-site-header a,
  .distill-site-header .title {
    display: inline-block;
    text-align: center;
    padding: 14px 10px 14px 10px;
  }

  .distill-site-header .title {
    font-size: 18px;
    min-width: 150px;
  }

  .distill-site-header .logo {
    padding: 0;
  }

  .distill-site-header .logo img {
    display: none;
    max-height: 20px;
    width: auto;
    margin-bottom: -4px;
  }

  .distill-site-header .nav-image img {
    max-height: 18px;
    width: auto;
    display: inline-block;
    margin-bottom: -3px;
  }



  @media screen and (min-width: 1000px) {
    .distill-site-header .logo img {
      display: inline-block;
    }
    .distill-site-header .nav-left {
      margin-left: 20px;
    }
    .distill-site-header .nav-right {
      margin-right: 20px;
    }
    .distill-site-header .title {
      padding-left: 12px;
    }
  }


  .distill-site-header .nav-toggle {
    display: none;
  }

  .nav-dropdown {
    display: inline-block;
    position: relative;
  }

  .nav-dropdown .nav-dropbtn {
    border: none;
    outline: none;
    color: rgba(255, 255, 255, 0.8);
    padding: 16px 10px;
    background-color: transparent;
    font-family: inherit;
    font-size: inherit;
    font-weight: inherit;
    margin: 0;
    margin-top: 1px;
    z-index: 2;
  }

  .nav-dropdown-content {
    display: none;
    position: absolute;
    background-color: white;
    min-width: 200px;
    border: 1px solid rgba(0,0,0,0.15);
    border-radius: 4px;
    box-shadow: 0px 8px 16px 0px rgba(0,0,0,0.1);
    z-index: 1;
    margin-top: 2px;
    white-space: nowrap;
    padding-top: 4px;
    padding-bottom: 4px;
  }

  .nav-dropdown-content hr {
    margin-top: 4px;
    margin-bottom: 4px;
    border: none;
    border-bottom: 1px solid rgba(0, 0, 0, 0.1);
  }

  .nav-dropdown-active {
    display: block;
  }

  .nav-dropdown-content a, .nav-dropdown-content .nav-dropdown-header {
    color: black;
    padding: 6px 24px;
    text-decoration: none;
    display: block;
    text-align: left;
  }

  .nav-dropdown-content .nav-dropdown-header {
    display: block;
    padding: 5px 24px;
    padding-bottom: 0;
    text-transform: uppercase;
    font-size: 14px;
    color: #999999;
    white-space: nowrap;
  }

  .nav-dropdown:hover .nav-dropbtn {
    color: white;
  }

  .nav-dropdown-content a:hover {
    background-color: #ddd;
    color: black;
  }

  .nav-right .nav-dropdown-content {
    margin-left: -45%;
    right: 0;
  }

  @media screen and (max-width: 768px) {
    .distill-site-header a, .distill-site-header .nav-dropdown  {display: none;}
    .distill-site-header a.nav-toggle {
      float: right;
      display: block;
    }
    .distill-site-header .title {
      margin-left: 0;
    }
    .distill-site-header .nav-right {
      margin-right: 0;
    }
    .distill-site-header {
      overflow: hidden;
    }
    .nav-right .nav-dropdown-content {
      margin-left: 0;
    }
  }


  @media screen and (max-width: 768px) {
    .distill-site-header.responsive {position: relative; min-height: 500px; }
    .distill-site-header.responsive a.nav-toggle {
      position: absolute;
      right: 0;
      top: 0;
    }
    .distill-site-header.responsive a,
    .distill-site-header.responsive .nav-dropdown {
      display: block;
      text-align: left;
    }
    .distill-site-header.responsive .nav-left,
    .distill-site-header.responsive .nav-right {
      width: 100%;
    }
    .distill-site-header.responsive .nav-dropdown {float: none;}
    .distill-site-header.responsive .nav-dropdown-content {position: relative;}
    .distill-site-header.responsive .nav-dropdown .nav-dropbtn {
      display: block;
      width: 100%;
      text-align: left;
    }
  }

  /* Site Footer */

  .distill-site-footer {
    width: 100%;
    overflow: hidden;
    box-sizing: border-box;
    z-index: 3;
    margin-top: 30px;
    padding-top: 30px;
    padding-bottom: 30px;
    text-align: center;
  }

  /* Headroom */

  d-title {
    padding-top: 6rem;
  }

  @media print {
    d-title {
      padding-top: 4rem;
    }
  }

  .headroom {
    z-index: 1000;
    position: fixed;
    top: 0;
    left: 0;
    right: 0;
  }

  .headroom--transition {
    transition: all .4s ease-in-out;
  }

  .headroom--unpinned {
    top: -100px;
  }

  .headroom--pinned {
    top: 0;
  }

  /* adjust viewport for navbar height */
  /* helps vertically center bootstrap (non-distill) content */
  .min-vh-100 {
    min-height: calc(100vh - 100px) !important;
  }

  </style>

  <script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
  <link href="site_libs/font-awesome-6.4.0/css/all.min.css" rel="stylesheet"/>
  <link href="site_libs/font-awesome-6.4.0/css/v4-shims.min.css" rel="stylesheet"/>
  <script src="site_libs/headroom-0.9.4/headroom.min.js"></script>
  <script src="site_libs/autocomplete-0.37.1/autocomplete.min.js"></script>
  <script src="site_libs/fuse-6.4.1/fuse.min.js"></script>

  <script type="application/javascript">

  function getMeta(metaName) {
    var metas = document.getElementsByTagName('meta');
    for (let i = 0; i < metas.length; i++) {
      if (metas[i].getAttribute('name') === metaName) {
        return metas[i].getAttribute('content');
      }
    }
    return '';
  }

  function offsetURL(url) {
    var offset = getMeta('distill:offset');
    return offset ? offset + '/' + url : url;
  }

  function createFuseIndex() {

    // create fuse index
    var options = {
      keys: [
        { name: 'title', weight: 20 },
        { name: 'categories', weight: 15 },
        { name: 'description', weight: 10 },
        { name: 'contents', weight: 5 },
      ],
      ignoreLocation: true,
      threshold: 0
    };
    var fuse = new window.Fuse([], options);

    // fetch the main search.json
    return fetch(offsetURL('search.json'))
      .then(function(response) {
        if (response.status == 200) {
          return response.json().then(function(json) {
            // index main articles
            json.articles.forEach(function(article) {
              fuse.add(article);
            });
            // download collections and index their articles
            return Promise.all(json.collections.map(function(collection) {
              return fetch(offsetURL(collection)).then(function(response) {
                if (response.status === 200) {
                  return response.json().then(function(articles) {
                    articles.forEach(function(article) {
                      fuse.add(article);
                    });
                  })
                } else {
                  return Promise.reject(
                    new Error('Unexpected status from search index request: ' +
                              response.status)
                  );
                }
              });
            })).then(function() {
              return fuse;
            });
          });

        } else {
          return Promise.reject(
            new Error('Unexpected status from search index request: ' +
                        response.status)
          );
        }
      });
  }

  window.document.addEventListener("DOMContentLoaded", function (event) {

    // get search element (bail if we don't have one)
    var searchEl = window.document.getElementById('distill-search');
    if (!searchEl)
      return;

    createFuseIndex()
      .then(function(fuse) {

        // make search box visible
        searchEl.classList.remove('hidden');

        // initialize autocomplete
        var options = {
          autoselect: true,
          hint: false,
          minLength: 2,
        };
        window.autocomplete(searchEl, options, [{
          source: function(query, callback) {
            const searchOptions = {
              isCaseSensitive: false,
              shouldSort: true,
              minMatchCharLength: 2,
              limit: 10,
            };
            var results = fuse.search(query, searchOptions);
            callback(results
              .map(function(result) { return result.item; })
            );
          },
          templates: {
            suggestion: function(suggestion) {
              var img = suggestion.preview && Object.keys(suggestion.preview).length > 0
                ? `<img src="${offsetURL(suggestion.preview)}"</img>`
                : '';
              var html = `
                <div class="search-item">
                  <h3>${suggestion.title}</h3>
                  <div class="search-item-description">
                    ${suggestion.description || ''}
                  </div>
                  <div class="search-item-preview">
                    ${img}
                  </div>
                </div>
              `;
              return html;
            }
          }
        }]).on('autocomplete:selected', function(event, suggestion) {
          window.location.href = offsetURL(suggestion.path);
        });
        // remove inline display style on autocompleter (we want to
        // manage responsive display via css)
        $('.algolia-autocomplete').css("display", "");
      })
      .catch(function(error) {
        console.log(error);
      });

  });

  </script>

  <style type="text/css">

  .nav-search {
    font-size: x-small;
  }

  /* Algolioa Autocomplete */

  .algolia-autocomplete {
    display: inline-block;
    margin-left: 10px;
    vertical-align: sub;
    background-color: white;
    color: black;
    padding: 6px;
    padding-top: 8px;
    padding-bottom: 0;
    border-radius: 6px;
    border: 1px #0F2E3D solid;
    width: 180px;
  }


  @media screen and (max-width: 768px) {
    .distill-site-nav .algolia-autocomplete {
      display: none;
      visibility: hidden;
    }
    .distill-site-nav.responsive .algolia-autocomplete {
      display: inline-block;
      visibility: visible;
    }
    .distill-site-nav.responsive .algolia-autocomplete .aa-dropdown-menu {
      margin-left: 0;
      width: 400px;
      max-height: 400px;
    }
  }

  .algolia-autocomplete .aa-input, .algolia-autocomplete .aa-hint {
    width: 90%;
    outline: none;
    border: none;
  }

  .algolia-autocomplete .aa-hint {
    color: #999;
  }
  .algolia-autocomplete .aa-dropdown-menu {
    width: 550px;
    max-height: 70vh;
    overflow-x: visible;
    overflow-y: scroll;
    padding: 5px;
    margin-top: 3px;
    margin-left: -150px;
    background-color: #fff;
    border-radius: 5px;
    border: 1px solid #999;
    border-top: none;
  }

  .algolia-autocomplete .aa-dropdown-menu .aa-suggestion {
    cursor: pointer;
    padding: 5px 4px;
    border-bottom: 1px solid #eee;
  }

  .algolia-autocomplete .aa-dropdown-menu .aa-suggestion:last-of-type {
    border-bottom: none;
    margin-bottom: 2px;
  }

  .algolia-autocomplete .aa-dropdown-menu .aa-suggestion .search-item {
    overflow: hidden;
    font-size: 0.8em;
    line-height: 1.4em;
  }

  .algolia-autocomplete .aa-dropdown-menu .aa-suggestion .search-item h3 {
    font-size: 1rem;
    margin-block-start: 0;
    margin-block-end: 5px;
  }

  .algolia-autocomplete .aa-dropdown-menu .aa-suggestion .search-item-description {
    display: inline-block;
    overflow: hidden;
    height: 2.8em;
    width: 80%;
    margin-right: 4%;
  }

  .algolia-autocomplete .aa-dropdown-menu .aa-suggestion .search-item-preview {
    display: inline-block;
    width: 15%;
  }

  .algolia-autocomplete .aa-dropdown-menu .aa-suggestion .search-item-preview img {
    height: 3em;
    width: auto;
    display: none;
  }

  .algolia-autocomplete .aa-dropdown-menu .aa-suggestion .search-item-preview img[src] {
    display: initial;
  }

  .algolia-autocomplete .aa-dropdown-menu .aa-suggestion.aa-cursor {
    background-color: #eee;
  }
  .algolia-autocomplete .aa-dropdown-menu .aa-suggestion em {
    font-weight: bold;
    font-style: normal;
  }

  </style>


  <!--/radix_placeholder_navigation_in_header-->
  <!--radix_placeholder_distill-->

  <style type="text/css">

  body {
    background-color: white;
  }

  .pandoc-table {
    width: 100%;
  }

  .pandoc-table>caption {
    margin-bottom: 10px;
  }

  .pandoc-table th:not([align]) {
    text-align: left;
  }

  .pagedtable-footer {
    font-size: 15px;
  }

  d-byline .byline {
    grid-template-columns: 2fr 2fr;
  }

  d-byline .byline h3 {
    margin-block-start: 1.5em;
  }

  d-byline .byline .authors-affiliations h3 {
    margin-block-start: 0.5em;
  }

  .authors-affiliations .orcid-id {
    width: 16px;
    height:16px;
    margin-left: 4px;
    margin-right: 4px;
    vertical-align: middle;
    padding-bottom: 2px;
  }

  d-title .dt-tags {
    margin-top: 1em;
    grid-column: text;
  }

  .dt-tags .dt-tag {
    text-decoration: none;
    display: inline-block;
    color: rgba(0,0,0,0.6);
    padding: 0em 0.4em;
    margin-right: 0.5em;
    margin-bottom: 0.4em;
    font-size: 70%;
    border: 1px solid rgba(0,0,0,0.2);
    border-radius: 3px;
    text-transform: uppercase;
    font-weight: 500;
  }

  d-article table.gt_table td,
  d-article table.gt_table th {
    border-bottom: none;
    font-size: 100%;
  }

  .html-widget {
    margin-bottom: 2.0em;
  }

  .l-screen-inset {
    padding-right: 16px;
  }

  .l-screen .caption {
    margin-left: 10px;
  }

  .shaded {
    background: rgb(247, 247, 247);
    padding-top: 20px;
    padding-bottom: 20px;
    border-top: 1px solid rgba(0, 0, 0, 0.1);
    border-bottom: 1px solid rgba(0, 0, 0, 0.1);
  }

  .shaded .html-widget {
    margin-bottom: 0;
    border: 1px solid rgba(0, 0, 0, 0.1);
  }

  .shaded .shaded-content {
    background: white;
  }

  .text-output {
    margin-top: 0;
    line-height: 1.5em;
  }

  .hidden {
    display: none !important;
  }

  d-article {
    padding-top: 2.5rem;
    padding-bottom: 30px;
  }

  d-appendix {
    padding-top: 30px;
  }

  d-article>p>img {
    width: 100%;
  }

  d-article h2 {
    margin: 1rem 0 1.5rem 0;
  }

  d-article h3 {
    margin-top: 1.5rem;
  }

  d-article iframe {
    border: 1px solid rgba(0, 0, 0, 0.1);
    margin-bottom: 2.0em;
    width: 100%;
  }

  /* Tweak code blocks */

  d-article div.sourceCode code,
  d-article pre code {
    font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
  }

  d-article pre,
  d-article div.sourceCode,
  d-article div.sourceCode pre {
    overflow: auto;
  }

  d-article div.sourceCode {
    background-color: white;
  }

  d-article div.sourceCode pre {
    padding-left: 10px;
    font-size: 12px;
    border-left: 2px solid rgba(0,0,0,0.1);
  }

  d-article pre {
    font-size: 12px;
    color: black;
    background: none;
    margin-top: 0;
    text-align: left;
    white-space: pre;
    word-spacing: normal;
    word-break: normal;
    word-wrap: normal;
    line-height: 1.5;

    -moz-tab-size: 4;
    -o-tab-size: 4;
    tab-size: 4;

    -webkit-hyphens: none;
    -moz-hyphens: none;
    -ms-hyphens: none;
    hyphens: none;
  }

  d-article pre a {
    border-bottom: none;
  }

  d-article pre a:hover {
    border-bottom: none;
    text-decoration: underline;
  }

  d-article details {
    grid-column: text;
    margin-bottom: 0.8em;
  }

  @media(min-width: 768px) {

  d-article pre,
  d-article div.sourceCode,
  d-article div.sourceCode pre {
    overflow: visible !important;
  }

  d-article div.sourceCode pre {
    padding-left: 18px;
    font-size: 14px;
  }

  d-article pre {
    font-size: 14px;
  }

  }

  figure img.external {
    background: white;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
    padding: 18px;
    box-sizing: border-box;
  }

  /* CSS for d-contents */

  .d-contents {
    grid-column: text;
    color: rgba(0,0,0,0.8);
    font-size: 0.9em;
    padding-bottom: 1em;
    margin-bottom: 1em;
    padding-bottom: 0.5em;
    margin-bottom: 1em;
    padding-left: 0.25em;
    justify-self: start;
  }

  @media(min-width: 1000px) {
    .d-contents.d-contents-float {
      height: 0;
      grid-column-start: 1;
      grid-column-end: 4;
      justify-self: center;
      padding-right: 3em;
      padding-left: 2em;
    }
  }

  .d-contents nav h3 {
    font-size: 18px;
    margin-top: 0;
    margin-bottom: 1em;
  }

  .d-contents li {
    list-style-type: none
  }

  .d-contents nav > ul {
    padding-left: 0;
  }

  .d-contents ul {
    padding-left: 1em
  }

  .d-contents nav ul li {
    margin-top: 0.6em;
    margin-bottom: 0.2em;
  }

  .d-contents nav a {
    font-size: 13px;
    border-bottom: none;
    text-decoration: none
    color: rgba(0, 0, 0, 0.8);
  }

  .d-contents nav a:hover {
    text-decoration: underline solid rgba(0, 0, 0, 0.6)
  }

  .d-contents nav > ul > li > a {
    font-weight: 600;
  }

  .d-contents nav > ul > li > ul {
    font-weight: inherit;
  }

  .d-contents nav > ul > li > ul > li {
    margin-top: 0.2em;
  }


  .d-contents nav ul {
    margin-top: 0;
    margin-bottom: 0.25em;
  }

  .d-article-with-toc h2:nth-child(2) {
    margin-top: 0;
  }


  /* Figure */

  .figure {
    position: relative;
    margin-bottom: 2.5em;
    margin-top: 1.5em;
  }

  .figure .caption {
    color: rgba(0, 0, 0, 0.6);
    font-size: 12px;
    line-height: 1.5em;
  }

  .figure img.external {
    background: white;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
    padding: 18px;
    box-sizing: border-box;
  }

  .figure .caption a {
    color: rgba(0, 0, 0, 0.6);
  }

  .figure .caption b,
  .figure .caption strong, {
    font-weight: 600;
    color: rgba(0, 0, 0, 1.0);
  }

  /* Citations */

  d-article .citation {
    color: inherit;
    cursor: inherit;
  }

  div.hanging-indent{
    margin-left: 1em; text-indent: -1em;
  }

  /* Citation hover box */

  .tippy-box[data-theme~=light-border] {
    background-color: rgba(250, 250, 250, 0.95);
  }

  .tippy-content > p {
    margin-bottom: 0;
    padding: 2px;
  }


  /* Tweak 1000px media break to show more text */

  @media(min-width: 1000px) {
    .base-grid,
    distill-header,
    d-title,
    d-abstract,
    d-article,
    d-appendix,
    distill-appendix,
    d-byline,
    d-footnote-list,
    d-citation-list,
    distill-footer {
      grid-template-columns: [screen-start] 1fr [page-start kicker-start] 80px [middle-start] 50px [text-start kicker-end] 65px 65px 65px 65px 65px 65px 65px 65px [text-end gutter-start] 65px [middle-end] 65px [page-end gutter-end] 1fr [screen-end];
      grid-column-gap: 16px;
    }

    .grid {
      grid-column-gap: 16px;
    }

    d-article {
      font-size: 1.06rem;
      line-height: 1.7em;
    }
    figure .caption, .figure .caption, figure figcaption {
      font-size: 13px;
    }
  }

  @media(min-width: 1180px) {
    .base-grid,
    distill-header,
    d-title,
    d-abstract,
    d-article,
    d-appendix,
    distill-appendix,
    d-byline,
    d-footnote-list,
    d-citation-list,
    distill-footer {
      grid-template-columns: [screen-start] 1fr [page-start kicker-start] 60px [middle-start] 60px [text-start kicker-end] 60px 60px 60px 60px 60px 60px 60px 60px [text-end gutter-start] 60px [middle-end] 60px [page-end gutter-end] 1fr [screen-end];
      grid-column-gap: 32px;
    }

    .grid {
      grid-column-gap: 32px;
    }
  }


  /* Get the citation styles for the appendix (not auto-injected on render since
     we do our own rendering of the citation appendix) */

  d-appendix .citation-appendix,
  .d-appendix .citation-appendix {
    font-size: 11px;
    line-height: 15px;
    border-left: 1px solid rgba(0, 0, 0, 0.1);
    padding-left: 18px;
    border: 1px solid rgba(0,0,0,0.1);
    background: rgba(0, 0, 0, 0.02);
    padding: 10px 18px;
    border-radius: 3px;
    color: rgba(150, 150, 150, 1);
    overflow: hidden;
    margin-top: -12px;
    white-space: pre-wrap;
    word-wrap: break-word;
  }

  /* Include appendix styles here so they can be overridden */

  d-appendix {
    contain: layout style;
    font-size: 0.8em;
    line-height: 1.7em;
    margin-top: 60px;
    margin-bottom: 0;
    border-top: 1px solid rgba(0, 0, 0, 0.1);
    color: rgba(0,0,0,0.5);
    padding-top: 60px;
    padding-bottom: 48px;
  }

  d-appendix h3 {
    grid-column: page-start / text-start;
    font-size: 15px;
    font-weight: 500;
    margin-top: 1em;
    margin-bottom: 0;
    color: rgba(0,0,0,0.65);
  }

  d-appendix h3 + * {
    margin-top: 1em;
  }

  d-appendix ol {
    padding: 0 0 0 15px;
  }

  @media (min-width: 768px) {
    d-appendix ol {
      padding: 0 0 0 30px;
      margin-left: -30px;
    }
  }

  d-appendix li {
    margin-bottom: 1em;
  }

  d-appendix a {
    color: rgba(0, 0, 0, 0.6);
  }

  d-appendix > * {
    grid-column: text;
  }

  d-appendix > d-footnote-list,
  d-appendix > d-citation-list,
  d-appendix > distill-appendix {
    grid-column: screen;
  }

  /* Include footnote styles here so they can be overridden */

  d-footnote-list {
    contain: layout style;
  }

  d-footnote-list > * {
    grid-column: text;
  }

  d-footnote-list a.footnote-backlink {
    color: rgba(0,0,0,0.3);
    padding-left: 0.5em;
  }



  /* Anchor.js */

  .anchorjs-link {
    /*transition: all .25s linear; */
    text-decoration: none;
    border-bottom: none;
  }
  *:hover > .anchorjs-link {
    margin-left: -1.125em !important;
    text-decoration: none;
    border-bottom: none;
  }

  /* Social footer */

  .social_footer {
    margin-top: 30px;
    margin-bottom: 0;
    color: rgba(0,0,0,0.67);
  }

  .disqus-comments {
    margin-right: 30px;
  }

  .disqus-comment-count {
    border-bottom: 1px solid rgba(0, 0, 0, 0.4);
    cursor: pointer;
  }

  #disqus_thread {
    margin-top: 30px;
  }

  .article-sharing a {
    border-bottom: none;
    margin-right: 8px;
  }

  .article-sharing a:hover {
    border-bottom: none;
  }

  .sidebar-section.subscribe {
    font-size: 12px;
    line-height: 1.6em;
  }

  .subscribe p {
    margin-bottom: 0.5em;
  }


  .article-footer .subscribe {
    font-size: 15px;
    margin-top: 45px;
  }


  .sidebar-section.custom {
    font-size: 12px;
    line-height: 1.6em;
  }

  .custom p {
    margin-bottom: 0.5em;
  }

  /* Styles for listing layout (hide title) */
  .layout-listing d-title, .layout-listing .d-title {
    display: none;
  }

  /* Styles for posts lists (not auto-injected) */


  .posts-with-sidebar {
    padding-left: 45px;
    padding-right: 45px;
  }

  .posts-list .description h2,
  .posts-list .description p {
    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
  }

  .posts-list .description h2 {
    font-weight: 700;
    border-bottom: none;
    padding-bottom: 0;
  }

  .posts-list h2.post-tag {
    border-bottom: 1px solid rgba(0, 0, 0, 0.2);
    padding-bottom: 12px;
  }
  .posts-list {
    margin-top: 60px;
    margin-bottom: 24px;
  }

  .posts-list .post-preview {
    text-decoration: none;
    overflow: hidden;
    display: block;
    border-bottom: 1px solid rgba(0, 0, 0, 0.1);
    padding: 24px 0;
  }

  .post-preview-last {
    border-bottom: none !important;
  }

  .posts-list .posts-list-caption {
    grid-column: screen;
    font-weight: 400;
  }

  .posts-list .post-preview h2 {
    margin: 0 0 6px 0;
    line-height: 1.2em;
    font-style: normal;
    font-size: 24px;
  }

  .posts-list .post-preview p {
    margin: 0 0 12px 0;
    line-height: 1.4em;
    font-size: 16px;
  }

  .posts-list .post-preview .thumbnail {
    box-sizing: border-box;
    margin-bottom: 24px;
    position: relative;
    max-width: 500px;
  }
  .posts-list .post-preview img {
    width: 100%;
    display: block;
  }

  .posts-list .metadata {
    font-size: 12px;
    line-height: 1.4em;
    margin-bottom: 18px;
  }

  .posts-list .metadata > * {
    display: inline-block;
  }

  .posts-list .metadata .publishedDate {
    margin-right: 2em;
  }

  .posts-list .metadata .dt-authors {
    display: block;
    margin-top: 0.3em;
    margin-right: 2em;
  }

  .posts-list .dt-tags {
    display: block;
    line-height: 1em;
  }

  .posts-list .dt-tags .dt-tag {
    display: inline-block;
    color: rgba(0,0,0,0.6);
    padding: 0.3em 0.4em;
    margin-right: 0.2em;
    margin-bottom: 0.4em;
    font-size: 60%;
    border: 1px solid rgba(0,0,0,0.2);
    border-radius: 3px;
    text-transform: uppercase;
    font-weight: 500;
  }

  .posts-list img {
    opacity: 1;
  }

  .posts-list img[data-src] {
    opacity: 0;
  }

  .posts-more {
    clear: both;
  }


  .posts-sidebar {
    font-size: 16px;
  }

  .posts-sidebar h3 {
    font-size: 16px;
    margin-top: 0;
    margin-bottom: 0.5em;
    font-weight: 400;
    text-transform: uppercase;
  }

  .sidebar-section {
    margin-bottom: 30px;
  }

  .categories ul {
    list-style-type: none;
    margin: 0;
    padding: 0;
  }

  .categories li {
    color: rgba(0, 0, 0, 0.8);
    margin-bottom: 0;
  }

  .categories li>a {
    border-bottom: none;
  }

  .categories li>a:hover {
    border-bottom: 1px solid rgba(0, 0, 0, 0.4);
  }

  .categories .active {
    font-weight: 600;
  }

  .categories .category-count {
    color: rgba(0, 0, 0, 0.4);
  }


  @media(min-width: 768px) {
    .posts-list .post-preview h2 {
      font-size: 26px;
    }
    .posts-list .post-preview .thumbnail {
      float: right;
      width: 30%;
      margin-bottom: 0;
    }
    .posts-list .post-preview .description {
      float: left;
      width: 45%;
    }
    .posts-list .post-preview .metadata {
      float: left;
      width: 20%;
      margin-top: 8px;
    }
    .posts-list .post-preview p {
      margin: 0 0 12px 0;
      line-height: 1.5em;
      font-size: 16px;
    }
    .posts-with-sidebar .posts-list {
      float: left;
      width: 75%;
    }
    .posts-with-sidebar .posts-sidebar {
      float: right;
      width: 20%;
      margin-top: 60px;
      padding-top: 24px;
      padding-bottom: 24px;
    }
  }


  /* Improve display for browsers without grid (IE/Edge <= 15) */

  .downlevel {
    line-height: 1.6em;
    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
    margin: 0;
  }

  .downlevel .d-title {
    padding-top: 6rem;
    padding-bottom: 1.5rem;
  }

  .downlevel .d-title h1 {
    font-size: 50px;
    font-weight: 700;
    line-height: 1.1em;
    margin: 0 0 0.5rem;
  }

  .downlevel .d-title p {
    font-weight: 300;
    font-size: 1.2rem;
    line-height: 1.55em;
    margin-top: 0;
  }

  .downlevel .d-byline {
    padding-top: 0.8em;
    padding-bottom: 0.8em;
    font-size: 0.8rem;
    line-height: 1.8em;
  }

  .downlevel .section-separator {
    border: none;
    border-top: 1px solid rgba(0, 0, 0, 0.1);
  }

  .downlevel .d-article {
    font-size: 1.06rem;
    line-height: 1.7em;
    padding-top: 1rem;
    padding-bottom: 2rem;
  }


  .downlevel .d-appendix {
    padding-left: 0;
    padding-right: 0;
    max-width: none;
    font-size: 0.8em;
    line-height: 1.7em;
    margin-bottom: 0;
    color: rgba(0,0,0,0.5);
    padding-top: 40px;
    padding-bottom: 48px;
  }

  .downlevel .footnotes ol {
    padding-left: 13px;
  }

  .downlevel .base-grid,
  .downlevel .distill-header,
  .downlevel .d-title,
  .downlevel .d-abstract,
  .downlevel .d-article,
  .downlevel .d-appendix,
  .downlevel .distill-appendix,
  .downlevel .d-byline,
  .downlevel .d-footnote-list,
  .downlevel .d-citation-list,
  .downlevel .distill-footer,
  .downlevel .appendix-bottom,
  .downlevel .posts-container {
    padding-left: 40px;
    padding-right: 40px;
  }

  @media(min-width: 768px) {
    .downlevel .base-grid,
    .downlevel .distill-header,
    .downlevel .d-title,
    .downlevel .d-abstract,
    .downlevel .d-article,
    .downlevel .d-appendix,
    .downlevel .distill-appendix,
    .downlevel .d-byline,
    .downlevel .d-footnote-list,
    .downlevel .d-citation-list,
    .downlevel .distill-footer,
    .downlevel .appendix-bottom,
    .downlevel .posts-container {
    padding-left: 150px;
    padding-right: 150px;
    max-width: 900px;
  }
  }

  .downlevel pre code {
    display: block;
    border-left: 2px solid rgba(0, 0, 0, .1);
    padding: 0 0 0 20px;
    font-size: 14px;
  }

  .downlevel code, .downlevel pre {
    color: black;
    background: none;
    font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
    text-align: left;
    white-space: pre;
    word-spacing: normal;
    word-break: normal;
    word-wrap: normal;
    line-height: 1.5;

    -moz-tab-size: 4;
    -o-tab-size: 4;
    tab-size: 4;

    -webkit-hyphens: none;
    -moz-hyphens: none;
    -ms-hyphens: none;
    hyphens: none;
  }

  .downlevel .posts-list .post-preview {
    color: inherit;
  }



  </style>

  <script type="application/javascript">

  function is_downlevel_browser() {
    if (bowser.isUnsupportedBrowser({ msie: "12", msedge: "16"},
                                   window.navigator.userAgent)) {
      return true;
    } else {
      return window.load_distill_framework === undefined;
    }
  }

  // show body when load is complete
  function on_load_complete() {

    // add anchors
    if (window.anchors) {
      window.anchors.options.placement = 'left';
      window.anchors.add('d-article > h2, d-article > h3, d-article > h4, d-article > h5');
    }


    // set body to visible
    document.body.style.visibility = 'visible';

    // force redraw for leaflet widgets
    if (window.HTMLWidgets) {
      var maps = window.HTMLWidgets.findAll(".leaflet");
      $.each(maps, function(i, el) {
        var map = this.getMap();
        map.invalidateSize();
        map.eachLayer(function(layer) {
          if (layer instanceof L.TileLayer)
            layer.redraw();
        });
      });
    }

    // trigger 'shown' so htmlwidgets resize
    $('d-article').trigger('shown');
  }

  function init_distill() {

    init_common();

    // create front matter
    var front_matter = $('<d-front-matter></d-front-matter>');
    $('#distill-front-matter').wrap(front_matter);

    // create d-title
    $('.d-title').changeElementType('d-title');

    // create d-byline
    var byline = $('<d-byline></d-byline>');
    $('.d-byline').replaceWith(byline);

    // create d-article
    var article = $('<d-article></d-article>');
    $('.d-article').wrap(article).children().unwrap();

    // move posts container into article
    $('.posts-container').appendTo($('d-article'));

    // create d-appendix
    $('.d-appendix').changeElementType('d-appendix');

    // flag indicating that we have appendix items
    var appendix = $('.appendix-bottom').children('h3').length > 0;

    // replace footnotes with <d-footnote>
    $('.footnote-ref').each(function(i, val) {
      appendix = true;
      var href = $(this).attr('href');
      var id = href.replace('#', '');
      var fn = $('#' + id);
      var fn_p = $('#' + id + '>p');
      fn_p.find('.footnote-back').remove();
      var text = fn_p.html();
      var dtfn = $('<d-footnote></d-footnote>');
      dtfn.html(text);
      $(this).replaceWith(dtfn);
    });
    // remove footnotes
    $('.footnotes').remove();

    // move refs into #references-listing
    $('#references-listing').replaceWith($('#refs'));

    $('h1.appendix, h2.appendix').each(function(i, val) {
      $(this).changeElementType('h3');
    });
    $('h3.appendix').each(function(i, val) {
      var id = $(this).attr('id');
      $('.d-contents a[href="#' + id + '"]').parent().remove();
      appendix = true;
      $(this).nextUntil($('h1, h2, h3')).addBack().appendTo($('d-appendix'));
    });

    // show d-appendix if we have appendix content
    $("d-appendix").css('display', appendix ? 'grid' : 'none');

    // localize layout chunks to just output
    $('.layout-chunk').each(function(i, val) {

      // capture layout
      var layout = $(this).attr('data-layout');

      // apply layout to markdown level block elements
      var elements = $(this).children().not('details, div.sourceCode, pre, script');
      elements.each(function(i, el) {
        var layout_div = $('<div class="' + layout + '"></div>');
        if (layout_div.hasClass('shaded')) {
          var shaded_content = $('<div class="shaded-content"></div>');
          $(this).wrap(shaded_content);
          $(this).parent().wrap(layout_div);
        } else {
          $(this).wrap(layout_div);
        }
      });


      // unwrap the layout-chunk div
      $(this).children().unwrap();
    });

    // remove code block used to force  highlighting css
    $('.distill-force-highlighting-css').parent().remove();

    // remove empty line numbers inserted by pandoc when using a
    // custom syntax highlighting theme
    $('code.sourceCode a:empty').remove();

    // load distill framework
    load_distill_framework();

    // wait for window.distillRunlevel == 4 to do post processing
    function distill_post_process() {

      if (!window.distillRunlevel || window.distillRunlevel < 4)
        return;

      // hide author/affiliations entirely if we have no authors
      var front_matter = JSON.parse($("#distill-front-matter").html());
      var have_authors = front_matter.authors && front_matter.authors.length > 0;
      if (!have_authors)
        $('d-byline').addClass('hidden');

      // article with toc class
      $('.d-contents').parent().addClass('d-article-with-toc');

      // strip links that point to #
      $('.authors-affiliations').find('a[href="#"]').removeAttr('href');

      // add orcid ids
      $('.authors-affiliations').find('.author').each(function(i, el) {
        var orcid_id = front_matter.authors[i].orcidID;
        if (orcid_id) {
          var a = $('<a></a>');
          a.attr('href', 'https://orcid.org/' + orcid_id);
          var img = $('<img></img>');
          img.addClass('orcid-id');
          img.attr('alt', 'ORCID ID');
          img.attr('src','data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg==');
          a.append(img);
          $(this).append(a);
        }
      });

      // hide elements of author/affiliations grid that have no value
      function hide_byline_column(caption) {
        $('d-byline').find('h3:contains("' + caption + '")').parent().css('visibility', 'hidden');
      }

      // affiliations
      var have_affiliations = false;
      for (var i = 0; i<front_matter.authors.length; ++i) {
        var author = front_matter.authors[i];
        if (author.affiliation !== "&nbsp;") {
          have_affiliations = true;
          break;
        }
      }
      if (!have_affiliations)
        $('d-byline').find('h3:contains("Affiliations")').css('visibility', 'hidden');

      // published date
      if (!front_matter.publishedDate)
        hide_byline_column("Published");

      // document object identifier
      var doi = $('d-byline').find('h3:contains("DOI")');
      var doi_p = doi.next().empty();
      if (!front_matter.doi) {
        // if we have a citation and valid citationText then link to that
        if ($('#citation').length > 0 && front_matter.citationText) {
          doi.html('Citation');
          $('<a href="#citation"></a>')
            .text(front_matter.citationText)
            .appendTo(doi_p);
        } else {
          hide_byline_column("DOI");
        }
      } else {
        $('<a></a>')
           .attr('href', "https://doi.ieeecomputersociety.org/" + front_matter.doi)
           .html(front_matter.doi)
           .appendTo(doi_p);
      }

       // change plural form of authors/affiliations
      if (front_matter.authors.length === 1) {
        var grid = $('.authors-affiliations');
        grid.children('h3:contains("Authors")').text('Author');
        grid.children('h3:contains("Affiliations")').text('Affiliation');
      }

      // remove d-appendix and d-footnote-list local styles
      $('d-appendix > style:first-child').remove();
      $('d-footnote-list > style:first-child').remove();

      // move appendix-bottom entries to the bottom
      $('.appendix-bottom').appendTo('d-appendix').children().unwrap();
      $('.appendix-bottom').remove();

      // hoverable references
      $('span.citation[data-cites]').each(function() {
        const citeChild = $(this).children()[0]
        // Do not process if @xyz has been used without escaping and without bibliography activated
        // https://github.com/rstudio/distill/issues/466
        if (citeChild === undefined) return true

        if (citeChild.nodeName == "D-FOOTNOTE") {
          var fn = citeChild
          $(this).html(fn.shadowRoot.querySelector("sup"))
          $(this).id = fn.id
          fn.remove()
        }
        var refs = $(this).attr('data-cites').split(" ");
        var refHtml = refs.map(function(ref) {
          // Could use CSS.escape too here, we insure backward compatibility in navigator
          return "<p>" + $('div[id="ref-' + ref + '"]').html() + "</p>";
        }).join("\n");
        window.tippy(this, {
          allowHTML: true,
          content: refHtml,
          maxWidth: 500,
          interactive: true,
          interactiveBorder: 10,
          theme: 'light-border',
          placement: 'bottom-start'
        });
      });

      // fix footnotes in tables (#411)
      // replacing broken distill.pub feature
      $('table d-footnote').each(function() {
        // we replace internal showAtNode methode which is triggered when hovering a footnote
        this.hoverBox.showAtNode = function(node) {
          // ported from https://github.com/distillpub/template/pull/105/files
          calcOffset = function(elem) {
              let x = elem.offsetLeft;
              let y = elem.offsetTop;
              // Traverse upwards until an `absolute` element is found or `elem`
              // becomes null.
              while (elem = elem.offsetParent && elem.style.position != 'absolute') {
                  x += elem.offsetLeft;
                  y += elem.offsetTop;
              }

              return { left: x, top: y };
          }
          // https://developer.mozilla.org/en-US/docs/Web/API/HTMLElement/offsetTop
          const bbox = node.getBoundingClientRect();
          const offset = calcOffset(node);
          this.show([offset.left + bbox.width, offset.top + bbox.height]);
        }
      })

      // clear polling timer
      clearInterval(tid);

      // show body now that everything is ready
      on_load_complete();
    }

    var tid = setInterval(distill_post_process, 50);
    distill_post_process();

  }

  function init_downlevel() {

    init_common();

     // insert hr after d-title
    $('.d-title').after($('<hr class="section-separator"/>'));

    // check if we have authors
    var front_matter = JSON.parse($("#distill-front-matter").html());
    var have_authors = front_matter.authors && front_matter.authors.length > 0;

    // manage byline/border
    if (!have_authors)
      $('.d-byline').remove();
    $('.d-byline').after($('<hr class="section-separator"/>'));
    $('.d-byline a').remove();

    // remove toc
    $('.d-contents').remove();

    // move appendix elements
    $('h1.appendix, h2.appendix').each(function(i, val) {
      $(this).changeElementType('h3');
    });
    $('h3.appendix').each(function(i, val) {
      $(this).nextUntil($('h1, h2, h3')).addBack().appendTo($('.d-appendix'));
    });


    // inject headers into references and footnotes
    var refs_header = $('<h3></h3>');
    refs_header.text('References');
    $('#refs').prepend(refs_header);

    var footnotes_header = $('<h3></h3');
    footnotes_header.text('Footnotes');
    $('.footnotes').children('hr').first().replaceWith(footnotes_header);

    // move appendix-bottom entries to the bottom
    $('.appendix-bottom').appendTo('.d-appendix').children().unwrap();
    $('.appendix-bottom').remove();

    // remove appendix if it's empty
    if ($('.d-appendix').children().length === 0)
      $('.d-appendix').remove();

    // prepend separator above appendix
    $('.d-appendix').before($('<hr class="section-separator" style="clear: both"/>'));

    // trim code
    $('pre>code').each(function(i, val) {
      $(this).html($.trim($(this).html()));
    });

    // move posts-container right before article
    $('.posts-container').insertBefore($('.d-article'));

    $('body').addClass('downlevel');

    on_load_complete();
  }


  function init_common() {

    // jquery plugin to change element types
    (function($) {
      $.fn.changeElementType = function(newType) {
        var attrs = {};

        $.each(this[0].attributes, function(idx, attr) {
          attrs[attr.nodeName] = attr.nodeValue;
        });

        this.replaceWith(function() {
          return $("<" + newType + "/>", attrs).append($(this).contents());
        });
      };
    })(jQuery);

    // prevent underline for linked images
    $('a > img').parent().css({'border-bottom' : 'none'});

    // mark non-body figures created by knitr chunks as 100% width
    $('.layout-chunk').each(function(i, val) {
      var figures = $(this).find('img, .html-widget');
      // ignore leaflet img layers (#106)
      figures = figures.filter(':not(img[class*="leaflet"])')
      if ($(this).attr('data-layout') !== "l-body") {
        figures.css('width', '100%');
      } else {
        figures.css('max-width', '100%');
        figures.filter("[width]").each(function(i, val) {
          var fig = $(this);
          fig.css('width', fig.attr('width') + 'px');
        });

      }
    });

    // auto-append index.html to post-preview links in file: protocol
    // and in rstudio ide preview
    $('.post-preview').each(function(i, val) {
      if (window.location.protocol === "file:")
        $(this).attr('href', $(this).attr('href') + "index.html");
    });

    // get rid of index.html references in header
    if (window.location.protocol !== "file:") {
      $('.distill-site-header a[href]').each(function(i,val) {
        $(this).attr('href', $(this).attr('href').replace(/^index[.]html/, "./"));
      });
    }

    // add class to pandoc style tables
    $('tr.header').parent('thead').parent('table').addClass('pandoc-table');
    $('.kable-table').children('table').addClass('pandoc-table');

    // add figcaption style to table captions
    $('caption').parent('table').addClass("figcaption");

    // initialize posts list
    if (window.init_posts_list)
      window.init_posts_list();

    // implmement disqus comment link
    $('.disqus-comment-count').click(function() {
      window.headroom_prevent_pin = true;
      $('#disqus_thread').toggleClass('hidden');
      if (!$('#disqus_thread').hasClass('hidden')) {
        var offset = $(this).offset();
        $(window).resize();
        $('html, body').animate({
          scrollTop: offset.top - 35
        });
      }
    });
  }

  document.addEventListener('DOMContentLoaded', function() {
    if (is_downlevel_browser())
      init_downlevel();
    else
      window.addEventListener('WebComponentsReady', init_distill);
  });

  </script>

  <!--/radix_placeholder_distill-->
  <script src="site_libs/header-attrs-2.21/header-attrs.js"></script>
  <script src="site_libs/popper-2.6.0/popper.min.js"></script>
  <link href="site_libs/tippy-6.2.7/tippy.css" rel="stylesheet" />
  <link href="site_libs/tippy-6.2.7/tippy-light-border.css" rel="stylesheet" />
  <script src="site_libs/tippy-6.2.7/tippy.umd.min.js"></script>
  <script src="site_libs/anchor-4.2.2/anchor.min.js"></script>
  <script src="site_libs/bowser-1.9.3/bowser.min.js"></script>
  <script src="site_libs/webcomponents-2.0.0/webcomponents.js"></script>
  <script src="site_libs/distill-2.2.21/template.v2.js"></script>
  <!--radix_placeholder_site_in_header-->
  <style type="text/css">
  @media only screen and (max-width: 768px) {
    .header {display: none; }
    .d-title {padding-top: 21px; }
  }

  p {
    text-rendering: optimizeLegibility;
    text-align: justify;
    hyphens: auto;
    -webkit-hyphens: auto;
    -moz-hyphens: auto;
  }

  h2 {
    font-size: 22px;
  }

  d-title h1 {
    font-size: 30px;
  }

  d-article h2 {
      font-weight: 600;
      font-size: 24px;
      line-height: 1.25em;
      margin: 2rem 0 1.5rem 0;
      border-bottom: 1px solid rgba(0, 0, 0, 0.1);
      padding-bottom: 1rem;
  }


  </style>
  <!--/radix_placeholder_site_in_header-->

  <link rel="stylesheet" href="style.css" type="text/css"/>

</head>

<body>

<!--radix_placeholder_front_matter-->

<script id="distill-front-matter" type="text/json">
{"title":"SoK: Cryptographic Neural-Network Computation","doi":"10.1109/SP46215.2023.00198","authors":[{"author":"Lucien K. L. Ng","authorURL":"https://lucieno.github.io/","affiliation":"Georgia Institute of Technology","affiliationURL":"https://www.gatech.edu/","orcidID":"0000-0003-3662-3237"},{"author":"Sherman S. M Chow","authorURL":"https://staff.ie.cuhk.edu.hk/~smchow/","affiliation":"Chinese University of Hong Kong","affiliationURL":"https://www.cuhk.edu.hk/","orcidID":"0000-0001-7306-453X"}],"publishedDate":"2023-05-26T00:00:00.000-07:00","citationText":"Ng & Chow, 2023"}
</script>

<!--/radix_placeholder_front_matter-->
<!--radix_placeholder_navigation_before_body-->
<header class="header header--fixed" role="banner">
<nav class="distill-site-nav distill-site-header">
<div class="nav-left">
<a href="index.html" class="title">SoK: Cryptographic Neural-Network Computation</a>
</div>
<div class="nav-right">
<a href="index.html">SoK</a>
<a href="javascript:void(0);" class="nav-toggle">&#9776;</a>
</div>
</nav>
</header>
<!--/radix_placeholder_navigation_before_body-->
<!--radix_placeholder_site_before_body-->
<!--/radix_placeholder_site_before_body-->

<div class="d-title">
<h1>SoK: Cryptographic Neural-Network Computation</h1>
<!--radix_placeholder_categories-->
<!--/radix_placeholder_categories-->

</div>

<div class="d-byline">
  
  true
  
,   
  true
  
<br/>May 26, 2023
</div>

<div class="d-article">
<div class="d-contents d-contents-float">
<nav class="l-text toc figcaption" id="TOC">
<h3>Contents</h3>
<ul>
<li><a href="#abstract" id="toc-abstract">Abstract</a></li>
<li><a href="#introduction" id="toc-introduction">Introduction</a></li>
<li><a href="#sec:preliminary" id="toc-sec:preliminary">Preliminary</a></li>
<li><a href="#sec:major-approaches" id="toc-sec:major-approaches">Systematizing Machine learning <span class="math inline">\(\times\)</span> Cryptology</a></li>
<li><a href="#sec:pure-lhe" id="toc-sec:pure-lhe">Pure-LHE Frameworks</a></li>
<li><a href="#sec:circuit" id="toc-sec:circuit">Boolean-Circuit-based Approaches</a></li>
<li><a href="#sec:mixed" id="toc-sec:mixed">Mixed Frameworks for Oblivious Inference</a></li>
<li><a href="#sec:mpc" id="toc-sec:mpc">MPC-based Inference/Training Frameworks</a></li>
<li><a href="#sec:deploy" id="toc-sec:deploy">Practical Deployment Issues</a></li>
<li><a href="#sec:benchmark" id="toc-sec:benchmark">Status Quo in Accuracy/Latency/Throughput</a></li>
<li><a href="#sec:discussion" id="toc-sec:discussion">Discussion</a></li>
<li><a href="#sec:sota-diff-setting" id="toc-sec:sota-diff-setting">The State of the Art for Various Setups/Features</a></li>
<li><a href="#genealogy" id="toc-genealogy">Genealogy</a></li>
<li><a href="#summary-fw" id="toc-summary-fw">Summary of Frameworks</a></li>
<li><a href="#interactive-chart" id="toc-interactive-chart">Interactive Chart</a></li>
</ul>
</nav>
</div>
<script type="text/javascript">
window.MathJax = {
  AuthorInit: function () {
    MathJax.Hub.setRenderer('CommonHTML');
    MathJax.Hub.Rerender();
  }
};

function waitForElm(selector) {
    return new Promise(resolve => {
        if (document.querySelector(selector)) {
            return resolve(document.querySelector(selector));
        }

        const observer = new MutationObserver(mutations => {
            if (document.querySelector(selector)) {
                resolve(document.querySelector(selector));
                observer.disconnect();
            }
        });

        observer.observe(document.body, {
            childList: true,
            subtree: true
        });
    });
}

waitForElm("body > d-byline > div > div:nth-child(2)").then((elm) => {
    elm.innerHTML = "<div><h3>Last Updated</h3><p>19 June, 2023</p></div>"
    console.log(elm.textContent);
});


</script>
<div class="sourceCode" id="cb1"><pre class="sourceCode r distill-force-highlighting-css"><code class="sourceCode r"></code></pre></div>
<h2 id="abstract">Abstract</h2>
<p>We studied <span class="math inline">\(53\)</span> privacy-preserving neural-network papers in 2016-2022
based on cryptography (without trusted processors or differential
privacy), <span class="math inline">\(16\)</span> of which only use homomorphic encryption, <span class="math inline">\(19\)</span> use secure
computation for <em>inference</em>, and <span class="math inline">\(18\)</span> use non-colluding servers (among
which <span class="math inline">\(12\)</span> support <em>training</em>), solving a wide variety of research
problems. We dissect their cryptographic techniques and “love-hate
relationships” with machine learning alongside a genealogy highlighting
noteworthy developments. We also re-evaluate the state of the art under
WAN. We hope this can serve as a go-to guide connecting different
experts in related fields.</p>
<h2 id="introduction">Introduction</h2>
<p>Secure machine-learning (ML) computation, specifically privacy-preserving neural networks (PPNN), has attracted much attention for their broad applicability in often privacy-sensitive scenarios.
Model owners who provide inference services or powerful cloud platforms that provide training services might not always be trusted to ensure the data privacy of the inference query or the training data.
Cryptographic techniques come in handy in realizing four privacy services (Table <a href="#tab:set">1</a>) – oblivious inference, outsourced inference, outsourced training (by one data owner), and private training (for <em>multiple</em> data owners).</p>
<p><strong>Oblivious Inference.</strong>
Many inference applications
involve sensitive data in queries.</p>
<p>Model owners could give the neural network (NN) to queriers for local evaluation, but they often see the model as a valuable intellectual property <span class="citation" data-cites="uss/TramerZJRR16">(<a href="#ref-uss/TramerZJRR16" role="doc-biblioref">Tramèr et al. 2016</a>)</span>.
Leaking models also paves the way for attacks <span class="citation" data-cites="iclr/GoodfellowSS15 eurosp/PapernotMSW18 ieeesp/RiaziRK19">(<a href="#ref-iclr/GoodfellowSS15" role="doc-biblioref">Goodfellow, Shlens, and Szegedy 2015</a>; <a href="#ref-eurosp/PapernotMSW18" role="doc-biblioref">Papernot et al. 2018</a>; <a href="#ref-ieeesp/RiaziRK19" role="doc-biblioref">Riazi, Rouhani, and Koushanfar 2019</a>)</span>.</p>
<p>In <em>oblivious inference</em>, a querier can learn the inference result (and what is inferrable from it) in a secure two-party computation manner over the query and the model parameters acquired from the training.</p>
<p><strong>Outsourced Inference/Training.</strong>
Model owners may not have in-house facilities for running inference services.
With secure <em>outsourced inference</em>, untrusted third parties can be delegated to answer
queries without knowing the model, i.e.,
it is a threat that any server is curious about the model too.</p>
<p>Likewise, the data owner may desire secure <em>outsourced training</em>.
(Frameworks for outsourced training mostly support outsourced inference because handling queries is a sub-routine of NN training.)</p>
<p><strong>Private Training.</strong>
<em>Multiple</em> mutually distrustful data owners can
collectively train an NN with privacy using interactive protocols.
Typically, the job can be done by servers that none of them trust for data privacy.</p>
<p>We use the term <em>private training</em>
for <em>multiple</em> owners, which covers
outsourced training (for one data owner).</p>
<p><strong>Early Works</strong> focused on the less complex task of inference.
Barni, Orlandi, and Piva <span class="citation" data-cites="mmsec/BarniOP06 ejisec/OrlandiPB07">(<a href="#ref-mmsec/BarniOP06" role="doc-biblioref">Barni, Orlandi, and Piva 2006</a>; <a href="#ref-ejisec/OrlandiPB07" role="doc-biblioref">Orlandi, Piva, and Barni 2007</a>)</span> process model and query by (additive) homomorphic encryption (HE) for linear NN computations
and garbled circuits <span class="citation" data-cites="focs/Yao86">(<a href="#ref-focs/Yao86" role="doc-biblioref">Yao 1986</a>)</span> and HE (with polynomial approximation) for non-linear ones.</p>
<p>Sadeghi and Schneider <span class="citation" data-cites="icisc/SadeghiS08">(<a href="#ref-icisc/SadeghiS08" role="doc-biblioref">Sadeghi and Schneider 2008</a>)</span> rely on secure universal-circuit evaluation <span class="citation" data-cites="focs/Yao86 stoc/GoldreichMW87">(<a href="#ref-focs/Yao86" role="doc-biblioref">Yao 1986</a>; <a href="#ref-stoc/GoldreichMW87" role="doc-biblioref">Goldreich, Micali, and Wigderson 1987</a>)</span> solely.
It was 2006-08 when NN did not (re-)gain its popularity,
and fully-HE <span class="citation" data-cites="stoc/Gentry09">(<a href="#ref-stoc/Gentry09" role="doc-biblioref">Gentry 2009</a>)</span> and leveled-HE (LHE) were not available.</p>
<p>In 2016,
a purely LHE-based oblivious inference solution CryptoNets was proposed <span class="citation" data-cites="icml/Gilad-BachrachD16">(<a href="#ref-icml/Gilad-BachrachD16" role="doc-biblioref">Gilad-Bachrach et al. 2016</a>)</span>.
Shortly afterward,
MiniONN <span class="citation" data-cites="ccs/LiuJLA17">(<a href="#ref-ccs/LiuJLA17" role="doc-biblioref">Liu et al. 2017</a>)</span> and SecureML <span class="citation" data-cites="sp/MohasselZ17">(<a href="#ref-sp/MohasselZ17" role="doc-biblioref">Mohassel and Zhang 2017</a>)</span> appeared.
The former uses secure two-party computation techniques beyond LHE for oblivious inference,
while the latter is the first <span class="math inline">\(2\)</span>-server solution of private training.</p>
<p><strong>Dissecting the Development.</strong></p>
<p>PPNN has then rapidly developed across different disciplines, such as system research, scientific computing, machine learning, and cryptography (or crypto in short), posing a rich set of challenging problems requiring quite a few different skill sets to tackle.
We systematize <span class="math inline">\(53\)</span> <em>frameworks</em>
born in
these <span class="math inline">\(8\)</span> years,
microscopically explore their different
technical contributions,
and categorize them by their
problem settings and major approaches,
summarized in <a href="#summary-fw">our Summary of all Frameworks</a>.</p>
<aside>
We call NN software frameworks (e.g., PyTorch) by libraries instead.
</aside>
<p><a href="#genealogy">Our genealogy</a> sorts out the branches in the literature for a clear genealogy and easy
identification of research gaps/opportunities.</p>
<aside>
Prior surveys focus on HE-based works <span class="citation" data-cites="ml4cs/TanuwidjajaCK19">(<a href="#ref-ml4cs/TanuwidjajaCK19" role="doc-biblioref">Tanuwidjaja, Choi, and Kim 2019</a>)</span> or an overlapping subset of <span class="math inline">\({\leq}13\)</span> frameworks <span class="citation" data-cites="ieeesp/RiaziRK19 primelife/AzraouiBBCCEMMO19">(<a href="#ref-ieeesp/RiaziRK19" role="doc-biblioref">Riazi, Rouhani, and Koushanfar 2019</a>; <a href="#ref-primelife/AzraouiBBCCEMMO19" role="doc-biblioref">Azraoui et al. 2019</a>)</span> without aiming for a clear lineage of ideas.
</aside>
<p><strong>Easier Comparisons.</strong>
This SoK also explains the commonly-used benchmarks and sheds light on how to gauge a framework, which to use for a given setting/goal, and which frameworks to compare with,
e.g., to avoid unfair comparison between one assuming non-colluding servers while the other does not.
For the state-of-the-art frameworks assumed to run on LAN, we re-evaluate them on WAN.</p>
<p>We found <span class="math inline">\(1000\)</span>Mbps is needed
for
peak performance,
but lower bandwidth (<span class="math inline">\(10 \sim 100\)</span>Mbps)
suffices for some of them in non-real-time applications.</p>
<p><strong>Fostering Interdisciplinary Research.</strong>
Many issues in PPNN may look complicated at first glance to “outsiders.”
We explain the challenges and tricks for addressing them.
Together with the categorization of contributions in <a href="#summary-fw">our Summary of all Frameworks</a>, experts of different fields can quickly identify the papers related to their interested (sub-)topics, dive right into the crux, and avoid reinventing the wheel.
Finally, we share some outstanding challenges
and less-explored open problems at the end of a (sub-)section.</p>
<p><strong>Scope.</strong>
We focus on purely-crypto-based approaches and exclude those that use a trusted
execution environment, e.g., <span class="citation" data-cites="aaai/NgCWW021">(<a href="#ref-aaai/NgCWW021" role="doc-biblioref">Ng et al. 2021</a>)</span>.
As crypto literature, membership inference <span class="citation" data-cites="sp/ShokriSSS17">(<a href="#ref-sp/ShokriSSS17" role="doc-biblioref">Shokri et al. 2017</a>)</span> and model inversion <span class="citation" data-cites="ccs/FredriksonJR15">(<a href="#ref-ccs/FredriksonJR15" role="doc-biblioref">Fredrikson, Jha, and Ristenpart 2015</a>)</span>
are not in our scope;
and neither is
federated <span class="citation" data-cites="tist/YangLCT19">(<a href="#ref-tist/YangLCT19" role="doc-biblioref">Q. Yang et al. 2019</a>)</span> or coopetitive learning <span class="citation" data-cites="sp/ZhengPGS19">(<a href="#ref-sp/ZhengPGS19" role="doc-biblioref">Zheng et al. 2019</a>)</span>.</p>
<aside>
Differential privacy <span class="citation" data-cites="tcc/DworkMNS06 corr/privacyInDeepLearning www/DuYCS23 ccs/DuYCWHS23">(<a href="#ref-tcc/DworkMNS06" role="doc-biblioref">Dwork et al. 2006</a>; <a href="#ref-corr/privacyInDeepLearning" role="doc-biblioref">Mireshghallah et al. 2020</a>; <a href="#ref-www/DuYCS23" role="doc-biblioref">M. Du, Yue, Chow, and Sun 2023</a>; <a href="#ref-ccs/DuYCWHS23" role="doc-biblioref">M. Du, Yue, Chow, Wang, et al. 2023</a>)</span> can serve as
defenses.
</aside>
<p>Finally, we refer to a recent SoK <span class="citation" data-cites="popets/Cabrero-Holgueras21">(<a href="#ref-popets/Cabrero-Holgueras21" role="doc-biblioref">Cabrero-Holgueras and Pastrana 2021</a>)</span> for longer expositions of the basics,
with its scope confined to open-sourced projects and
MPC libraries.</p>
<h2 id="sec:preliminary">Preliminary</h2>
<h3 id="sec:prelim_ml">Deep Neural Network (DNN)</h3>
<p>DNN (or just NN)
imitates how biological NNs learn
by inferring an underlying model from samples,
which can be used for prediction/classification.
DNNs simplify the imitation to a stack of
linear and non-linear layers <span class="math inline">\(f_1, \ldots, f_n\)</span>;
each (except the first input layer) takes the outputs from its previous layer.</p>
<aside>
More advanced NNs may have multiple paths of layers from inputs to outputs; each may take inputs from more previous layers.
</aside>
<p>One may see it as a chained function <span class="math inline">\(\mathsf{NN} = f_n \circ \cdots \circ f_1\)</span>
if inputs other than weights are seen as “hardwired” parameters.
The description in this section supposes the NNs are for visual tasks.</p>
<p><strong>Linear Layers.</strong>
Common examples include dense/fully-connected layers (matrix multiplication) and 2D convolution.
The weight, i.e., matrices of the model and kernels of the convolution, are learnable parameters
that should be kept private.
A linear layer outputs
<span class="math inline">\(y_i = \sum_j w_{ij} \cdot x_{ij}\)</span>
for weight <span class="math inline">\(w_{ij}\)</span> and input <span class="math inline">\(x_{ij}\)</span>.
Its evaluation can be reduced to dot products.</p>
<p><strong>Common Kinds of Non-Linear Layers:</strong></p>
<ol type="1">
<li><p>Activation layers apply an activation function to all inputs,
e.g.,
popular <span class="math inline">\(\mathsf{softmax}\)</span>,
rectified linear unit
<span class="math inline">\(\mathsf{ReLU}\)</span>, <span class="math inline">\(\mathsf{sigmoid}\)</span>,
etc.</p></li>
<li><p>Pooling layers aggregate a window from the <span class="math inline">\(2\)</span>D-output from their previous layers,
e.g.,
<span class="math inline">\(\mathsf{maxpool}\)</span>, <span class="math inline">\(\mathsf{meanpool}\)</span>,
etc.</p></li>
<li><p>Normalization layers regularize
the inputs
into a smaller range,
e.g., batch normalization (BatchNorm) <span class="citation" data-cites="icml/IoffeS15">(<a href="#ref-icml/IoffeS15" role="doc-biblioref">Ioffe and Szegedy 2015</a>)</span>
maps each pixel
to a standard normal distribution across the input batch.</p></li>
<li><p>The output layer computes the inference output based on all its inputs, e.g., <span class="math inline">\(\mathsf{softmax}\)</span> or <span class="math inline">\(\mathsf{argmax}\)</span>.
For training, it further computes a <em>loss value</em>,
judging how good/bad the NN is.</p></li>
</ol>
<p><strong>The architecture</strong> of an NN specifies the type and dimension of each layer.
The dimension of a layer means the size and shape of the inputs and output.
Layers may include other specifications,
e.g., convolutional and pooling layers take specifications on the shape of the kernels or the pooling windows, respectively.</p>
<p><strong>(Supervised) Training</strong> takes a labeled training data set and an architecture
and outputs the weights (or learnable parameters) of an NN.</p>
<p>Most NN training adopts <em>stochastic gradient descent</em> (SGD),
which updates all weights <span class="math inline">\(\mathbf{w}\)</span> iteratively over the training data batch-by-batch.
During training,</p>
<p>besides computing the outputs,
the output layer computes a loss value <span class="math inline">\(L = \mathcal{L}_\mathbf{w}(y&#39;, y)\)</span> respective to <span class="math inline">\(\mathbf{w}\)</span>,
where <span class="math inline">\(y&#39;\)</span> and <span class="math inline">\(y\)</span> are the labels and the inference outputs of that batch, respectively.</p>
<p>Let <span class="math inline">\(w_i\)</span> be a weight tensor of the <span class="math inline">\(i\)</span>-th layer <span class="math inline">\(f_i\)</span>,
and <span class="math inline">\(\Delta w_i\)</span> be its corresponding derivative, the <em>gradient</em>.
The weight is updated via <span class="math inline">\(w_i&#39; = w_i - \alpha \Delta w_i\)</span>,
where <span class="math inline">\(\alpha\)</span> is a learning rate to regulate the weight change.
With a reasonable <span class="math inline">\(\alpha\)</span>, the loss will gradually decrease,
meaning the model can make the right inference according to the training data.</p>
<p><em>Backward propagation</em> derives <span class="math inline">\(\Delta w_i\)</span>.
The derivative of a layer can be computed with that of the next layer recursively via <span class="math inline">\(\Delta w_i = f&#39;_{i, w}(\Delta x_{i+1}, x_i)\)</span> and <span class="math inline">\(\Delta x_i = f&#39;_{i, x}(\Delta x_{i+1}, w_i)\)</span>,
where <span class="math inline">\(f&#39;_{i, v}\)</span> are functions for computing the derivative for <span class="math inline">\(v\)</span> in the <span class="math inline">\(i\)</span>-th layer.
Their computation is similar to the layer’s original operations.</p>
<p>So,
it can happen recursively
and in backward order from <span class="math inline">\(\Delta w_n = \partial_y L\)</span> in the loss layer to the input layer.</p>
<p><strong>Machine-learning Libraries</strong>
(e.g.,
Caffe <span class="citation" data-cites="mm/JiaSDKLGGD14">(<a href="#ref-mm/JiaSDKLGGD14" role="doc-biblioref">Jia et al. 2014</a>)</span>,
<a href="https://keras.io">Keras</a>, <a href="https://pytorch.org">PyTorch</a>, TensorFlow <span class="citation" data-cites="osdi/AbadiBCCDDDGIIK16">(<a href="#ref-osdi/AbadiBCCDDDGIIK16" role="doc-biblioref">Abadi et al. 2016</a>)</span>, and <a href="http://deeplearning.net/software/theano">Theano</a>)
mostly take care of tedious computations of NN inference and training
for developers to conveniently realize an NN by just specifying its architecture.</p>
<p>They translate NN specifications into <em>computation graphs</em>, which are then relayed to compilers to generate the machine code.
Some PPNN frameworks aim to provide similar convenience.</p>
<p><strong>Common Benchmarks.</strong>
Oblivious inference aims to reduce the <em>latency</em>,
i.e., the time between the initiating query and the return of the result.
Another factor,
<em>throughput</em>, counts how many queries or training data one can process for a given time.
They are jointly affected by the choices of secure protocols, NN architectures, network conditions, and hardware components.</p>
<p>Private training also aims to improve the accuracy of the trained model.
It depends on the convergence rate, indicating the needed steps to attain saturated accuracy.</p>
<p><strong>GPUs</strong> feature an <em>instruction throughput</em> orders of magnitude higher than CPU when fully utilized:
SIMD (<em>single instruction multiple data</em>) performs the same operation across vectorized data
simultaneously, while <em>parallelism</em> divides a task for many
GPU cores to process.
GPUs are mainly optimized for floating-point numbers.
They are popular choices for NN computations.</p>
<p>Crypto primitives, however, cannot benefit from GPUs by default as they mostly work on fixed-point numbers.
Leveraging GPUs for privacy-preserving NN poses challenges <span class="citation" data-cites="iclr/TramerB19 aaai/NgCWW021">(<a href="#ref-iclr/TramerB19" role="doc-biblioref">Tramèr and Boneh 2019</a>; <a href="#ref-aaai/NgCWW021" role="doc-biblioref">Ng et al. 2021</a>)</span>.</p>
<h3 id="subsec:problem">Problem Formulation and Computation and Threat Models</h3>
<p>We consider
an abstract threat model in typical crypto literature.</p>
<aside>
e.g., the honest party’s hardware and software are not compromised.
</aside>
<p>Oblivious inference can be cleanly abstracted as the secure two-party computation of
<span class="math inline">\(f(\mathit{model}, \mathit{query}) \rightarrow (\bot, \mathit{result})\)</span>.
The model owner and the querier are also called the <em>server</em> and the <em>client</em>, respectively.
This paper refers to it as <em>server/client</em> or <em>S/C</em> (to avoid confusion with <span class="math inline">\(2\)</span>PC
frameworks using two non-colluding servers on top of two-party computation (<span class="math inline">\(2\)</span>PC) techniques).
In practice, some metadata, such as the dimension of the query as a feature vector, might be considered public knowledge (see <a href="#subsec:leakage">the Section on Leakage</a>).</p>
<p>Outsourced inference refers to the same computation,
while the <span class="math inline">\(\mathit{model}\)</span> is outsourced in an encrypted form to a single server
or secret-shared (<a href="#subsec:ss">the Section on Secret Sharing</a>) among different non-colluding servers.
The querier may or may not interact with servers during the computation.
Finally, <span class="math inline">\(n\)</span>-party private training can be abstracted as the secure multi-party computation of
<span class="math inline">\(f(\langle \mathit{dataset} \rangle_1, \ldots, \langle \mathit{dataset} \rangle_n) \rightarrow (\langle \mathit{model} \rangle_1, \ldots,\langle \mathit{model} \rangle_n)\)</span>.
Each participant is a non-colluding server,
inputs a secret share <span class="math inline">\(\langle \cdot \rangle\)</span> of the training dataset received from the data contributors,
and outputs a secret share <span class="math inline">\(\langle \cdot \rangle\)</span> of the trained model.</p>
<p>The trained parameters in <span class="math inline">\(\mathsf{NN}\)</span> are secret protected by secret shares.</p>
<p><em>Non-colluding</em> assumption means the participants,
e.g., <em>helper servers</em> <span class="citation" data-cites="dac/RouhaniRK18">(<a href="#ref-dac/RouhaniRK18" role="doc-biblioref">Rouhani, Riazi, and Koushanfar 2018</a>)</span>, will not reveal their secrets to any other parties,
e.g., competing cloud servers, prestigious agents,
a subset of model owners (in inference),
and/or a subset of data contributors (in training).</p>
<p>The number of servers required by existing frameworks ranges from <span class="math inline">\(2\)</span> to <span class="math inline">\(4\)</span>.
An adversary just needs to compromise any two servers for a total break against most of these frameworks covered in this SoK.
A <span class="math inline">\(4\)</span>-party setup is easier to be compromised than a <span class="math inline">\(3\)</span>-party setup
as there is one more server as an attack target.
The same applies to <span class="math inline">\(4\)</span>-/<span class="math inline">\(3\)</span>- vs. <span class="math inline">\(2\)</span>-party setups.</p>
<p>Many frameworks adopt <em>offline/online computation</em> to reduce the <em>online latency/throughput</em>.</p>
<p>Consider oblivious inference; in the offline phase, one can prepare, probably in batch, pre-computed results without knowing the query, using techniques that might be heavier than those in online computation.
In the online phase, where the query is known, the pre-computed results can speed up the online computation.</p>
<h3 id="sec:crypto_primitives">Cryptographic Primitives</h3>
<h4 id="secure-multi-party-computation-mpc">Secure Multi-Party Computation (MPC)</h4>
<p>allows mutually distrustful parties to jointly compute
a known function over their respective secret inputs
via interactions.
The computation results are often secret-shared in some forms
among the parties.
We refer to the MPC for <span class="math inline">\(n\)</span> parties by <span class="math inline">\(n\)</span>PC (e.g., <span class="math inline">\(4\)</span>PC).
We also refer to frameworks with non-colluding servers by MPC frameworks.</p>
<h4 id="subsec:ss"><span class="math inline">\((t, n)\)</span> Secret Sharing (SS)</h4>
<p>SS distributes a secret <span class="math inline">\(x\)</span> into
shares <span class="math inline">\(\{\langle x \rangle_i\}\)</span> for <span class="math inline">\(n\)</span> parties;
<span class="math inline">\(x\)</span> can be recovered from any <span class="math inline">\(t \geq n\)</span> shares,
e.g.,
<span class="math inline">\(\langle x \rangle_0 \in \mathbb{Z}_p\)</span>
and <span class="math inline">\(\langle x \rangle_1 = x - \langle x \rangle_0 \bmod{p}\)</span> are <span class="math inline">\((2, 2)\)</span> <em>additive</em> SS of <span class="math inline">\(x \in \mathbb{Z}_p\)</span>.
When <span class="math inline">\(p \gg 2\)</span>, we also call them <em>arithmetic SS</em>.
Addition and constant multiplication can be done locally.</p>
<p>Multiplication over SSs <span class="math inline">\(\langle x \cdot y \rangle\)</span> from <span class="math inline">\(\langle x \rangle, \langle y \rangle\)</span> can be done using Beaver’s trick <span class="citation" data-cites="crypto/Beaver91a">(<a href="#ref-crypto/Beaver91a" role="doc-biblioref">Beaver 1991</a>)</span>,
which requires both parties to have prepared offline (via HE, oblivious transfer, or a non-colluding server)
additive SS of <span class="math inline">\(u, v, z\)</span> (<em>Beaver triplets</em>), where <span class="math inline">\(z = u \cdot v\)</span>.</p>
<p><em>Boolean</em> SS <span class="math inline">\((2, 2)\)</span>-shares a bit-string via bit-wise exclusive OR (<span class="math inline">\(\mathsf{XOR}\)</span>) over <span class="math inline">\(\mathbb{Z}_2\)</span>.</p>
<p>GMW protocol of Goldreich <em>et al.</em> <span class="citation" data-cites="stoc/GoldreichMW87">(<a href="#ref-stoc/GoldreichMW87" role="doc-biblioref">Goldreich, Micali, and Wigderson 1987</a>)</span> can evaluate logic gates (e.g., <span class="math inline">\(\mathsf{AND}\)</span>, <span class="math inline">\(\mathsf{XOR}\)</span>),
simple circuits (e.g., <span class="math inline">\(\mathsf{CMP}\)</span> for comparison), and a composed circuit over them.
To save communication rounds, we may run GMW in parallel for some gates, but its runtime is still linear in the circuit depth.</p>
<h4 id="sec:prelim_gc">Yao’s Garbled Circuit (GC)</h4>
<p>GC <span class="citation" data-cites="focs/Yao86">(<a href="#ref-focs/Yao86" role="doc-biblioref">Yao 1986</a>)</span> also supports oblivious <span class="math inline">\(2\)</span>PC circuit evaluation
over “encrypted” truth tables of gates, with
decryption keys named <em>Yao’s shares</em>.
GC only takes constant communication rounds.</p>
<p>Moreover, the encrypted truth table can be sent in the offline phase.
In the online phase, the garbler only obliviously sends the evaluator the shares for input.
In contrast, GMW requires interaction for each <span class="math inline">\(\mathsf{AND}\)</span> gate.
Thus, GC is thus a better choice than GMW for <span class="math inline">\(2\)</span>PC <span class="citation" data-cites="ndss/DemmlerSZ15">(<a href="#ref-ndss/DemmlerSZ15" role="doc-biblioref">Demmler, Schneider, and Zohner 2015</a>)</span>.
<!-- (see Fig. 3 in [@ndss/DemmlerSZ15]). --></p>
<h4 id="sec:prelim_ot">Oblivious Transfer (OT)</h4>
<p>allows the receiver to pick the <span class="math inline">\(i\)</span>-th item obliviously (hiding <span class="math inline">\(i\)</span>) from the sender’s table of items <span class="math inline">\(A_1, \ldots, A_n\)</span>,
i.e., it realizes <span class="math inline">\(2\)</span>PC <span class="math inline">\(f(\{A_j\}, i) \rightarrow (\bot, A_i)\)</span>.
In OT extension <span class="citation" data-cites="crypto/IshaiKNP03">(<a href="#ref-crypto/IshaiKNP03" role="doc-biblioref">Ishai et al. 2003</a>)</span>, online crypto operations are solely symmetric-key ones, while public-key operations are shifted offline.
OT is a building block for GC and Beaver triplets <span class="citation" data-cites="crypto/Beaver91a">(<a href="#ref-crypto/Beaver91a" role="doc-biblioref">Beaver 1991</a>)</span>.
It also realizes oblivious <em>table lookup</em> with <span class="math inline">\(O(n)\)</span> communication cost <span class="citation" data-cites="ndss/DessoukyKS0ZZ17">(<a href="#ref-ndss/DessoukyKS0ZZ17" role="doc-biblioref">Dessouky et al. 2017</a>)</span>.</p>
<p>With <span class="math inline">\({\geq} 3\)</span> non-colluding servers,
GMW becomes a more popular choice than GC due to the cheap offline phase and the low storage requirement.
The third server can cheaply distribute the beaver triplets instead of running the expensive OTs.
After the offline phase,
GC requires <span class="math inline">\(E\)</span> to store <span class="math inline">\({\approx} 256\)</span>
bits for each <span class="math inline">\(\mathsf{AND}\)</span> gate.
Yet, GMW only requires the parties to store <span class="math inline">\(3\)</span> bits in total.
It thus reduces the parties’ storage pressure stemming from offline preparation.</p>
<h4 id="sec:prelim_he">Homomorphic Encryption (HE)</h4>
<p>supports operations over encrypted messages.
Encryption <span class="math inline">\([x] = \mathsf{Enc}_{\mathsf{pk}}(x)\)</span> of <span class="math inline">\(x\)</span> can be decrypted by secret key <span class="math inline">\(\mathsf{sk}\)</span> when <span class="math inline">\((\mathsf{pk}, \mathsf{sk})\)</span> is a matching key pair.</p>
<p><em>Additive-HE (AHE)</em>, linear-HE, supports additive homomorphism like arithmetic SS.</p>
<p><em>Leveled-HE (LHE)</em>, somewhat-HE,
supports a limited number of multiplications
corresponding to the depth of an <em>arithmetic circuit</em>.
The number is a tunable parameter that affects the ciphertext size and efficiency of the homomorphic operations (<a href="#sec:he-deploy">the Section on HE optimization</a>).</p>
<p>LHE like BFV-HE <span class="citation" data-cites="crypto/Brakerski12 eprint/FanV12">(<a href="#ref-crypto/Brakerski12" role="doc-biblioref">Brakerski 2012</a>; <a href="#ref-eprint/FanV12" role="doc-biblioref">Fan and Vercauteren 2012</a>)</span>,
BGV-HE <span class="citation" data-cites="toct/BrakerskiGV14">(<a href="#ref-toct/BrakerskiGV14" role="doc-biblioref">Brakerski, Gentry, and Vaikuntanathan 2014</a>)</span>,
and CKKS-HE <span class="citation" data-cites="asiacrypt/CheonKKS17">(<a href="#ref-asiacrypt/CheonKKS17" role="doc-biblioref">Cheon et al. 2017</a>)</span>
are
SIMD-friendly,
i.e., a ciphertext has multiple slots, each fits a message in ring <span class="math inline">\(\mathbb{Z}_p\)</span>, and a homomorphic operation
(<span class="math inline">\(\star\)</span>, <span class="math inline">\(\ast\)</span>, and <span class="math inline">\(\circ\)</span>)
applies on all slots simultaneously.</p>
<p>Namely,</p>
<ol type="1">
<li><p>addition: <span class="math inline">\(\mathsf{Dec}([(x_i)_i] \star [(y_i)_i]) = (x_i + y_i)_i\)</span>,</p></li>
<li><p>plaintext-ciphertext multiplication: <span class="math inline">\(\mathsf{Dec}((x_i)_i \circ [(y_i)_i]) = (x_i \cdot y_i)_i\)</span>, and</p></li>
<li><p>ciphertext-ciphertext multiplication: <span class="math inline">\(\mathsf{Dec}([(x_i)_i] \ast [(y_i)_i]) = (x_i \cdot y_i)_i\)</span>.</p></li>
</ol>
<p>(When the context is obvious, we may replace <span class="math inline">\(\circ\)</span> and <span class="math inline">\(\ast\)</span> by <span class="math inline">\(\cdot\)</span> and <span class="math inline">\(\star\)</span> by <span class="math inline">\(+\)</span>.)</p>
<p>For security, they introduce <em>noises</em> to ciphertexts.
Fresh encryption starts with very little noise.
Noises accumulate upon evaluating <span class="math inline">\(\star\)</span>, <span class="math inline">\(\ast\)</span>, and <span class="math inline">\(\circ\)</span>.
If they exceed the <em>noise quota</em>,
decryption will fail.</p>
<p>The quota implicitly defines the number of <span class="math inline">\(\star\)</span>, <span class="math inline">\(\ast\)</span>, and <span class="math inline">\(\circ\)</span> allowed to take.
Yet,
one can ask
the secret key holder to help “denoise” the ciphertext by
adding an additive mask (via <span class="math inline">\(\star\)</span>) without leaking the encrypted message (see <a href="#sec:he-simd">the Section on efficient HE</a>).</p>
<p><em>Two SIMD Layouts.</em>
Consider the central part of a photo is of interest; one ciphertext thus stores the same part of many photos.
Such kind of batch-query processing via SIMD is relevant only when the client always has a batch of queries.
An alternative usage is to handle one single (feature-vector) query at a time, but with its features spread across SIMD slots of a ciphertext.</p>
<p>One may want to homomorphically evaluate over multiple slots of a query (e.g., for a dot product).
A homomorphic <em>rotation</em> over the encrypted query is required.
Rotation is more computationally expensive than plaintext-ciphertext multiplication and addition.
It also increases the noise additively <span class="citation" data-cites="toct/BrakerskiGV14 tr/he-stat">(<a href="#ref-toct/BrakerskiGV14" role="doc-biblioref">Brakerski, Gentry, and Vaikuntanathan 2014</a>; <a href="#ref-tr/he-stat" role="doc-biblioref">D. J. Wu and Haven 2012</a>)</span>.</p>
<p>TFHE <span class="citation" data-cites="joc/ChillottiGGI20">(<a href="#ref-joc/ChillottiGGI20" role="doc-biblioref">Chillotti et al. 2020</a>)</span> is torus-based fully-HE (FHE)
with much faster bootstrap procedures
but slower
addition/multiplication
than BFV-HE and BGV-HE.
TFHE is mainly used to evaluate
<em>binary gates</em> <span class="citation" data-cites="crypto/BourseMMP18 icml/SanyalKGK18 nips/Lou019">(<a href="#ref-crypto/BourseMMP18" role="doc-biblioref">Bourse et al. 2018</a>; <a href="#ref-icml/SanyalKGK18" role="doc-biblioref">Sanyal et al. 2018</a>; <a href="#ref-nips/Lou019" role="doc-biblioref">Lou and Jiang 2019</a>)</span>
with very little communication cost
(proportional to the input size) when compared to
GC/GMW (the whole circuit size),
but it takes a longer computation time.</p>
<p>BFV-HE, BGV-HE, and TFHE are all based on ring-LWE.
A ciphertext under one scheme can be switched to that of another
with the message inside unchanged <span class="citation" data-cites="nutmic/BouraGGJ19">(<a href="#ref-nutmic/BouraGGJ19" role="doc-biblioref">Boura et al. 2019</a>)</span>.</p>
<p><a href="https://github.com/Microsoft/SEAL">Microsoft SEAL</a> implements BFV-HE and CKKS-HE.
<a href="https://github.com/shaih/HElib">HElib</a> implements BGV-HE.
<a href="https://github.com/tfhe/tfhe">TFHE library</a> is on GitHub.</p>
<h4 id="subsec:together">General Comparison of the Gadgets.</h4>
<p>Arithmetic SS operations can be as cheap as a few CPU instructions, but each multiplication takes a communication round.</p>
<p>HE could be a better choice
in general
in a high-latency network as no communication is needed in between.
This motivates pure-HE frameworks
(the Sections <a href="#sec:pure-lhe">on pure-LHE frameworks</a> and <a href="#sec:tfhe">on TFHE frameworks</a>)
and differentiates them from the rest.</p>
<p>LHE and arithmetic SS are born to handle linear layers.
Boolean/circuit-based primitives,
including TFHE, boolean SS, and Yao’s shares,
support arbitrary functions, including non-linear ones.
This motivates <a href="#sec:mixed">the mixed framework</a>.</p>
<p>Arithmetic SS, boolean SS, and Yao’s shares (A/B/Y) can be converted to each other with a few communication rounds (except Y to B, which is free) such that one can use <em>mixed-protocol computation</em> with
the most-fit form for each kind of computation.
ABY <span class="citation" data-cites="ndss/DemmlerSZ15">(<a href="#ref-ndss/DemmlerSZ15" role="doc-biblioref">Demmler, Schneider, and Zohner 2015</a>)</span> is proposed for this purpose.</p>
<p>This leads to interesting interbreeds in mixed and MPC frameworks (<a href="#sec:mpc">the Section on MPC</a>).</p>
<h4 id="subsec:bitwidth-issue">Struggles in Fixed-Points and Limited Bitwidth.</h4>
<p>Most crypto primitives operate over <span class="math inline">\(\mathbb{Z}_p\)</span> for a large prime <span class="math inline">\(p\)</span>,
which we call <em>fixed-points</em>, while
NN runs over floating-point numbers,
which we call <em>floats</em>.</p>
<p>A float <span class="math inline">\(x_f\)</span> can be quantized as fixed-point <span class="math inline">\(x_Q = \lfloor x_f \cdot P_x \rceil\)</span> for the scaling factor <span class="math inline">\(P_x\)</span>.
We can then avoid <em>overflow</em> in, say, dot product by computing over fixed points.
The resulting fixed <span class="math inline">\(y_Q\)</span> can then be dequantized by <span class="math inline">\(y_f = y_Q / P_y\)</span>.</p>
<p><em>Bitwidth</em> is the number of bits needed to represent the operands,
which is logarithmic in the
largest magnitude.</p>
<p>To fully emulate single-precision floats, one may need <span class="math inline">\(256\)</span>-bit fixed points.
More bitwidth means more transferred bytes, CPU/GPU instructions,
larger circuits for GMW/GC, and collectively much higher communication and computation costs.</p>
<p>As a rather fundamental issue,
different classes of frameworks tackle a slightly different form of it in their own ways.</p>
<h2 id="sec:major-approaches">Systematizing Machine learning <span class="math inline">\(\times\)</span> Cryptology</h2>
<p>Crypto operations usually incur high overhead
and tend to attain a lower accuracy
for operating over fixed-points
(e.g., see <a href="#sec:mixed-bitwidth">the Section on mixed bitwidth</a>).</p>
<p>We highlight the initial struggles and some milestones.
A query on MNIST takes CryptoNets <span class="citation" data-cites="icml/Gilad-BachrachD16">(<a href="#ref-icml/Gilad-BachrachD16" role="doc-biblioref">Gilad-Bachrach et al. 2016</a>)</span> <span class="math inline">\(300\)</span>s.
Two years later, Gazelle <span class="citation" data-cites="uss/JuvekarVC18">(<a href="#ref-uss/JuvekarVC18" role="doc-biblioref">Juvekar, Vaikuntanathan, and Chandrakasan 2018</a>)</span> takes only <span class="math inline">\(0.03\)</span>s.
Building upon the success of mixed frameworks, GForce <span class="citation" data-cites="uss/NgC21">(<a href="#ref-uss/NgC21" role="doc-biblioref">Ng and Chow 2021</a>)</span>
can process a query on CIFAR-10, a harder dataset, within <span class="math inline">\(0.3\)</span>s online latency using the popular VGG-16 NN.
For the same dataset, Gazelle takes <span class="math inline">\(3.56\)</span>s with a much smaller, custom NN.</p>
<p>For private training, SecureML <span class="citation" data-cites="sp/MohasselZ17">(<a href="#ref-sp/MohasselZ17" role="doc-biblioref">Mohassel and Zhang 2017</a>)</span> over <span class="math inline">\(2\)</span> non-colluding servers achieves an online training throughput of <span class="math inline">\({\sim}3.3 \times 10^5\)</span> images/hour on MNIST at <span class="math inline">\(93\%\)</span> accuracy.
Trident <span class="citation" data-cites="ndss/RachuriS20">(<a href="#ref-ndss/RachuriS20" role="doc-biblioref">Rachuri and Suresh 2020</a>)</span> over <span class="math inline">\(4\)</span> servers
attained an online throughput <span class="math inline">\({\sim}1.4\times 10^7\)</span> on the same NN.</p>
<p>To support training on more realistic datasets and popular NNs,
CryptGPU <span class="citation" data-cites="sp/TanKTW21">(<a href="#ref-sp/TanKTW21" role="doc-biblioref">Tan et al. 2021</a>)</span> integrates more advanced ML techniques, e.g., BatchNorm and <span class="math inline">\(\mathsf{softmax}\)</span>, and achieves <span class="math inline">\(83.7\%\)</span> accuracy on CIFAR-10 with VGG-16 at a <span class="math inline">\({\sim}4\times 10^4\)</span>
online throughput.</p>
<p>PPNN researchers look into the nature of the operations at various levels and their interactions, and tailor-make protocols optimized under constraints due to many reasons, e.g., scientific computation for the traditional setting (of CPU/GPU instead of crypto layers), the ML expectation, and crypto tools.
At the layer level,
different protocols handle each layer type separately.
Such modular designs allow flexibility over
many architectures.</p>
<p><strong>Roadmap.</strong>
Following sections will discuss <a href="#sec:pure-lhe">pure-LHE-based</a>, <a href="#sec:mixed">mixed</a>, and <a href="#sec:mpc">MPC-based</a> frameworks.
<a href="#sec:deploy">The Section on depolyment</a>
discusses special focuses,
which might interest experts outside crypto communities.
<a href="#sec:benchmark">The Section on benchmarks</a> compares the performance of different frameworks and puts them on the Pareto frontier.
<a href="#sec:sota-diff-setting">The Section on SoTA</a> recommends the state of the arts for specific functions under different settings/assumptions.</p>
<div class="layout-chunk" data-layout="l-body">
<table>
<caption><span id="tab:set">Table 1: </span>Privacy Services Supported by Different Paradigms.</caption>
<colgroup>
<col style="width: 17%" />
<col style="width: 24%" />
<col style="width: 25%" />
<col style="width: 33%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Framework Type</th>
<th style="text-align: center;">Oblivious Inference</th>
<th style="text-align: center;">Outsourced Inference</th>
<th style="text-align: center;">Outsourced/Private Training</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Pure-LHE</td>
<td style="text-align: center;">All</td>
<td style="text-align: center;">Some</td>
<td style="text-align: center;">None</td>
</tr>
<tr class="even">
<td style="text-align: left;">TFHE-based</td>
<td style="text-align: center;">All</td>
<td style="text-align: center;">Some</td>
<td style="text-align: center;">Some</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Pure-GC</td>
<td style="text-align: center;">All</td>
<td style="text-align: center;">Some</td>
<td style="text-align: center;">None</td>
</tr>
<tr class="even">
<td style="text-align: left;">Mixed</td>
<td style="text-align: center;">All</td>
<td style="text-align: center;">None</td>
<td style="text-align: center;">None</td>
</tr>
<tr class="odd">
<td style="text-align: left;">MPC-based</td>
<td style="text-align: center;">All</td>
<td style="text-align: center;">All</td>
<td style="text-align: center;">Some</td>
</tr>
</tbody>
</table>
</div>
<h2 id="sec:pure-lhe">Pure-LHE Frameworks</h2>
<p>Pure-LHE frameworks (e.g., CryptoNets <span class="citation" data-cites="icml/Gilad-BachrachD16">(<a href="#ref-icml/Gilad-BachrachD16" role="doc-biblioref">Gilad-Bachrach et al. 2016</a>)</span>) use LHE as the only crypto tool throughout the entire evaluation of oblivious inference.</p>
<p>The client first encrypts the query <span class="math inline">\(x\)</span> and sends it to the server.
For linear layers with weights <span class="math inline">\(\{w_{ij}\}\)</span>,
the server computes <span class="math inline">\([y_i] = \sum_j w_{ij} \cdot [x_{ij}]\)</span> for
encrypted layer input <span class="math inline">\([x_{ij}]\)</span>.</p>
<p>Some <a href="#sec:mixed">mixed frameworks</a>
also handle linear layers in this way.
(We thus defer to <a href="#sec:he-simd">the Section on efficient HE</a> for LoLa <span class="citation" data-cites="icml/BrutzkusGE19">(<a href="#ref-icml/BrutzkusGE19" role="doc-biblioref">Brutzkus, Gilad-Bachrach, and Elisha 2019</a>)</span>.)
Below, we discuss
“BNormCrypt” <span class="citation" data-cites="eprint/2017:035">(<a href="#ref-eprint/2017:035" role="doc-biblioref">Chabanne et al. 2017</a>)</span>,
CryptoDL <span class="citation" data-cites="popets/HesamifardTGW18">(<a href="#ref-popets/HesamifardTGW18" role="doc-biblioref">Hesamifard et al. 2018</a>)</span>,
FasterCryptoNets <span class="citation" data-cites="corr/fastercryptonets">(<a href="#ref-corr/fastercryptonets" role="doc-biblioref">Chou et al. 2018</a>)</span>,
HCNN <span class="citation" data-cites="tetc/BadawiJLMJTNAC21">(<a href="#ref-tetc/BadawiJLMJTNAC21" role="doc-biblioref">Badawi et al. 2021</a>)</span>,
E2DM <span class="citation" data-cites="ccs/JiangKLS18">(<a href="#ref-ccs/JiangKLS18" role="doc-biblioref">Jiang et al. 2018</a>)</span>,
and Glyph <span class="citation" data-cites="nips/LouFF020">(<a href="#ref-nips/LouFF020" role="doc-biblioref">Lou et al. 2020</a>)</span>
(See <a href="#sec:he-deploy">the Section on HE compilers/optimizers</a>).</p>
<h3 id="major-developments-for-pure-lhe-based-inference">Major Developments for Pure-LHE-based Inference</h3>
<p><strong>Non-linear layers</strong>
pose a challenge to pure-LHE frameworks.
They tend to choose specific functions that can be either supported or approximated by polynomials.
CryptoNets is a
polynomial-network <span class="citation" data-cites="nips/LivniSS14">(<a href="#ref-nips/LivniSS14" role="doc-biblioref">Livni, Shalev-Shwartz, and Shamir 2014</a>)</span> approach, which
uses only squaring <span class="math inline">\([x] \cdot [x]\)</span> as the activation function.
For pooling layers, it only supports <span class="math inline">\(\mathsf{meanpool}\)</span> by <span class="math inline">\([y_i] = \sum_j [x_j]\)</span>.
The client internally maintains the output’s scale after computing each layer (e.g., the output is scaled up by <span class="math inline">\(d\)</span> for the <span class="math inline">\(d\)</span>-size window <span class="math inline">\(\mathsf{meanpool}\)</span>).
Eventually, the server sends the output layer’s <span class="math inline">\([y]\)</span> to the client, who
decrypts it (and divides it by the scale)
to obtain the query result.</p>
<p><strong>Batch normalization</strong> <span class="citation" data-cites="icml/IoffeS15">(<a href="#ref-icml/IoffeS15" role="doc-biblioref">Ioffe and Szegedy 2015</a>)</span> is an ML trick that helps ensure accuracy by normalizing the range of the inputs since polynomial approximation is only accurate within a small range.
“BNormCrypt” <span class="citation" data-cites="eprint/2017:035">(<a href="#ref-eprint/2017:035" role="doc-biblioref">Chabanne et al. 2017</a>)</span> and subsequent works insert <em>BatchNorm layers</em> before activation layers.
BatchNorm in inference can be efficiently done with HE ciphertexts since the computation is basically point-wise (plaintext-ciphertext) multiplication and addition.
BNormCrypt approximates <span class="math inline">\(\mathsf{ReLU}\)</span> by <span class="math inline">\(0.2[x]^2 + 0.5[x] + 0.2\)</span>.</p>
<p><strong>Speeding up Multiplication.</strong>
CryptoDL <span class="citation" data-cites="popets/HesamifardTGW18">(<a href="#ref-popets/HesamifardTGW18" role="doc-biblioref">Hesamifard et al. 2018</a>)</span> exploits various methods for approximation by low-degree (<span class="math inline">\(2{\sim}4\)</span>) polynomials,
including numerical analysis, Taylor series,
and
Chebyshev polynomial, since ciphertext-ciphertext multiplication is not cheap.</p>
<p>FasterCryptoNets <span class="citation" data-cites="corr/fastercryptonets">(<a href="#ref-corr/fastercryptonets" role="doc-biblioref">Chou et al. 2018</a>)</span> uses power-of-<span class="math inline">\(2\)</span> coefficients for the polynomials
to enable more efficient plaintext-ciphertext multiplication.</p>
<p>It also quantizes the weights of linear layers
to power-of-<span class="math inline">\(2\)</span>
for more efficient multiplication.</p>
<p><strong>Pruning</strong>
sets some weights in a trained NN to <span class="math inline">\(0\)</span>.
Pure-LHE frameworks above
dedicate each HE ciphertext to a specific entry across all queries in a batch;
each entry is then multiplied by the same weight.</p>
<p>These frameworks (e.g., <span class="citation" data-cites="corr/fastercryptonets">(<a href="#ref-corr/fastercryptonets" role="doc-biblioref">Chou et al. 2018</a>)</span>) can benefit from pruning by skipping the multiplication of the ciphertext and the zeroed weights,
which can skip up to <span class="math inline">\({&gt;}90\%\)</span> of those.</p>
<p><strong>GPU for HE.</strong>
HCNN <span class="citation" data-cites="tetc/BadawiJLMJTNAC21">(<a href="#ref-tetc/BadawiJLMJTNAC21" role="doc-biblioref">Badawi et al. 2021</a>)</span> is like CryptoNets.
They both use BFV-HE for oblivious inference,
but HCNN implements A*FV <span class="citation" data-cites="tches/BadawiVMA18">(<a href="#ref-tches/BadawiVMA18" role="doc-biblioref">Badawi et al. 2018</a>)</span>, a variant that better utilizes GPU’s capability.</p>
<h3 id="outsource-without-non-colluding">Outsourced Inference and Training without non-collusion</h3>
<p>Notably, HE can support outsourced inference and training without non-colluding assumptions.
For example, E2DM <span class="citation" data-cites="ccs/JiangKLS18">(<a href="#ref-ccs/JiangKLS18" role="doc-biblioref">Jiang et al. 2018</a>)</span> uses only
cipher-cipher multiplications over private weights.
It only demonstrated reasonable performance on NNs with square activation and a few fully-connected layers.
Deeper NNs with convolutions require more costly bootstrapping and rotations.</p>
<p>Glyph <span class="citation" data-cites="nips/LouFF020">(<a href="#ref-nips/LouFF020" role="doc-biblioref">Lou et al. 2020</a>)</span> supports outsourced training.
It adopts transfer learning to reduce the costly homomorphic operations.
It trains many layers close to the input layer of an NN
with publicly available data
and only uses
HE for training the remaining layers with private data.</p>
<p>Glyph does not consider (multi-party) private training
as the single secret key owner,
effectively <em>the</em> data owner, can decrypt the ciphertexts and acquire all training data.
One could use
heavyweight multi-key FHE.</p>
<p>We thus ask:</p>
<p><strong>Open Problem 1.</strong>
<em>How to realize high-throughput and accurate private training without non-colluding assumptions?</em></p>
<p>Section <a href="#sec:mpc">MPC frameworks</a> will explore such “non-colluding” frameworks.</p>
<p>To sum up, the distinctive feature of the pure-LHE framework is the use of polynomial networks.
This begs the question:</p>
<p><strong>Open Problem 2.</strong>
<em>What positive/negative results can we establish on the sample complexity, expressiveness, and (the practical) training time for networks based on polynomials?</em></p>
<p>Linvi <em>et al.</em> <span class="citation" data-cites="nips/LivniSS14">(<a href="#ref-nips/LivniSS14" role="doc-biblioref">Livni, Shalev-Shwartz, and Shamir 2014</a>)</span> provided some results.
Understanding more about deep learning still needs more research <span class="citation" data-cites="cacm/ZhangBHRV21">(<a href="#ref-cacm/ZhangBHRV21" role="doc-biblioref">C. Zhang et al. 2021</a>)</span>.
In contrast, Sections <a href="#sec:comparison"><span class="math inline">\(\mathsf{CMP}\)</span></a> and <a href="#sec:non-linear">non-linear layers</a> explore non-polynomial approaches.</p>
<h2 id="sec:circuit">Boolean-Circuit-based Approaches</h2>
<h3 id="sec:tfhe">TFHE-based (Inference) Frameworks</h3>
<p>To avoid the higher cost of
homomorphic arithmetic operations, an alternative is TFHE <span class="citation" data-cites="joc/ChillottiGGI20">(<a href="#ref-joc/ChillottiGGI20" role="doc-biblioref">Chillotti et al. 2020</a>)</span>, optimized for binary gates.
Glyph <span class="citation" data-cites="nips/LouFF020">(<a href="#ref-nips/LouFF020" role="doc-biblioref">Lou et al. 2020</a>)</span> switches between different kinds of ring-LWE-based HE schemes.
It uses BGV-HE <span class="citation" data-cites="toct/BrakerskiGV14">(<a href="#ref-toct/BrakerskiGV14" role="doc-biblioref">Brakerski, Gentry, and Vaikuntanathan 2014</a>)</span> for linear arithmetics and TFHE for de-quantization and non-linear layers.</p>
<p>Binarized neural network (BNN) <span class="citation" data-cites="nips/HubaraCSEB16">(<a href="#ref-nips/HubaraCSEB16" role="doc-biblioref">Hubara et al. 2016</a>)</span> confines its weights and inputs of linear layers to <span class="math inline">\(\{-1, 1\}\)</span>.
It uses <span class="math inline">\(\mathsf{Sign}(x)=|x|/x\)</span> as the activation function,
which is easy to implement in boolean circuits.
FHE-DiNN <span class="citation" data-cites="crypto/BourseMMP18">(<a href="#ref-crypto/BourseMMP18" role="doc-biblioref">Bourse et al. 2018</a>)</span> implements BNN on TFHE <span class="citation" data-cites="joc/ChillottiGGI20">(<a href="#ref-joc/ChillottiGGI20" role="doc-biblioref">Chillotti et al. 2020</a>)</span>.
It can perform inference on MNIST in <span class="math inline">\({&lt;}1\)</span>s, but its accuracy is much lower than CryptoNets since it only supports small BNNs.
TAPAS <span class="citation" data-cites="icml/SanyalKGK18">(<a href="#ref-icml/SanyalKGK18" role="doc-biblioref">Sanyal et al. 2018</a>)</span> also uses TFHE over BNN.
It implements ripple carry adders for linear layers in a recursive manner to avoid errors due to accumulated noise in TFHE.</p>
<p>However,
to maintain a reasonable accuracy level,
BNN often requires scaling the architecture,
i.e., having more outputs in each layer,
which enlarges the circuits and cancels out part of the performance gain.</p>
<p>Unlike TAPAS,
SHE <span class="citation" data-cites="nips/Lou019">(<a href="#ref-nips/Lou019" role="doc-biblioref">Lou and Jiang 2019</a>)</span> supports most common NNs using TFHE.
It implements <span class="math inline">\(\mathsf{ReLU}\)</span> and maxpool as binary circuits.
It also uses the power-of-<span class="math inline">\(2\)</span> idea <span class="citation" data-cites="corr/fastercryptonets">(<a href="#ref-corr/fastercryptonets" role="doc-biblioref">Chou et al. 2018</a>)</span>,
and running shifting circuits in TFHE is efficient.</p>
<p>To efficiently handle the addition after the multiplication, it reduces the circuit size via a mixed-bit-width accumulator, whose size is smaller when the input size is smaller.
SHE adopts an intra-layer mixed-bitwidth design for smaller circuits, which also allows the bitwidth to vary across layers to balance the performance and accuracy.</p>
<h3 id="sec:pure-gc">Pure-GC (Inference) Frameworks</h3>
<p>The major hurdle of circuit-based approaches is that the circuits (even for linear layers) are bulky
and incur a large overhead.</p>
<p>DeepSecure <span class="citation" data-cites="dac/RouhaniRK18">(<a href="#ref-dac/RouhaniRK18" role="doc-biblioref">Rouhani, Riazi, and Koushanfar 2018</a>)</span> uses only GC.
It decomposes the input (linear) layer into lower-rank matrices and releases one of them
to the client for computing a low-rank projection (of the input).
It also uses circuit optimization tools to
approximate activation
with fewer gates
and
skip computation for pruned neurons (in a pruned NN).</p>
<p>XONN <span class="citation" data-cites="uss/RiaziS0LLK19">(<a href="#ref-uss/RiaziS0LLK19" role="doc-biblioref">Riazi et al. 2019</a>)</span> is dedicated to BNN with pruning applied.
It implements XNOR (<span class="math inline">\(\odot\)</span>) in GC since BNN uses it to count the number of bits after multiplication.</p>
<p>“GarbledNN” <span class="citation" data-cites="eprint:GarbledNN">(<a href="#ref-eprint:GarbledNN" role="doc-biblioref">Ball et al. 2019</a>)</span> aims to show that GC can be applied to not only BNN but also “off-the-shelf” NNs, e.g., the TensorFlow model.
It uses arithmetic garbling techniques and optimizes for the <span class="math inline">\(\mathsf{sign}\)</span> function (and <span class="math inline">\(\mathsf{ReLU}\)</span>).</p>
<p>Namely, it uses a <em>stochastic approach</em> that simplifies comparison gates (<span class="math inline">\(\mathsf{CMP}\)</span>) in GC by tolerating an error probability <span class="math inline">\({&lt;}0.1\%\)</span>.
Yet, this simplified circuit can only work on <span class="math inline">\(\mathbb{Z}_N\)</span>, where <span class="math inline">\(N\)</span> is a product of many distinct small primes.
GarbledNN outperforms DeepSecure, but it still takes <span class="math inline">\({\sim}15\)</span> minutes to run an NN with only <span class="math inline">\(6\)</span> linear layers on CIFAR-10.
As an approach orthogonal to XONN (for BNN), one might consider combining optimization tricks of both.</p>
<p><strong>Open Problem 3.</strong>
Tailoring ML techniques for compatibility with cryptographic techniques (vs. the other way round, e.g., HE for linear and GC for non-linear layers, which many earlier PPNN works aim at) is a promising direction.
We ask:</p>
<p><em>“Apart from BNN (and its generalization QNN to be discussed later), any ML techniques can help cryptographic NN?”</em></p>
<h2 id="sec:mixed">Mixed Frameworks for Oblivious Inference</h2>
<p>Many oblivious inference frameworks use both HE and SS and/or GC
for the best of both worlds, e.g., reduced runtime.
We first explore SS-based computation
(Delphi, CrypTFlow2, Circa, GForce) before revisiting HE (Gazelle, GALA, “FalconI”).</p>
<h3 id="sec:off-on-share">Offline/Online Share Computation for <span class="math inline">\(2\)</span>PC (over GPU)</h3>
<p>MiniONN <span class="citation" data-cites="ccs/LiuJLA17">(<a href="#ref-ccs/LiuJLA17" role="doc-biblioref">Liu et al. 2017</a>)</span> uses the offline/online trick for computing
linear layers <span class="math inline">\(\langle y_i \rangle = \sum_j w_{ij} \cdot \langle x_{ij} \rangle\)</span> over arithmetic SS
and weights <span class="math inline">\(w_{ij}\)</span> known to the server.</p>
<p>In the offline phase, the client sends <span class="math inline">\(\{[\langle x_j \rangle_1]\}_j\)</span>
encrypting random <span class="math inline">\(\{\langle x_j \rangle_1\}\)</span> to the server,
which then returns <span class="math inline">\(\sum_j w_j \cdot [\langle x_j \rangle_1] - r&#39;\)</span> for a random <span class="math inline">\(r&#39;\)</span>.
The client decrypts it to get <span class="math inline">\(\langle y \rangle_0 = \sum_j w_j \cdot \langle x_j \rangle_1 - r&#39;\)</span>.
In the online phase, the client with input <span class="math inline">\(\{x_j\}_j\)</span> sends <span class="math inline">\(\{\langle x_j \rangle_0 = x_j - \langle x_j \rangle_1\}_j\)</span> to the server, which then computes <span class="math inline">\(\langle y \rangle_1 = \sum_j w_j \cdot \langle x_j \rangle_0 + r&#39;\)</span>.
We call it <em>secure online/offline share computation (SOS)</em>.
Note that <span class="math inline">\(\{\langle x_j \rangle_1\}\)</span> can be “reused” for different weights.</p>
<p>SOS-style multiplication of <span class="math inline">\(\ell\)</span>-bit integers <span class="citation" data-cites="sp/MohasselZ17 ccs/RatheeR0CGR020 ccs/HussainJSK21">(<a href="#ref-sp/MohasselZ17" role="doc-biblioref">Mohassel and Zhang 2017</a>; <a href="#ref-ccs/RatheeR0CGR020" role="doc-biblioref">Rathee et al. 2020</a>; <a href="#ref-ccs/HussainJSK21" role="doc-biblioref">Hussain et al. 2021</a>)</span> can also be done by OT.
The client picks <span class="math inline">\(r&#39;_{i}\)</span>
and sets <span class="math inline">\(A_i = (r - r&#39;_i, -r&#39;_i), \forall i \in [0, \ell-1]\)</span> for random <span class="math inline">\(\langle x \rangle_1 = r\)</span>.
For each bit <span class="math inline">\(w_i\)</span> of <span class="math inline">\(w\)</span> in binary, the server gets <span class="math inline">\(A_i[w_i]\)</span> via OT and computes
<span class="math inline">\(\langle w \cdot r \rangle_0 = \sum_{i=0}^{\ell-1}{2^iA_i[w_i]} = w\cdot r - \langle y \rangle_1\)</span>.
The client then computes <span class="math inline">\(\langle y \rangle_1 = \langle w \cdot r \rangle_1 = \sum_{i=0}^{\ell-1} 2^i r&#39;_i\)</span>.
(During the online phase, the server computes <span class="math inline">\(\langle y \rangle_0 = w \cdot \langle x \rangle_0 + \langle w \cdot r \rangle_0\)</span>.)
Since each OT
sends <span class="math inline">\({\approx}\ell\)</span> bits
(after using correlated OTs and packing multiple instances for cost amortization),
a multiplication takes <span class="math inline">\(O(\ell^2)\)</span> bits.</p>
<p>Generalizing to matrices/tensors,
the client/server obtains <span class="math inline">\(\langle Y \rangle_0 = W \otimes R - R&#39;\)</span> or
<span class="math inline">\(\langle Y \rangle_1 = W \otimes (X - R) + R&#39;\)</span>, respectively.</p>
<p>For repeated dot products,
multiple OTs are needed for varying weights,
which takes more communication than using HE.</p>
<p><strong>GPU for SS.</strong>
In SOS, the client can generate an AHE of an additive mask
offline.
Once the input arrives, an SS-based protocol is run on the client’s masked input <span class="math inline">\(\langle X \rangle\)</span>.
The server in Delphi <span class="citation" data-cites="uss/MishraLSZP20">(<a href="#ref-uss/MishraLSZP20" role="doc-biblioref">Mishra et al. 2020</a>)</span> and GForce <span class="citation" data-cites="uss/NgC21">(<a href="#ref-uss/NgC21" role="doc-biblioref">Ng and Chow 2021</a>)</span>
can use GPU to compute
<span class="math inline">\(W \otimes \langle X \rangle\)</span>
with performance close to plaintext computation.</p>
<p>This observation generalizes SOS to the <em>AHE-to-SOS</em> trick <span class="citation" data-cites="uss/NgC21">(<a href="#ref-uss/NgC21" role="doc-biblioref">Ng and Chow 2021</a>)</span> for many compatible operations (e.g., truncation and GForce’s wrap-around protocol below) and non-linear layers.
Also, the offline protocol can be optimized separately (see <a href="#sec:he-simd">the Section on efficient HE</a>).</p>
<h3 id="sec:comparison">Comparison Functions for <span class="math inline">\(\mathsf{sign}\)</span>, <span class="math inline">\(\mathsf{max}\)</span>, and <span class="math inline">\(\mathsf{ReLU}\)</span></h3>
<p>Non-linear layers in many popular NNs,
e.g.,
VGG and ResNet,
only use <span class="math inline">\(\mathsf{ReLU}\)</span> and <span class="math inline">\(\mathsf{maxpool}\)</span>,
which essentially compute <span class="math inline">\(\langle \mathsf{max}(x, y) \rangle = \mathsf{CMP}_{x \leq_? y}(\langle x \rangle, \langle y \rangle) \cdot \langle y-x \rangle + \langle x \rangle\)</span> (expressed in SS) using
a <em>secure comparison protocol</em> <span class="math inline">\(\mathsf{CMP}\)</span>.</p>
<p><span class="math inline">\(\mathsf{CMP}\)</span> is also called the derivative of <span class="math inline">\(\mathsf{ReLU}\)</span> <span class="citation" data-cites="popets/WaghGC19 popets/WaghTBKMR21">(<a href="#ref-popets/WaghGC19" role="doc-biblioref">Wagh, Gupta, and Chandran 2019</a>; <a href="#ref-popets/WaghTBKMR21" role="doc-biblioref">Wagh et al. 2021</a>)</span>
or <em>bit-extraction</em>
for its equivalence to
extract the MSB of (<span class="math inline">\(y - x)\)</span>.</p>
<p><strong><span class="math inline">\(\mathsf{CMP}\)</span> via GC.</strong>
<span class="math inline">\(\mathsf{ReLU}\)</span> and <span class="math inline">\(\mathsf{maxpool}\)</span> can be implemented by circuits with the comparison gate for <span class="math inline">\(\mathsf{CMP}\)</span> and multiplexer via the Beaver’s trick after the Yao-to-Arithmetic (Y2A) conversion of ABY <span class="citation" data-cites="ndss/DemmlerSZ15">(<a href="#ref-ndss/DemmlerSZ15" role="doc-biblioref">Demmler, Schneider, and Zohner 2015</a>)</span>.</p>
<p>MiniONN computes <span class="math inline">\(\langle \mathsf{ReLU}(x) \rangle = \mathsf{CMP}(\langle x \rangle, 0) \cdot \langle x \rangle\)</span>.
Gazelle <span class="citation" data-cites="uss/JuvekarVC18">(<a href="#ref-uss/JuvekarVC18" role="doc-biblioref">Juvekar, Vaikuntanathan, and Chandrakasan 2018</a>)</span>
computes the entire <span class="math inline">\(\langle \mathsf{ReLU}(x) \rangle\)</span> in GC.
Many later works focus on optimizing the expensive <span class="math inline">\(\mathsf{CMP}\)</span>.</p>
<p><strong>Selective Approximation.</strong>
The online latency due to linear layers is <span class="math inline">\({&lt;}0.1\)</span> seconds
by using GPU in Delphi.
The bottleneck lies in non-linear layers.
Delphi computes some <span class="math inline">\(\mathsf{ReLU}\)</span> layers by GC but others (strategically decided by a planner) by <span class="math inline">\(x^2\)</span>-approximation.
Accuracy is still degraded <span class="citation" data-cites="uss/NgC21">(<a href="#ref-uss/NgC21" role="doc-biblioref">Ng and Chow 2021</a>)</span>,
albeit
confined.</p>
<p><strong>Stochastic Approaches.</strong>
Circa <span class="citation" data-cites="nips/GhodsiJ21">(<a href="#ref-nips/GhodsiJ21" role="doc-biblioref">Ghodsi et al. 2021</a>)</span> simplifies and thus accelerates <span class="math inline">\(\mathsf{CMP}\)</span> in GC,
and truncates its input to suppress its error probability, which is linear in the input’s absolute value.</p>
<p><strong>DGK for GPU.</strong>
DGK protocol of Damgard <em>et al.</em> <span class="citation" data-cites="acisp/DamgardGK07">(<a href="#ref-acisp/DamgardGK07" role="doc-biblioref">Damgård, Geisler, and Krøigaard 2007</a>)</span>
compares two HE-encrypted integers and outputs a result bit.
Its naive implementation is slower than GC/GMW’s <span class="math inline">\(\mathsf{CMP}\)</span>.
GForce <span class="citation" data-cites="uss/NgC21">(<a href="#ref-uss/NgC21" role="doc-biblioref">Ng and Chow 2021</a>)</span> transforms DGK
into a bunch of linear operations and fits them with its AHE-to-SOS trick,
which outperforms Gazelle <span class="citation" data-cites="uss/JuvekarVC18">(<a href="#ref-uss/JuvekarVC18" role="doc-biblioref">Juvekar, Vaikuntanathan, and Chandrakasan 2018</a>)</span> and Delphi <span class="citation" data-cites="uss/MishraLSZP20">(<a href="#ref-uss/MishraLSZP20" role="doc-biblioref">Mishra et al. 2020</a>)</span>,
also without any approximation.</p>
<p><strong>An OT-based approach</strong> is proposed by
CryptFlow2 <span class="citation" data-cites="ccs/RatheeR0CGR020">(<a href="#ref-ccs/RatheeR0CGR020" role="doc-biblioref">Rathee et al. 2020</a>)</span>.
We discuss a simplified version:
For <span class="math inline">\(x, y \in \mathbb{Z}_{2^\ell}\)</span> respectively held by party <span class="math inline">\(P_0\)</span> and <span class="math inline">\(P_1\)</span>,
<span class="math inline">\(P_1\)</span> can <em>obliviously</em> select <span class="math inline">\(\langle x \leq_? y \rangle_1\)</span> from <span class="math inline">\(P_0\)</span> with <span class="math inline">\(O(2^\ell)\)</span> communication for bitwidth <span class="math inline">\(\ell\)</span>.
To reduce it, CryptFlow2 decomposes <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> into <span class="math inline">\(k\)</span> equal parts, e.g., <span class="math inline">\(x=x_k||\cdots||x_1\)</span>,
and uses OT for oblivious selection over each pair of <span class="math inline">\((x_i, y_i)\)</span>.</p>
<p>Finally, they run a GMW-like <span class="math inline">\(\mathsf{CMP}\)</span> over <span class="math inline">\(\{\langle x_i \leq_? y_i \rangle\}_1^{\ell/k}\)</span> and <span class="math inline">\(\{\langle x_i =_? y_i \rangle\}_1^{\ell/k}\)</span> to get <span class="math inline">\(\langle x \leq_? y \rangle\)</span>.
Now the new cost is <span class="math inline">\(O(2^{\ell/k} \cdot k)\)</span>.
With a proper <span class="math inline">\(k\)</span>, the cost can be lower than the typical GC/GMW approach, as OT is generally cheaper.
This protocol does not support preprocessing like GForce,
but the overall communication cost is modest (<span class="math inline">\(90{\sim}120\)</span> bits per input bit).</p>
<p>Below we pose a cryptographic challenge on the surface, which could be a scientific computation problem in disguise.</p>
<p><strong>Challenge 1.</strong>
<em>How to implement an even more efficient <span class="math inline">\(\mathsf{CMP}\)</span>?</em></p>
<h3 id="sec:mixed-bitwidth">Bitwidth and Overflow Issues, Truncation and Wrap-Around</h3>
<p>Dot products over fixed-points
would easily overflow,
i.e., the absolute value exceeds <span class="math inline">\(p/2\)</span> for a <span class="math inline">\(p\)</span>-sized plaintext space.</p>
<p>As the most significant (<span class="math inline">\({\sim}8\)</span>) bits matter more empirically,
<em>na"ive truncation</em> is done by
MiniONN, SecureML, and some subsequent (<span class="math inline">\(3\)</span>PC)
frameworks using SS (e.g., <span class="citation" data-cites="uss/JuvekarVC18 uss/MishraLSZP20">(<a href="#ref-uss/JuvekarVC18" role="doc-biblioref">Juvekar, Vaikuntanathan, and Chandrakasan 2018</a>; <a href="#ref-uss/MishraLSZP20" role="doc-biblioref">Mishra et al. 2020</a>)</span>).
To truncate
<span class="math inline">\(m\)</span> bits,
the server and the client locally divide their respective arithmetic SS
<span class="math inline">\(\langle x \rangle_S = x - r \bmod p\)</span> and <span class="math inline">\(\langle x \rangle_C = r \bmod p\)</span>
by <span class="math inline">\(2^m\)</span>.</p>
<p>However, an error occurs when a wrap-around happens in the secret share, i.e., <span class="math inline">\(x &lt; r\)</span>, so <span class="math inline">\(\langle x \rangle_0 = x - r + p\)</span>.
In this case, <span class="math inline">\(\langle x \rangle_0 / 2^m + \langle x \rangle_1 /2^m = (x + p)/2^m \neq x/2^m\)</span>.
The error probability is proportional to the message’s magnitude and <span class="math inline">\(p\)</span>.
<span class="math inline">\(p\)</span> needs to be large to suppress
the error probability.
The increased bitwidth (i.e., <span class="math inline">\(\lceil \log_2 p \rceil\)</span>) leads to a higher communication cost and bulky circuits in GC/GMW for non-linear layers.</p>
<p><strong>Truncation protocol</strong> of
GForce <span class="citation" data-cites="uss/NgC21">(<a href="#ref-uss/NgC21" role="doc-biblioref">Ng and Chow 2021</a>)</span> outputs <span class="math inline">\(\langle x/2^m \rangle = \langle x \rangle/2^m + \langle b \rangle \cdot \lfloor p/2^m \rceil\)</span>, which offsets the wrap-around error by a wrap-around detection sub-protocol that outputs <span class="math inline">\(\langle b \rangle = 1\)</span> if <span class="math inline">\(x &lt; r\)</span>; <span class="math inline">\(0\)</span> otherwise.
There still could be an off-by-one error,
but the impact on inference accuracy is
negligible <span class="citation" data-cites="uss/NgC21 popets/Dalskov0K20">(<a href="#ref-uss/NgC21" role="doc-biblioref">Ng and Chow 2021</a>; <a href="#ref-popets/Dalskov0K20" role="doc-biblioref">Dalskov, Escudero, and Keller 2020</a>)</span>.</p>
<p>GForce proposes an efficient sub-protocol that computes <span class="math inline">\(\langle b \rangle\)</span>
with a constant multiplication (<span class="math inline">\(W\)</span>) over additive SS (<span class="math inline">\(\langle X \rangle\)</span>).
It leverages the fact
that wrap-around happens when the following conditions are both met: <span class="math inline">\(\langle x \rangle_0 &gt; p/2\)</span> and <span class="math inline">\(\langle x \rangle_1 &lt; p/2\)</span>, assuming that <span class="math inline">\(x &lt; p/2\)</span> <span class="citation" data-cites="wifs/Veugen12">(<a href="#ref-wifs/Veugen12" role="doc-biblioref">Veugen 2012</a>)</span>.
This assumption can be met by swapping the <span class="math inline">\(\mathsf{ReLU}\)</span> layer before the truncation
or offsetting <span class="math inline">\(x\)</span> by a large public constant <span class="math inline">\(L\)</span> first and then subtracting the truncated value by <span class="math inline">\(L/2^m\)</span>.</p>
<p>The server generates a random SS <span class="math inline">\(\langle x \rangle_1 = r\)</span> offline and eventually derives
<span class="math inline">\(\langle b \rangle = W\langle X \rangle\)</span> where <span class="math inline">\(W = (\langle x \rangle_1 &lt;_? p/2)\)</span> and <span class="math inline">\(\langle X \rangle = \langle \langle x \rangle_0 &gt;_? p/2 \rangle\)</span>
using the AHE-to-SOS trick.
Since the client knows <span class="math inline">\(\langle x \rangle_0\)</span>, it can locally compute <span class="math inline">\(X\)</span> and send <span class="math inline">\(\langle X \rangle_1\)</span> to the server with an additive mask.</p>
<p>Most MPC-based frameworks <a href="#sec:mpc">for MPC frameworks</a> also face the fixed-point issues and need truncation after dot products.
As they propose their own special kind of SS,
they also come with their truncation protocols.</p>
<h3 id="quantized-neural-network-qnn">Quantized Neural Network (QNN)</h3>
<p>Inputs, (intermediate) outputs, and weights
of a QNN <span class="citation" data-cites="jmlr/HubaraCSEB17">(<a href="#ref-jmlr/HubaraCSEB17" role="doc-biblioref">Hubara et al. 2017</a>)</span>
are
all fixed-points.
QNN can be trained from scratch or post-training quantization (quantizing a trained DNN in floats),
possibly fine-tuned using training data.</p>
<p>QNNs are promising alternatives
for crypto-processing
as they tolerate limited-precision (<span class="math inline">\({\sim}8\)</span>-bit) computation.</p>
<p>We highlight two works at two ends of the spectrum in a sense.
“QuantizedNN” <span class="citation" data-cites="popets/Dalskov0K20">(<a href="#ref-popets/Dalskov0K20" role="doc-biblioref">Dalskov, Escudero, and Keller 2020</a>)</span> directly adopted a quantized inference scheme <span class="citation" data-cites="cvpr/JacobKCZTHAK18">(<a href="#ref-cvpr/JacobKCZTHAK18" role="doc-biblioref">Jacob et al. 2018</a>)</span> (originally designed for reducing the model size) to simplify the arithmetics and activations.</p>
<p>GForce <span class="citation" data-cites="uss/NgC21">(<a href="#ref-uss/NgC21" role="doc-biblioref">Ng and Chow 2021</a>)</span> adopts SWALP
(<strong>s</strong>tochastic <strong>w</strong>eight <strong>a</strong>veraging in <strong>l</strong>ow-<strong>p</strong>recision training) <span class="citation" data-cites="icml/YangZKBWS19">(<a href="#ref-icml/YangZKBWS19" role="doc-biblioref">G. Yang et al. 2019</a>)</span>
that trains a quantized model for inference with <span class="math inline">\({&lt;}1\)</span> percentage point (pp) of accuracy drop,
further abstracts the (de)quantization to “SRT layers,”
with a more efficient truncation protocol,
and analytically proves that the output distribution is close to such <strong>s</strong>tochastic <strong>r</strong>ounding after <strong>t</strong>runcation.
Interestingly, SRT is also GPU-friendly using the AHE-to-SOS trick.</p>
<p>With what the two previous subsections discussed, we ask:</p>
<p><strong>Challenge 2.</strong>
<em>How to utilize GPU for better performance, possibly with tricks to deal with the low-bitwidth constraint?</em></p>
<p><strong>Challenge 3.</strong>
<em>Is there any other secure protocol for non-linear functions that can be made to fit with the AHE-to-SOS trick?</em></p>
<h3 id="sec:he-simd">Improving Efficiency of HE Operations</h3>
<p>SOS computation reduces only online latency.
Many mixed frameworks use various HE tricks to reduce the overall latency.</p>
<p><strong>Denoising.</strong>
Gazelle uses tight HE parameters to reduce latency, imposing a tight noise quota over HE operations (also see <a href="#sec:he-deploy">the Section on HE Optimization</a>).
After each linear layer,
it “refreshes” <span class="math inline">\([y + r]\)</span>, where <span class="math inline">\(y\)</span> is the intermediate result, and <span class="math inline">\(r\)</span> is a mask from additive SS,
by sending it to the client to decrypt and
re-encrypt it to clear the noise (vs. running the entire NN without the client).
This requires a good network condition, and the client must remain online.</p>
<p>A tempting trick is to let the client receive <span class="math inline">\(y\)</span>, possibly with “noise” (but not masks) injected for efficient non-linear computation.</p>
<p>Such heuristics were quickly proven insecure as a malicious client can average out the noise with multiple queries <span class="citation" data-cites="ijcai/WongMWNC20">(<a href="#ref-ijcai/WongMWNC20" role="doc-biblioref">Wong et al. 2020</a>)</span>.</p>
<p><strong>Optimizing/Reducing Rotations.</strong>
SIMD is widely adopted, e.g. <span class="citation" data-cites="ccs/LiuJLA17">(<a href="#ref-ccs/LiuJLA17" role="doc-biblioref">Liu et al. 2017</a>)</span>, but its rotation is not cheap (<a href="#sec:prelim_he">the Section on HE background knowledge</a>).</p>
<p>Gazelle handles linear layers by dedicating all SIMD slots in an HE ciphertext to a single inference query to reduce its latency.</p>
<p>To align the slots for summation, it proposes a slot encoding and rotation scheme to reduce the number of ciphertexts the server sends back.</p>
<p>LoLa <span class="citation" data-cites="icml/BrutzkusGE19">(<a href="#ref-icml/BrutzkusGE19" role="doc-biblioref">Brutzkus, Gilad-Bachrach, and Elisha 2019</a>)</span> (a pure-LHE framework) also uses SIMD in the same way while minimizing the rotation cost according to the overall NN architecture.</p>
<p>GALA <span class="citation" data-cites="ndss/ZhangXW21">(<a href="#ref-ndss/ZhangXW21" role="doc-biblioref">Q. Zhang, Xin, and Wu 2021</a>)</span> further reduces the use of rotations. First, some can be cheaply replaced by rotating the plaintext weights during encoding.</p>
<p>Second, the server can skip some rotation-and-sum on the ciphertexts
by sending intermediate encrypted results with an additive mask to the clients, who will then decrypt them and (cheaply) sum the
additively masked values during denoising.</p>
<p>“FalconI” <span class="citation" data-cites="cvpr/LiXZDGWW20">(<a href="#ref-cvpr/LiXZDGWW20" role="doc-biblioref">Li et al. 2020</a>)</span> applies Fast Fourier Transform (FFT) on both weight tensors and input ciphertexts. Convolution then becomes point-wise multiplication. This approach avoids rotations. However, it is only suitable for (interactive) S/C frameworks and not pure-HE frameworks because it requires the client to perform inverse FFT on the resulting SS (converted from HE ciphertexts) for the subsequent non-linear layers.</p>
<p>Interestingly, this CVPR paper <span class="citation" data-cites="cvpr/LiXZDGWW20">(<a href="#ref-cvpr/LiXZDGWW20" role="doc-biblioref">Li et al. 2020</a>)</span> did not emphasize that it is motivated by avoiding rotation.
This inspires the challenge and open problem below, which probably needs insights into the NN computations.</p>
<p><strong>Challenge 4.</strong>
<em>How to reduce the costs of HE SIMD/rotations?</em></p>
<p><strong>Open Problem 4.</strong>
<em>We see various improvements in the mixed approach. Are there other scientific computation/machine learning techniques with cryptographic implications to explore?</em></p>
<h2 id="sec:mpc">MPC-based Inference/Training Frameworks</h2>
<p>Many frameworks employ multiple non-colluding servers to run MPC.
SS operations for MPC are very efficient.
In general, they have the best performance.
Many support private training.</p>
<h3 id="major-developments-for-mpc-based-inference">Major Developments for MPC-based Inference</h3>
<p>To support outsourced inference,
SecureML <span class="citation" data-cites="sp/MohasselZ17">(<a href="#ref-sp/MohasselZ17" role="doc-biblioref">Mohassel and Zhang 2017</a>)</span> distributes the arithmetic shares of the weights to <span class="math inline">\(2\)</span> servers to evaluate the matrix multiplication or convolution via Beaver’s trick.
It evaluates non-linear layers via GC or piecewise linear approximation (see <a href="#sec:non-linear">the Section on non-linear layers</a>).
It also supports training over SS.</p>
<p>ABY2.0 <span class="citation" data-cites="uss/Patra0SY21">(<a href="#ref-uss/Patra0SY21" role="doc-biblioref">Patra et al. 2021</a>)</span> supports more (online-)communication-efficient and higher fan-in multiplication and <span class="math inline">\(\mathsf{CMP}\)</span>
by using a new kind of additive SS.</p>
<p><strong>Say No to Beaver.</strong>
Quotient <span class="citation" data-cites="ccs/AgrawalSKG19">(<a href="#ref-ccs/AgrawalSKG19" role="doc-biblioref">Agrawal et al. 2019</a>)</span> uses OT and GC to avoid (the expensive offline phase for) Beaver triplets, which improves the overall throughput.
It is specifically designed for ternarized NNs with weights confined to <span class="math inline">\(\{-1, 0, 1\}\)</span>.
However, Quotient’s efficiency and accuracy are only demonstrated on small NNs with a few linear layers (using matrix multiplication).</p>
<p><strong>More Non-colluding Servers.</strong></p>
<p>Having more servers allows more options, e.g.,
a third server can aid in preparing triplets without interactions between two servers.
But it increases the risk.
They might use <span class="math inline">\(4\)</span> servers
(see Table <a href="#tab:sec-fw">2</a>),
but an adversary still only needs <span class="math inline">\(2\)</span> servers for a total break <span class="citation" data-cites="ndss/RachuriS20 popets/ByaliCPS20 uss/Dalskov0K21">(<a href="#ref-ndss/RachuriS20" role="doc-biblioref">Rachuri and Suresh 2020</a>; <a href="#ref-popets/ByaliCPS20" role="doc-biblioref">Byali et al. 2020</a>; <a href="#ref-uss/Dalskov0K21" role="doc-biblioref">Dalskov, Escudero, and Keller 2021</a>)</span>.</p>
<div class="layout-chunk" data-layout="l-body">
<table>
<caption><span id="tab:sec-fw">Table 2: </span>Frameworks Under Non-colluding Assumptions or with Malicious Security</caption>
<colgroup>
<col style="width: 59%" />
<col style="width: 25%" />
<col style="width: 15%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Framework</th>
<th style="text-align: right;">Number of Servers</th>
<th style="text-align: center;">Guarantee</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">SecureML <span class="citation" data-cites="sp/MohasselZ17">(<a href="#ref-sp/MohasselZ17" role="doc-biblioref">Mohassel and Zhang 2017</a>)</span></td>
<td style="text-align: right;"><span class="math inline">\(2\)</span></td>
<td style="text-align: center;"><span class="math inline">\(-\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">Quotient <span class="citation" data-cites="ccs/AgrawalSKG19">(<a href="#ref-ccs/AgrawalSKG19" role="doc-biblioref">Agrawal et al. 2019</a>)</span></td>
<td style="text-align: right;"><span class="math inline">\(2\)</span></td>
<td style="text-align: center;"><span class="math inline">\(-\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;">ABY2.0 <span class="citation" data-cites="uss/Patra0SY21">(<a href="#ref-uss/Patra0SY21" role="doc-biblioref">Patra et al. 2021</a>)</span></td>
<td style="text-align: right;"><span class="math inline">\(2\)</span></td>
<td style="text-align: center;"><span class="math inline">\(-\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">Piranha <span class="citation" data-cites="uss/WatsonWP22">(<a href="#ref-uss/WatsonWP22" role="doc-biblioref">Watson, Wagh, and Popa 2022</a>)</span></td>
<td style="text-align: right;"><span class="math inline">\(\geq 2\)</span></td>
<td style="text-align: center;"><span class="math inline">\(-\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;">CrypTen <span class="citation" data-cites="nips/KnottVHSIM21">(<a href="#ref-nips/KnottVHSIM21" role="doc-biblioref">Knott et al. 2021</a>)</span></td>
<td style="text-align: right;"><span class="math inline">\(\geq 2\)</span></td>
<td style="text-align: center;"><span class="math inline">\(-\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">QuantizedNN <span class="citation" data-cites="popets/Dalskov0K20">(<a href="#ref-popets/Dalskov0K20" role="doc-biblioref">Dalskov, Escudero, and Keller 2020</a>)</span></td>
<td style="text-align: right;"><span class="math inline">\(2/3\)</span></td>
<td style="text-align: center;">Abort</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Chameleon <span class="citation" data-cites="asiaccs/RiaziWTS0K18">(<a href="#ref-asiaccs/RiaziWTS0K18" role="doc-biblioref">Riazi et al. 2018</a>)</span></td>
<td style="text-align: right;"><span class="math inline">\(3\)</span></td>
<td style="text-align: center;"><span class="math inline">\(-\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">CrypTFlow <span class="citation" data-cites="sp/KumarRCGR020">(<a href="#ref-sp/KumarRCGR020" role="doc-biblioref">Kumar et al. 2020</a>)</span></td>
<td style="text-align: right;"><span class="math inline">\(3\)</span></td>
<td style="text-align: center;"><span class="math inline">\(-\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;">CryptGPU <span class="citation" data-cites="sp/TanKTW21">(<a href="#ref-sp/TanKTW21" role="doc-biblioref">Tan et al. 2021</a>)</span></td>
<td style="text-align: right;"><span class="math inline">\(3\)</span></td>
<td style="text-align: center;"><span class="math inline">\(-\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">SecureNN <span class="citation" data-cites="popets/WaghGC19">(<a href="#ref-popets/WaghGC19" role="doc-biblioref">Wagh, Gupta, and Chandran 2019</a>)</span></td>
<td style="text-align: right;"><span class="math inline">\(3\)</span></td>
<td style="text-align: center;">Abort</td>
</tr>
<tr class="odd">
<td style="text-align: left;">ABY3 <span class="citation" data-cites="ccs/MohasselR18">(<a href="#ref-ccs/MohasselR18" role="doc-biblioref">Mohassel and Rindal 2018</a>)</span></td>
<td style="text-align: right;"><span class="math inline">\(3\)</span></td>
<td style="text-align: center;">Abort</td>
</tr>
<tr class="even">
<td style="text-align: left;">FalconN <span class="citation" data-cites="popets/WaghTBKMR21">(<a href="#ref-popets/WaghTBKMR21" role="doc-biblioref">Wagh et al. 2021</a>)</span></td>
<td style="text-align: right;"><span class="math inline">\(3\)</span></td>
<td style="text-align: center;">Abort</td>
</tr>
<tr class="odd">
<td style="text-align: left;">AdanInPrivate <span class="citation" data-cites="popets/AttrapadungHIKM22">(<a href="#ref-popets/AttrapadungHIKM22" role="doc-biblioref">Attrapadung et al. 2022</a>)</span></td>
<td style="text-align: right;"><span class="math inline">\(3\)</span></td>
<td style="text-align: center;">Abort</td>
</tr>
<tr class="even">
<td style="text-align: left;">Blaze <span class="citation" data-cites="ndss/PatraS20">(<a href="#ref-ndss/PatraS20" role="doc-biblioref">Patra and Suresh 2020</a>)</span></td>
<td style="text-align: right;"><span class="math inline">\(3\)</span></td>
<td style="text-align: center;">Fair</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Swift <span class="citation" data-cites="uss/KotiPPS21">(<a href="#ref-uss/KotiPPS21" role="doc-biblioref">Koti et al. 2021</a>)</span></td>
<td style="text-align: right;"><span class="math inline">\(3/4\)</span></td>
<td style="text-align: center;">G.O.D.</td>
</tr>
<tr class="even">
<td style="text-align: left;">Fantastic 4 <span class="citation" data-cites="uss/Dalskov0K21">(<a href="#ref-uss/Dalskov0K21" role="doc-biblioref">Dalskov, Escudero, and Keller 2021</a>)</span></td>
<td style="text-align: right;"><span class="math inline">\(3/4\)</span></td>
<td style="text-align: center;">G.O.D.</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Flash <span class="citation" data-cites="popets/ByaliCPS20">(<a href="#ref-popets/ByaliCPS20" role="doc-biblioref">Byali et al. 2020</a>)</span></td>
<td style="text-align: right;"><span class="math inline">\(4\)</span></td>
<td style="text-align: center;">G.O.D.</td>
</tr>
<tr class="even">
<td style="text-align: left;">Trident <span class="citation" data-cites="ndss/RachuriS20">(<a href="#ref-ndss/RachuriS20" role="doc-biblioref">Rachuri and Suresh 2020</a>)</span></td>
<td style="text-align: right;"><span class="math inline">\(4\)</span></td>
<td style="text-align: center;">Fair</td>
</tr>
<tr class="odd">
<td style="text-align: left;">GarbledNN <span class="citation" data-cites="eprint:GarbledNN">(<a href="#ref-eprint:GarbledNN" role="doc-biblioref">Ball et al. 2019</a>)</span></td>
<td style="text-align: right;"><span class="math inline">\(-\)</span></td>
<td style="text-align: center;">Abort</td>
</tr>
<tr class="even">
<td style="text-align: left;">XONN <span class="citation" data-cites="uss/RiaziS0LLK19">(<a href="#ref-uss/RiaziS0LLK19" role="doc-biblioref">Riazi et al. 2019</a>)</span></td>
<td style="text-align: right;"><span class="math inline">\(-\)</span></td>
<td style="text-align: center;">Abort</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Muse <span class="citation" data-cites="uss/LehmkuhlMSP21">(<a href="#ref-uss/LehmkuhlMSP21" role="doc-biblioref">Lehmkuhl et al. 2021</a>)</span></td>
<td style="text-align: right;"><span class="math inline">\(-\)</span></td>
<td style="text-align: center;">Client</td>
</tr>
</tbody>
</table>
</div>
<p>Chameleon <span class="citation" data-cites="asiaccs/RiaziWTS0K18">(<a href="#ref-asiaccs/RiaziWTS0K18" role="doc-biblioref">Riazi et al. 2018</a>)</span> is a mixed framework that speeds up ABY with various tricks.
It computes some gates over boolean SS (by GMW)
and more advanced protocols by exploiting <span class="math inline">\(3\)</span> servers,
e.g., a communication-cost-saving variant of Du–Atallah protocol <span class="citation" data-cites="ais/DuA01">(<a href="#ref-ais/DuA01" role="doc-biblioref">W. Du and Atallah 2001</a>)</span> for efficient multiplication over additive SS.
ABY3 <span class="citation" data-cites="ccs/MohasselR18">(<a href="#ref-ccs/MohasselR18" role="doc-biblioref">Mohassel and Rindal 2018</a>)</span> extends ABY to <span class="math inline">\(3\)</span>PC over <span class="math inline">\(3\)</span>
servers.
It proposes a new kind of <span class="math inline">\(2\)</span>-out-of-<span class="math inline">\(3\)</span> additive shares for less communication cost, which is generated via a pseudorandom function by exchanging short secret seeds.
No Beaver’s triplets are used.</p>
<p>SecureNN <span class="citation" data-cites="popets/WaghGC19">(<a href="#ref-popets/WaghGC19" role="doc-biblioref">Wagh, Gupta, and Chandran 2019</a>)</span> and “FalconN” <span class="citation" data-cites="popets/WaghTBKMR21">(<a href="#ref-popets/WaghTBKMR21" role="doc-biblioref">Wagh et al. 2021</a>)</span> propose <span class="math inline">\(3\)</span>PC variants of DGK comparison protocol using additive SS to
outperform (non-colluding 3PC) GC/GMW-based ABY3 <span class="citation" data-cites="ccs/MohasselR18">(<a href="#ref-ccs/MohasselR18" role="doc-biblioref">Mohassel and Rindal 2018</a>)</span>.</p>
<p>Trident <span class="citation" data-cites="ndss/RachuriS20">(<a href="#ref-ndss/RachuriS20" role="doc-biblioref">Rachuri and Suresh 2020</a>)</span>
introduces the <span class="math inline">\(4\)</span>-th non-colluding server, which allows more SS kinds, enabling more efficient secure multiplication and comparison, and moving much computation and communication offline.
With <span class="math inline">\(32\)</span>-CPU-core parallelism,
Trident exhibits a much higher online throughput than ABY3, SecureNN, and FalconN in evaluation by orders of magnitude.
In principle, many protocols can also be implemented in parallel.</p>
<p>Flash <span class="citation" data-cites="popets/ByaliCPS20">(<a href="#ref-popets/ByaliCPS20" role="doc-biblioref">Byali et al. 2020</a>)</span>,
Blaze <span class="citation" data-cites="ndss/PatraS20">(<a href="#ref-ndss/PatraS20" role="doc-biblioref">Patra and Suresh 2020</a>)</span>,
and
Swift <span class="citation" data-cites="uss/KotiPPS21">(<a href="#ref-uss/KotiPPS21" role="doc-biblioref">Koti et al. 2021</a>)</span>
also employ <span class="math inline">\(4\)</span> non-colluding servers to enhance the throughput.
They aim to defend against malicious adversaries,
to be discussed in <a href="#sec:malicious">the Section on malicious security</a>.</p>
<h3 id="sec:limited_training">Fluctuating Weights in Limited-bitwidth Training</h3>
<p>Private and outsourced training frameworks run backward propagation with millions of iterations beyond forward propagation in inference.
The only purely-crypto frameworks demonstrating reasonable training time (hours/days for common NNs) use SS under non-colluding assumptions <span class="citation" data-cites="sp/TanKTW21 popets/WaghTBKMR21 popets/AttrapadungHIKM22">(<a href="#ref-sp/TanKTW21" role="doc-biblioref">Tan et al. 2021</a>; <a href="#ref-popets/WaghTBKMR21" role="doc-biblioref">Wagh et al. 2021</a>; <a href="#ref-popets/AttrapadungHIKM22" role="doc-biblioref">Attrapadung et al. 2022</a>)</span>.</p>
<p>Fluctuating weights
make private training suffers
even worse
from fixed-point issues
since increasing magnitudes (of weights, gradients, and intermediate inputs) in a low-bitwidth environment makes overflow happens easier.
Meanwhile, the weights may become zeros when their magnitudes decrease, nullifying the training as the scaling factor may be too small.
Most frameworks
are yet to demonstrate
running large NNs, e.g.,
VGG,
and more complicated datasets, e.g., CIFAR-10/100.</p>
<p>Approaches for inference discussed in <a href="#sec:mixed-bitwidth">the Section on mixed bitwidth</a> do not apply:
With a higher overflow chance, fixed-scale truncation damages the training easily.
Yet, we note that some underlying principles might remain relevant –</p>
<p>One may aim for quantized NN (QNN) using low-bitwidth training.
BatchNorm “for” non-linear function in <a href="#sec:non-linear">the Section on non-linear layers</a> might also “helps.”</p>
<h3 id="privtrain:highbit">High Bitwidth</h3>
<p>The private training frameworks usually use the <span class="math inline">\(64\)</span>-bit ring (<span class="math inline">\(\mathbb{Z}_{2^{64}}\)</span>) for their underlying SS.
In contrast to the <span class="math inline">\(32\)</span>-bit (or less) ring used by
S/C frameworks (for oblivious inference),
SS in <span class="math inline">\(\mathbb{Z}_{2^{64}}\)</span> doubles the communication and computation costs as it halves the throughput of vectorized operations on CPU/GPU, e.g., Intel AVX.
High bitwidth also leads to bulk circuits for the non-linear layers,
harming the performance.</p>
<h3 id="privtrain:batchnorm">BatchNorm</h3>
<p>Batchnorm layers in private training are supported by
FalconN <span class="citation" data-cites="popets/WaghTBKMR21">(<a href="#ref-popets/WaghTBKMR21" role="doc-biblioref">Wagh et al. 2021</a>)</span> using <span class="math inline">\(3\)</span>
servers.
Both the inputs in the forward propagation and the gradients in the backward propagation
are normalized in a smaller range to cater to
the low-bitwidth environment.
Unlike
inference, BatchNorm in
training
requires division and inverse square root where the divisors
should be private and unknown prior to training,
while it is public and fixed in inference.
FalconN thus resorts to a costly iterative method (see <a href="#sec:non-linear">the Section on non-linear layers</a>) while leaking the input magnitude.</p>
<h4 id="privtrain:adam">Adam</h4>
<p>“AdamInPrivate” <span class="citation" data-cites="popets/AttrapadungHIKM22">(<a href="#ref-popets/AttrapadungHIKM22" role="doc-biblioref">Attrapadung et al. 2022</a>)</span> adopts adaptive moment estimation (Adam),
a more advanced SGD algorithm for training.
To stabilize the gradient fluctuation between training batches,
it scales the gradient by <span class="math inline">\(1/\sqrt{G}\)</span>,
where <span class="math inline">\(G\)</span> is the moment, i.e.,
the sum of the squares of the previous gradients.
AdamInPrivate computes <span class="math inline">\(1/\sqrt{G}\)</span> using the techniques in <a href="#sec:non-linear">the Section on non-linear layers</a>.</p>
<h3 id="privtrain:underflow">Low-bitwidth QNN Training with “Small” Fixed-point Gradients</h3>
<p>BatchNorm and Adam cannot fully resolve the bitwidth hurdles.
When a DNN attains a certain level of accuracy,
<span class="math inline">\(\Delta w\)</span>
(for updating the weight to <span class="math inline">\(w - \alpha \cdot \Delta w\)</span>)
may become so small that it has to be accumulated
for many
iterations before making noticeable changes to the weights.
The training ceases to progress because these small gradients are discarded.</p>
<p>Quotient <span class="citation" data-cites="ccs/AgrawalSKG19">(<a href="#ref-ccs/AgrawalSKG19" role="doc-biblioref">Agrawal et al. 2019</a>)</span> adopts a QNN training scheme WAGE <span class="citation" data-cites="iclr/WuLCS18">(<a href="#ref-iclr/WuLCS18" role="doc-biblioref">S. Wu et al. 2018</a>)</span>
(constraining <strong>w</strong>eights, <strong>a</strong>ctivations, <strong>g</strong>radients, and <strong>e</strong>rrors)
to update the weights stochastically according to the magnitude of the gradient,
such that small gradients still have an impact on the big weight.
Yet, its attained accuracy is still inferior to</p>
<p>floating-point training <span class="citation" data-cites="iclr/WuLCS18">(<a href="#ref-iclr/WuLCS18" role="doc-biblioref">S. Wu et al. 2018</a>)</span>.
We note that perhaps deterred by the expensive GC for stochastical updates,
other private training frameworks are yet to integrate low-bitwidth training schemes.</p>
<h3 id="privtrain:softmax">Softmax</h3>
<p>Unlike inference,
where the output layer usually outputs the index of the maximal element of its inputs,
training requires the output layer to map its input via a differentiable function
to enable backward propagation.
A common choice is <span class="math inline">\(\mathsf{softmax}(\mathbf{x}) = (e^{x_i}/\sum_j e^{x_j})_{i}\)</span>.
Most schemes compute it with piecewise linear approximation.</p>
<p>CrypTen <span class="citation" data-cites="nips/KnottVHSIM21">(<a href="#ref-nips/KnottVHSIM21" role="doc-biblioref">Knott et al. 2021</a>)</span> proposes a more efficient and accurate approach combining private division and the limit characterization of <span class="math inline">\(e^x\)</span> (<a href="#sec:non-linear">the Section on non-linear layers</a>).</p>
<h3 id="gpu-for-training">6. GPU for Training</h3>
<p>Consider the dot product <span class="math inline">\(\sum_i \langle w_i \rangle \cdot \langle x_i \rangle\)</span>, the bitwidth of the SSs cannot exceed <span class="math inline">\(26\)</span> bits, not to mention redundant bitwidth to cater for additions.</p>
<p>CryptGPU <span class="citation" data-cites="sp/TanKTW21">(<a href="#ref-sp/TanKTW21" role="doc-biblioref">Tan et al. 2021</a>)</span> utilizes GPUs for private training.
It picks the <span class="math inline">\(64\)</span>-bit ring (<span class="math inline">\(\mathbb{Z}_{2^{64}}\)</span>) for SS,
which exceeds the bitwidth limit of GPUs.
It thus decomposes an SS <span class="math inline">\(\langle X \rangle\)</span> into <span class="math inline">\(2^{48} \langle X_4 \rangle + 2^{32} \langle X_3 \rangle + 2^{16} \langle X_2 \rangle + \langle X_1 \rangle\)</span>.
Multiplying the low-bitwidth sub-SSs <span class="math inline">\(\{\langle X_i \rangle\}_1^4\)</span>
requires many cross-term multiplications,
offsetting some performance gain from GPU.</p>
<h3 id="short-summary">Short Summary</h3>
<p>The high bitwidth training requirement conflicts with the low-bitwidth crypto environment.
(<a href="#privtrain:batchnorm">Batchnorm</a>) normalizes the operands into a smaller range.
Still, the gradients may become too small and easily <a href="#privtrain:underflow">underflow</a> as the training goes on.
The loss function <a href="#privtrain:softmax"><span class="math inline">\(\mathsf{softmax}\)</span></a> also requires accurate approximation.
With the current techniques, we still need relatively <a href="#privtrain:highbit">high bitwidth representation</a>,
harming the training throughput,
and the resulting accuracy could be shy of <span class="math inline">\({&gt;}30\)</span>pp:
CryptGPU <span class="citation" data-cites="sp/TanKTW21">(<a href="#ref-sp/TanKTW21" role="doc-biblioref">Tan et al. 2021</a>)</span> only attains <span class="math inline">\({\sim}60\%\)</span> accuracy in CIFAR-10, while GForce <span class="citation" data-cites="uss/NgC21">(<a href="#ref-uss/NgC21" role="doc-biblioref">Ng and Chow 2021</a>)</span> gets to <span class="math inline">\({&gt;}90\%\)</span> in oblivious inference <span class="citation" data-cites="uss/NgC21">(<a href="#ref-uss/NgC21" role="doc-biblioref">Ng and Chow 2021</a>)</span>.
Private training still desires more ML research,
tackling the following two challenges.</p>
<p><strong>Challenge 5.</strong>
<em>How to cater dynamic weights in secure training?</em></p>
<p><strong>Challenge 6.</strong>
<em>How to privately, efficiently, and accurately evaluate (both linear and non-linear) layers in low-bitwidth?</em></p>
<h3 id="sec:non-linear">Non-Linear Layers for Training and RNNs</h3>
<p>In training,
BatchNorm needs <em>private division</em> <span class="math inline">\(1/y\)</span>, <em>inverse square root</em> <span class="math inline">\(1/\sqrt{y}\)</span>; and <span class="math inline">\(\mathsf{softmax}\)</span> needs <em>exponential function</em> <span class="math inline">\(e^x\)</span>.</p>
<p>SiRnn <span class="citation" data-cites="sp/RatheeRGGSCR21">(<a href="#ref-sp/RatheeRGGSCR21" role="doc-biblioref">Rathee et al. 2021</a>)</span>, as an oblivious-inference-only framework,
also proposes protocols for these functions to support
recurrent NN (RNN) <span class="citation" data-cites="neco/HochreiterS97">(<a href="#ref-neco/HochreiterS97" role="doc-biblioref">Hochreiter and Schmidhuber 1997</a>)</span>, e.g., with gated recurrent unit <span class="citation" data-cites="ssst/ChoMBB14">(<a href="#ref-ssst/ChoMBB14" role="doc-biblioref">K. Cho et al. 2014</a>)</span> or LSTM
cells,
which is good at sequential data like voice records and articles.
RNN requires <span class="math inline">\(e^x\)</span> for <span class="math inline">\(\mathsf{sigmoid}\)</span>, <span class="math inline">\(\mathsf{tanh}\)</span>,
and normalization,
but their piecewise linear approximation <span class="citation" data-cites="ccs/LiuJLA17 sp/MohasselZ17">(<a href="#ref-ccs/LiuJLA17" role="doc-biblioref">Liu et al. 2017</a>; <a href="#ref-sp/MohasselZ17" role="doc-biblioref">Mohassel and Zhang 2017</a>)</span> unbearably fails.</p>
<p><strong>Piecewise Approximation</strong>
partitions non-comparison-based functions, e.g., <span class="math inline">\(\mathsf{sigmoid}\)</span>, into many linear functions, and uses <span class="math inline">\(\mathsf{CMP}\)</span> via GC to locate the piece of the input.
SecureML and MiniONN adopt it.
The accuracy increases with more dissections, but more <span class="math inline">\(\mathsf{CMP}\)</span> and multiplexers harm the performance.</p>
<p><strong>Private Division</strong> is known to be more expensive
as it frequently
invokes the truncation protocol
(cheaper as the divisor is public).</p>
<p>FalconN <span class="citation" data-cites="popets/WaghTBKMR21">(<a href="#ref-popets/WaghTBKMR21" role="doc-biblioref">Wagh et al. 2021</a>)</span> computes <span class="math inline">\(\langle 1/y \rangle\)</span> with Goldschmidt’s method.
With an initial guess <span class="math inline">\(\langle w_0 \rangle\)</span>,
it computes the approximation <span class="math inline">\(\langle w_{i} \rangle = \langle w_{i-1} \rangle \cdot (1 + \langle \epsilon_{i-1} \rangle)\)</span>
with the error correction term <span class="math inline">\(\langle \epsilon_i \rangle = \langle \epsilon_{i-1} \rangle^2\)</span>.
(Here, we omit the scaling up and truncation.)</p>
<p>Each iteration can double the precision (i.e., quadratic convergence) – the absolute error <span class="math inline">\(|w_t - 1/y|\)</span> is less than <span class="math inline">\(|w_0 - 1/y|^{2^t}\)</span>,
but it takes <span class="math inline">\(1\)</span> more communication round for each SS multiplication.
It outputs <span class="math inline">\(\langle w_t \rangle \approx \langle 1/y \rangle\)</span> after <span class="math inline">\(t\)</span> iterations.</p>
<p>A good initial guess reduces the iterations needed.
FalconN adopts <span class="math inline">\(\langle w_0 \rangle = 2.9142 - 2 \cdot \langle y \rangle / 2^{\alpha}\)</span> from the prior art in private division <span class="citation" data-cites="fc/CatrinaS10">(<a href="#ref-fc/CatrinaS10" role="doc-biblioref">Catrina and Saxena 2010</a>)</span>.
<span class="math inline">\(\alpha\)</span> is the (unprotected) magnitude of <span class="math inline">\(y\)</span> (<span class="math inline">\(2^\alpha\!\leq y\!\leq 2^{\alpha + 1}\)</span>).
The fixed-point representation of <span class="math inline">\(w_0\)</span> and <span class="math inline">\(y\)</span> are scaled up to precision <span class="math inline">\(\alpha + 1\)</span> to avoid value underflow.</p>
<p>CrypTen <span class="citation" data-cites="nips/KnottVHSIM21">(<a href="#ref-nips/KnottVHSIM21" role="doc-biblioref">Knott et al. 2021</a>)</span> computes <span class="math inline">\(\langle w_0 \rangle = 3 \cdot \langle e^{0.5-y} \rangle + 0.003\)</span>
(see below for <span class="math inline">\(\langle e^{y} \rangle\)</span>).
CryptGPU <span class="citation" data-cites="sp/TanKTW21">(<a href="#ref-sp/TanKTW21" role="doc-biblioref">Tan et al. 2021</a>)</span>
assumes <span class="math inline">\(y\)</span> is bounded in <span class="math inline">\([1, Y]\)</span>,
which is applicable to <span class="math inline">\(\mathsf{softmax}\)</span> in private training
but may not to <span class="math inline">\(\mathsf{sigmoid}\)</span> and <span class="math inline">\(\mathsf{tanh}\)</span> in RNN.
The initial guess is set to <span class="math inline">\(w_0 = 1/Y\)</span>.</p>
<p>AdamInPrivate <span class="citation" data-cites="popets/AttrapadungHIKM22">(<a href="#ref-popets/AttrapadungHIKM22" role="doc-biblioref">Attrapadung et al. 2022</a>)</span> proposes to privately extract magnitude <span class="math inline">\(\alpha\)</span> by a circuit,
use it to shift <span class="math inline">\(y\)</span> to <span class="math inline">\(1 - y&#39; \in [0.5, 1)\)</span>,
and compute <span class="math inline">\(\langle 1/(1 - y&#39;) \rangle = \langle 1 + y&#39; + y&#39;^2 + \ldots \rangle = \langle 1+y&#39; \rangle \cdot \langle 1+y&#39;^2 \rangle \cdot \langle 1+y&#39;^4 \rangle \cdots\)</span> by <span class="math inline">\(O(\log_2 t)\)</span> squaring and multiplications for a <span class="math inline">\(t\)</span>-degree approximation,
and scale <span class="math inline">\(\langle 1/(1 - y&#39;) \rangle\)</span> back to <span class="math inline">\(\langle 1/y \rangle\)</span>.</p>
<p><strong>Inverse Square Root.</strong>
<span class="math inline">\(\langle 1/\sqrt{y} \rangle\)</span>
can be evaluated by Newton’s iterative method
<span class="math inline">\(w_i = \langle w_{i-1} \rangle \cdot (3 - \langle y \rangle \cdot \langle w_{i-1} \rangle^2)/2\)</span> with quadratic convergence.
AdamInPrivate privately extracts in a circuit <span class="math inline">\(\lfloor \alpha/2 \rceil\)</span> and hides <span class="math inline">\(w_0\)</span>’s value. FalconN picks <span class="math inline">\(w_0 = 2^{\lfloor \alpha/2 \rceil}\)</span>.</p>
<p><strong>Exponential Function.</strong>
For <span class="math inline">\(\langle e^x \rangle\)</span>,
CrypTen <span class="citation" data-cites="nips/KnottVHSIM21">(<a href="#ref-nips/KnottVHSIM21" role="doc-biblioref">Knott et al. 2021</a>)</span>
approximates <span class="math inline">\(\langle e^x \rangle\)</span> more efficiently
by computing the limit characterization <span class="math inline">\((1 + \langle x \rangle/2^t)^{2^t}\)</span>.
It takes <span class="math inline">\(t\)</span> iterations of SS multiplications to reduce relative error to <span class="math inline">\(e^{O(-x^2/2^t)}\)</span> for <span class="math inline">\(|x| &lt; 2^t\)</span>.</p>
<p><strong>Lookup Table.</strong>
Borrowing memory-saving tricks in embedded systems, SiRnn <span class="citation" data-cites="sp/RatheeRGGSCR21">(<a href="#ref-sp/RatheeRGGSCR21" role="doc-biblioref">Rathee et al. 2021</a>)</span> extends a provably precise lookup approach for getting a good initial guess before using iteration to improve upon the approximation.
Namely, it obliviously looks up <span class="math inline">\(d\)</span>-digit <span class="math inline">\(\langle \mathsf{exp}({2^{d\cdot (i-1)} x_i} \rangle)\)</span> over <span class="math inline">\(\langle x_i \rangle\)</span>
for <span class="math inline">\(x = x_k || \cdots || x_1\)</span> and multiplies the shares
as <span class="math inline">\(e^x = \mathsf{exp}({2^{d\cdot (k-1)} \cdot x_k + \cdots + 2^d \cdot x + x_1})\)</span>.
An upside is that the oblivious lookup can be parallelized,
while iterative approaches are inherently sequential.</p>
<p>The above approaches only work on additive shares.
Glyph <span class="citation" data-cites="nips/LouFF020">(<a href="#ref-nips/LouFF020" role="doc-biblioref">Lou et al. 2020</a>)</span> uses the lookup table implemented by TFHE and BGV-HE to compute <span class="math inline">\(\mathsf{sigmoid}\)</span> and <span class="math inline">\(\mathsf{softmax}\)</span>.
Yet, it is impractically slow due to the overhead caused by the FHE schemes.
Lately, Heath and Kolesnikov <span class="citation" data-cites="ccs/HeathK21">(<a href="#ref-ccs/HeathK21" role="doc-biblioref">Heath and Kolesnikov 2021</a>)</span> propose one-hot garbling that allows efficient privacy-free lookup tables in garbled circuits.
It would be interesting to know if it can speed up non-linear layers,
especially in a high-latency communication environment where GC trumps OT.</p>
<p>To sum up, many tricks are inspired by
numerical approximations, possibly with a new angle, such as the lookup approach
for iterative computations.
Their accurate approximations
are also crucial for private training.
We pose two challenges:</p>
<p><strong>Challenge 7.</strong>
<em>How to efficiently and accurately approximate <span class="math inline">\(x/y\)</span>, <span class="math inline">\(1/\sqrt{y}\)</span>, <span class="math inline">\(e^x\)</span>, <span class="math inline">\(\mathsf{sigmoid}(x)\)</span>, and <span class="math inline">\(\mathsf{tanh}(x)\)</span> for secret <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>? More broadly, is there any other crypto-friendly numerical method for mathematical functions useful for NN?</em></p>
<p><strong>Challenge 8.</strong>
<em>Can we generalize the lookup approach or extend other low-bitwidth tricks of embedding systems to other non-linear functions for more efficient approximations?</em></p>
<h2 id="sec:deploy">Practical Deployment Issues</h2>
<h3 id="sec:nn-search">Neural Network Architecture Optimization</h3>
<p>Tuning NN architecture can cut the cost of cryptographic NN computation
with moderate inference accuracy degradation.</p>
<p><strong>Pruning</strong> <em>outsourced</em> inference would let the server learn the number of zeroed weights from how much computation
is saved.
Such leakage is not a concern in non-outsourced inference.
Still, a custom implementation of SS operations is needed to take advantage of zeroed weights as they are hidden
by SS,
abandoning off-the-shelf optimized math libraries.
This asks for dedicated customization and secure programming effort to ensure no potential side-channel leakage.</p>
<p><strong>Open Problem 5.</strong>
<em>How to build a leakage-free data-oblivious library exploiting data-dependent optimization like pruning?</em></p>
<p><strong>Cost Optimizers</strong>
tune the architecture to optimize an objective function, factoring
in the inference accuracy and the cost of cryptographic primitives.
NASS <span class="citation" data-cites="ecai/BianJLSS20">(<a href="#ref-ecai/BianJLSS20" role="doc-biblioref">Bian et al. 2020</a>)</span>
performs an automatic search for NN architecture and HE parameters to attain the best performance and accuracy over Gazelle <span class="citation" data-cites="uss/JuvekarVC18">(<a href="#ref-uss/JuvekarVC18" role="doc-biblioref">Juvekar, Vaikuntanathan, and Chandrakasan 2018</a>)</span>.
COINN <span class="citation" data-cites="ccs/HussainJSK21">(<a href="#ref-ccs/HussainJSK21" role="doc-biblioref">Hussain et al. 2021</a>)</span> optimizes over the bit-width of operands in each linear layer.</p>
<p><strong>Weight Clustering.</strong>
Observing <span class="math inline">\(\sum{w_i \cdot x_i}\)</span> over <span class="math inline">\(N\)</span> weights <span class="math inline">\(\{w_i\}\)</span>
can be factorized into
<span class="math inline">\(\sum_{i \in [1, V]}{w_i \sum_{j \in S_i}{x_j}}\)</span>
over <span class="math inline">\(V\)</span> unique weights after clustering, where <span class="math inline">\(S_i\)</span> is the index set,
COINN uses <span class="math inline">\(N\)</span>
OTs to select and accumulate <span class="math inline">\(\sum_{j \in S_i}{X_j}\)</span> for <span class="math inline">\(i \in [V]\)</span>,
resulting in <span class="math inline">\({\approx} VN\ell\)</span> bits of communication over <span class="math inline">\(\ell\)</span>-bit arithmetic shares (vs. <span class="math inline">\({\approx}N\ell^2\)</span>, as in <a href="#sec:off-on-share">the Section on offline-online computation</a>).</p>
<p>Then, it computes the shortened dot product over size-<span class="math inline">\(V\)</span> vectors with <span class="math inline">\(V\ell^2\)</span> bits of communication.
The total cost is reduced to <span class="math inline">\(V\ell(N + \ell)\)</span>
with COINN’s bit-width optimization
(<span class="math inline">\(N \gg \ell\)</span>) when <span class="math inline">\(V &lt; \ell\)</span>.</p>
<p><strong>Reducing <span class="math inline">\(\mathsf{ReLU}\)</span>.</strong>
With the huge performance gap between polynomial approximation and <span class="math inline">\(\mathsf{CMP}\)</span>,
Delphi’s planner <span class="citation" data-cites="uss/MishraLSZP20">(<a href="#ref-uss/MishraLSZP20" role="doc-biblioref">Mishra et al. 2020</a>)</span> searches for NNs that
replace <span class="math inline">\(\mathsf{ReLU}\)</span> layers
by quadratic approximation with mild accuracy degradation.
SafeNet <span class="citation" data-cites="iclr/LouSJ021">(<a href="#ref-iclr/LouSJ021" role="doc-biblioref">Lou et al. 2021</a>)</span> provides a better tradeoff by a more fine-grained replacement of <span class="math inline">\(\mathsf{ReLU}\)</span> by approximation on some channels instead of a whole layer.</p>
<p>CryptoNAS <span class="citation" data-cites="nips/GhodsiVRG20">(<a href="#ref-nips/GhodsiVRG20" role="doc-biblioref">Ghodsi et al. 2020</a>)</span>
reduces the number of <span class="math inline">\(\mathsf{ReLU}\)</span>’s by observing that a <span class="math inline">\(\mathsf{ReLU}\)</span> layer in ResNet may skip some or even all inputs, and just outputs identical values.
DeepReDuce <span class="citation" data-cites="icml/JhaGGR21">(<a href="#ref-icml/JhaGGR21" role="doc-biblioref">Jha et al. 2021</a>)</span> further subsamples the inputs to <span class="math inline">\(\mathsf{ReLU}\)</span> layers.
It adopts knowledge distillation <span class="citation" data-cites="iccv/ChoH19">(<a href="#ref-iccv/ChoH19" role="doc-biblioref">J. H. Cho and Hariharan 2019</a>)</span> to compensate for the accuracy loss, which retrains the <span class="math inline">\(\mathsf{ReLU}\)</span>-reduced NN with the input-output pairs of the original NN.
Surprisingly, the retrained NN may even attain higher accuracy than the original NN.</p>
<p>Despite their empirical performance, they do not appear to be the optimal search for <span class="math inline">\(\mathsf{ReLU}\)</span> reduction.
We thus ask:</p>
<p><strong>Challenge 9.</strong>
<em>How to “optimally” approximate <span class="math inline">\(\mathsf{ReLU}\)</span> layers while mildly affecting (or even improving) accuracy?</em></p>
<p>Cost optimization can be seen as a pursuit complementary to
<a href="#outsource-without-non-colluding">the open problem for cyrpto-friendly machine learning techniques</a>,
which explores the use of polynomials as crypto-friendly operations to approximate non-linear layers.</p>
<p>ML research for broader classes of crypto-friendly computations (beyond polynomials)
remains largely unexplored.
Despite this,
this SoK discussed two different kinds of fusing from the system (nGraph-HE <span class="citation" data-cites="cf/BoemerLCW19">(<a href="#ref-cf/BoemerLCW19" role="doc-biblioref">Boemer, Lao, et al. 2019</a>)</span>) and crypto (GForce <span class="citation" data-cites="uss/NgC21">(<a href="#ref-uss/NgC21" role="doc-biblioref">Ng and Chow 2021</a>)</span>) communities.
The latter can minimize non-crypto-friendly computations.</p>
<p>Broadly:</p>
<p><strong>Open Problem 6.</strong>
<em>Are there other “fusing” opportunities that can reduce cryptographic operations in PPNN?</em></p>
<p>Hyperparameters affect how ML algorithms perform.
Hyperparameter tuning aims to find out the best configuration of the training algorithm that will output a model generalizing to new data well.
There is growing attention to hyperparameter optimization (HO) since tuning procedures could be tedious.</p>
<p><strong>Open Problem 7.</strong>
<em>Are there any HO opportunities beneficial to PPNN, say, minimizing the cost of cryptographic NN computation while maintaining an “acceptable” accuracy level?</em></p>
<p><strong>Open Problem 8.</strong>
<em>Can we realize “private training with HO”?</em></p>
<h3 id="sec:he-deploy">Deploying Optimized HE Frameworks</h3>
<p>To sustain the
magnitude growth
in iterated ciphertext operations,
pure-LHE frameworks
using BFV-HE or BGV-HE,
e.g., CryptoNets and FasterCryptoNets <span class="citation" data-cites="corr/fastercryptonets">(<a href="#ref-corr/fastercryptonets" role="doc-biblioref">Chou et al. 2018</a>)</span> deal with the overflow issue by picking large plaintext space.
CKKS-HE</p>
<p>allows discarding the least significant bits by rescaling;
each such operation requires large HE parameters.
These enlarged HE parameters increase the runtime and communication cost.
It explains why pure-LHE frameworks are not efficient enough for deep NNs.</p>
<p><strong>HE parameters</strong> include
the finite-field size for the plaintext (i.e., bitwidth),
the degree of the polynomial representing the ciphertexts
(often also the numbers of SIMD slots),
and the coefficient’s moduli of the polynomial (determining the number of “levels” of HE operations and noise tolerance).
Each operation accumulates noise in a ciphertext, which may eventually destroy its plaintext.
HE parameters determine how much noise it can withstand.</p>
<p>It is not easy for non-cryptographers to pick the optimal parameters under constraints with cryptographic implications.
Large parameters lead to larger ciphertexts and more expensive computations.
For a good balance of security and efficiency,
one should carefully choose <em>tight parameters</em>
if possible.</p>
<p><strong>Challenge 10.</strong>
<em>How to guide non-cryptographers to select “tight” HE parameters or avoid the troubles of the noise?</em></p>
<p>Compilers are proposed to do it w.r.t. NN architecture.
We defer them to <a href="#sec:compiler">the Section on compilers</a> for their multiple purposes <span class="citation" data-cites="cf/BoemerLCW19">(<a href="#ref-cf/BoemerLCW19" role="doc-biblioref">Boemer, Lao, et al. 2019</a>)</span>.</p>
<p><strong>Selecting tight HE parameters</strong> is critical for performance.
A few frameworks automatically pick just-enough choices to prevent overflow
with operational costs confined.
They also select scaling factors for the fixed-point representation.</p>
<p>CHET <span class="citation" data-cites="pldi/DathathriSCLLMM19">(<a href="#ref-pldi/DathathriSCLLMM19" role="doc-biblioref">Dathathri et al. 2019</a>)</span> derives HE parameters based on
the training dataset and the computation graph (for the NN architecture).
SEALion <span class="citation" data-cites="van2019sealion">(<a href="#ref-van2019sealion" role="doc-biblioref">Elsloo, Patrini, and Ivey-Law 2019</a>)</span> adopts a similar approach.
nGraph-HE2 <span class="citation" data-cites="wahc/BoemerCCW19">(<a href="#ref-wahc/BoemerCCW19" role="doc-biblioref">Boemer, Costache, et al. 2019</a>)</span>
selects small parameters (<span class="math inline">\(32\)</span>-bit moduli) to reduce CPU instructions for HE.</p>
<p><strong>Optimizing the data layout</strong>
can lower rotation costs
(e.g., for dot product, see <a href="#sec:he-simd">the Section on efficient HE</a>).
Most frameworks automatically encode (plaintext/encrypted) tensors into the HE polynomials,
without developers noticing the underlying data layout.</p>
<p>CHET <span class="citation" data-cites="pldi/DathathriSCLLMM19">(<a href="#ref-pldi/DathathriSCLLMM19" role="doc-biblioref">Dathathri et al. 2019</a>)</span> further optimizes the data layout to minimize the overall rotation cost,
as different rotations cost differently.</p>
<p><strong>Fusing or skipping operations</strong>
is common in compiler optimization.
nGraph-HE <span class="citation" data-cites="cf/BoemerLCW19">(<a href="#ref-cf/BoemerLCW19" role="doc-biblioref">Boemer, Lao, et al. 2019</a>)</span>
parallelizes HE operations (on CPU),
exploiting the SIMD of HE over the batches of inputs,
fusing the multiplications in BatchNorm layers,
and skipping some HE operations when the plaintext is <span class="math inline">\(0, 1\)</span>, or <span class="math inline">\(-1\)</span>.
nGraph-HE2 <span class="citation" data-cites="wahc/BoemerCCW19">(<a href="#ref-wahc/BoemerCCW19" role="doc-biblioref">Boemer, Costache, et al. 2019</a>)</span> groups up or skips expensive denoising.</p>
<p><strong>Optimizing over the hardware intrinsics</strong> is done by
PlaidML-HE <span class="citation" data-cites="iccd/ChenCVR19">(<a href="#ref-iccd/ChenCVR19" role="doc-biblioref">Chen et al. 2019</a>)</span>
for more efficient HE operations and linear layers.
It covers CPUs and GPUs of various brands and
CKKS-HE <span class="citation" data-cites="asiacrypt/CheonKKS17">(<a href="#ref-asiacrypt/CheonKKS17" role="doc-biblioref">Cheon et al. 2017</a>)</span> for its residue number system.</p>
<p>If we had an HE scheme that could be instantiated with small parameters,
it immediately reduces the communication cost.
Likewise, cutting the bootstrapping time (e.g., of TFHE) also reduces the
latency.
Cryptographers’ research efforts are mostly needed for the problems/challenges below.</p>
<p><strong>Open Problem 9.</strong>
<em>How to improve HE’s performance?</em></p>
<p><strong>Challenge 11.</strong>
<em>How to perform low-latency inference under limited-bandwidth conditions without non-colluding servers?</em></p>
<p><strong>Challenge 12.</strong>
<em>How to efficiently realize secure outsourcing of inference for deep NNs without non-colluding assumptions?</em></p>
<h3 id="sec:compiler">Compilers for Rapid Development and Optimization</h3>
<p><strong>Developer-Friendly Compilers.</strong>
Many works proposed a specific compiler that compiles a (plaintext) NN, including the architecture and the weights, into a PPNN, often with optimized cryptographic computation.
This allows coding with a standard machine-learning library or high-level APIs without knowing much about (optimizing) cryptographic computation, reducing laborious hand-engineering and possible coding errors.</p>
<p><em>Graph-Based Compilers</em>
transform
an NN representation of an NN library,
e.g., TensorFlow or PyTorch,
into the “cryptographic version”
for oblivious inference.
They include nGraph-HE/2 <span class="citation" data-cites="cf/BoemerLCW19 wahc/BoemerCCW19">(<a href="#ref-cf/BoemerLCW19" role="doc-biblioref">Boemer, Lao, et al. 2019</a>; <a href="#ref-wahc/BoemerCCW19" role="doc-biblioref">Boemer, Costache, et al. 2019</a>)</span>,
PlaidML-HE <span class="citation" data-cites="iccd/ChenCVR19">(<a href="#ref-iccd/ChenCVR19" role="doc-biblioref">Chen et al. 2019</a>)</span>,
and CrypTFlow <span class="citation" data-cites="sp/KumarRCGR020">(<a href="#ref-sp/KumarRCGR020" role="doc-biblioref">Kumar et al. 2020</a>)</span>.</p>
<p><em>Custom APIs</em> are proposed by some frameworks
to help developers adopt them.
EzPC <span class="citation" data-cites="eurosp/ChandranGRST19">(<a href="#ref-eurosp/ChandranGRST19" role="doc-biblioref">Chandran et al. 2019</a>)</span> is a general compiler for <span class="math inline">\(2\)</span>PC that
converts its Python-like source code
to C++, calling ABY API functions for oblivious inference with 2 non-colluding servers.
XONN <span class="citation" data-cites="uss/RiaziS0LLK19">(<a href="#ref-uss/RiaziS0LLK19" role="doc-biblioref">Riazi et al. 2019</a>)</span> supports high-level API for BNN layer specification
and translates an NN description from other DNN libraries into its API.
SEALion <span class="citation" data-cites="van2019sealion">(<a href="#ref-van2019sealion" role="doc-biblioref">Elsloo, Patrini, and Ivey-Law 2019</a>)</span> uses Keras-like syntax for implementing PPNN,
while CHET <span class="citation" data-cites="pldi/DathathriSCLLMM19">(<a href="#ref-pldi/DathathriSCLLMM19" role="doc-biblioref">Dathathri et al. 2019</a>)</span> uses TensorFlow-like syntax.
CrypTen <span class="citation" data-cites="nips/KnottVHSIM21">(<a href="#ref-nips/KnottVHSIM21" role="doc-biblioref">Knott et al. 2021</a>)</span> proposes PyTorch-style APIs for <span class="math inline">\(2\)</span>PC protocols (mainly from FalconN’s protocol <span class="citation" data-cites="popets/WaghTBKMR21">(<a href="#ref-popets/WaghTBKMR21" role="doc-biblioref">Wagh et al. 2021</a>)</span>).</p>
<p><em>GPU Platform for Arithmetic SS.</em>
Piranha <span class="citation" data-cites="uss/WatsonWP22">(<a href="#ref-uss/WatsonWP22" role="doc-biblioref">Watson, Wagh, and Popa 2022</a>)</span>
is a platform
for MPC from arithmetic SS to run on top of GPU.
Its modular design allows easier integration of new SS-based protocols (for a certain layer)
than re-implementing the entire framework.
These compilers bring convenience to developers.</p>
<p>However,
PPNN researchers often still need much engineering effort to implement
specific tricks they devised, let alone re-implement existing ones for comparison when they are often implemented with different languages on top of different libraries
with slightly different low-level optimization.
To let researchers focus on what they are good at and save resources from<br />
incessant
replications of old results
(e.g., <span class="math inline">\(96\)</span> cores <span class="citation" data-cites="tetc/BadawiJLMJTNAC21">(<a href="#ref-tetc/BadawiJLMJTNAC21" role="doc-biblioref">Badawi et al. 2021</a>)</span>, Nvidia V100 GPU <span class="citation" data-cites="uss/NgC21 sp/TanKTW21 tetc/BadawiJLMJTNAC21 uss/WatsonWP22">(<a href="#ref-uss/NgC21" role="doc-biblioref">Ng and Chow 2021</a>; <a href="#ref-sp/TanKTW21" role="doc-biblioref">Tan et al. 2021</a>; <a href="#ref-tetc/BadawiJLMJTNAC21" role="doc-biblioref">Badawi et al. 2021</a>; <a href="#ref-uss/WatsonWP22" role="doc-biblioref">Watson, Wagh, and Popa 2022</a>)</span>,
processing <span class="math inline">\(1\)</span>TB of data for an encrypted query <span class="citation" data-cites="corr/fastercryptonets">(<a href="#ref-corr/fastercryptonets" role="doc-biblioref">Chou et al. 2018</a>)</span>),
a rapid benchmarking platform is desired:</p>
<p><strong>Challenge 13.</strong>
<em>Can we build a universal compiler that enables rapid prototyping and allow uniform experimental comparison?</em></p>
<aside>
e.g.,
<a href="https://github.com/facebookresearch/CrypTen">CrypTen</a>,
<a href="https://github.com/mc2-project/delphi">Delphi</a>,
<a href="https://github.com/mpc-msri/EzPC">EzPC</a>,
<a href="https://github.com/Lucieno/gforce-public">GForce</a>
</aside>
<p><strong>Optimization.</strong> EzPC <span class="citation" data-cites="eurosp/ChandranGRST19">(<a href="#ref-eurosp/ChandranGRST19" role="doc-biblioref">Chandran et al. 2019</a>)</span> optimizes performance over the choices of A/B/Y SS
for oblivious (program evaluation and) inference.
Most others optimize arithmetic HE:</p>
<p><strong>Challenge 14.</strong>
<em>Can we build an end-to-end compiler like PlaidML-HE over other major HE schemes such as TFHE?</em></p>
<h3 id="subsec:leakage">Leakages of Metadata and Difficult-to-Hide Information</h3>
<p><strong>Convolution Parameters.</strong>
Most frameworks leak the stride, padding, and kernel size in convolution layers.
When using circuit-based primitives in the outsourced setting (e.g., TFHE, GC, and GMW), adversaries can derive them from the circuit’s topology.
Likewise, they can be leaked from the computation steps
by an outsourced server unless they are locally done.</p>
<p><strong>Dimensions of Layers</strong> refer to the input and output size of the layers.
For outsourced or server/client frameworks, they can be derived from the intermediate SSs or HE ciphertexts.
For pure-HE inference frameworks, the client does not see the intermediate ciphertexts and hence does not see the leakage.</p>
<p><strong>Types of Layers</strong> are similar to dimensions since PPNN frameworks do not hide the computed functions in general.</p>
<p><strong>Total Number of Layers.</strong>
Leaking the dimension also leaks the total number of layers.
LHE frameworks still leak this information because the client can deduce the number of multiplications performed.
Usually, a linear or activation layer requires one.</p>
<h3 id="are-practitioners-willing-to-compromise">Are Practitioners Willing to Compromise?</h3>
<p>The state-of-the-art private training using <span class="math inline">\(3\)</span> GPUs is still far from the performance of plaintext training.
Meanwhile, the ML community has started considering solutions using trusted execution environments and lightweight cryptographic outsourcing protocols for leveraging (untrusted) GPUs <span class="citation" data-cites="aaai/NgCWW021">(<a href="#ref-aaai/NgCWW021" role="doc-biblioref">Ng et al. 2021</a>)</span>.
With the performance gap from plaintext training, some might care less about trust assumptions or malicious security.</p>
<p><strong>Open Problem 10.</strong>
<em>Is there any gap between theoretical security guarantees and what practitioners are (un)willing to trust? How should those perspectives inform PPNN research efforts?</em></p>
<p>The answer depends on the application scenarios and deployment settings.
On one hand, there are highly sensitive scenarios, e.g., child exploitation imagery detection, in which one might expect the highest security level.</p>
<p>On the other, it can be a life-saving effort to speed up the progress in, e.g., cancer diagnosis <span class="citation" data-cites="aaai/NgCWW021">(<a href="#ref-aaai/NgCWW021" role="doc-biblioref">Ng et al. 2021</a>)</span>, genomic precision medicine, drug-target interactions <span class="citation" data-cites="sci/HieCB18">(<a href="#ref-sci/HieCB18" role="doc-biblioref">Hie, Cho, and Berger 2018</a>)</span>, and reliance on the non-colluding assumption could become a secondary concern <span class="citation" data-cites="ndss/ChowLS09">(<a href="#ref-ndss/ChowLS09" role="doc-biblioref">Chow, Lee, and Subramanian 2009</a>)</span>.</p>
<p>One may have faith in a privacy-respecting government or privacy advocates for the needed trust.
Meanwhile, industry players might not welcome non-colluding assumptions since the idea of trusting competitors can be illusory to some.</p>
<h2 id="sec:benchmark">Status Quo in Accuracy/Latency/Throughput</h2>
<p>We integrate evaluation results presented by the papers covered by this SoK in the same plots for general trends.</p>
<p>Each framework shines in its way in its targeted settings.
We highlight noteworthy points in the space.
We then present re-evaluations of the most promising frameworks we identified.</p>
<p>Below, we discuss typical benchmark datasets in the literature.
More complicated tasks often require a more complex NN architecture with more internal parameters for high accuracy.</p>
<p><strong>MNIST</strong> is a dataset for classifying black-and-white hand-written digitals over <span class="math inline">\(28\times 28\)</span> images,
which can be handled via a multilayer perceptron (MLP) NN with <span class="math inline">\(1\)</span> fully-connected layer at <span class="math inline">\({&gt;}99\%\)</span> accuracy.
It can no longer demonstrate the superiority of new advancements in rapidly developing PPNN research.</p>
<p><strong>CIFAR-10/100</strong>
became popular for benchmarking.
They are datasets consisting of 10/100 classes of <span class="math inline">\(32 \times 32\)</span> colorful images of vehicles and animals, e.g., dog, ship, and truck,
which are harder to classify than the simpler MNIST,
and require convolution layers for better accuracy.
VGG-16 <span class="citation" data-cites="iclr/SimonyanZ15">(<a href="#ref-iclr/SimonyanZ15" role="doc-biblioref">Simonyan and Zisserman 2015</a>)</span>, having <span class="math inline">\(16\)</span> linear layers, is a commonly chosen architecture to handle CIFAR-10/100.
ResNet <span class="citation" data-cites="cvpr/HeZRS16">(<a href="#ref-cvpr/HeZRS16" role="doc-biblioref">He et al. 2016</a>)</span> can attain a higher accuracy
but also incurs longer inference latency as it has more linear layers.</p>
<p><strong>ImageNet</strong> is an even more challenging object recognition dataset.
It consists of <span class="math inline">\({&gt;}14\)</span> millions <span class="math inline">\(224 \times 224\)</span> colorful images with <span class="math inline">\({&gt;}20000\)</span> classes.
Handling this dataset requires capacity beyond current PPNN frameworks.</p>
<p>Still, one <span class="citation" data-cites="ccs/RatheeR0CGR020">(<a href="#ref-ccs/RatheeR0CGR020" role="doc-biblioref">Rathee et al. 2020</a>)</span> may process <span class="math inline">\(224 \times 224\)</span> (ImageNet-scale) images to estimate its performance on such large images and claims that it loses no accuracy given enough bitwidth.</p>
<p>A few frameworks <span class="citation" data-cites="corr/fastercryptonets popets/WaghTBKMR21">(<a href="#ref-corr/fastercryptonets" role="doc-biblioref">Chou et al. 2018</a>; <a href="#ref-popets/WaghTBKMR21" role="doc-biblioref">Wagh et al. 2021</a>)</span>
shrink the dataset into Tiny ImageNet
with <span class="math inline">\(120,000\)</span> <span class="math inline">\(64 \times 64\)</span> images under <span class="math inline">\(200\)</span> classes to demonstrate their ability beyond
CIFAR-10/100.
Unfortunately, they rarely provide accuracy, probably because fixed-point issues are yet to be overcome for the large NNs needed.
It is hard to have a fair comparison.</p>
<p>To put accuracy and latency into perspective,
we may also factor in:</p>
<ul>
<li>the problem setting (oblivious/outsourced inference or outsourced/private training),</li>
<li>the chosen dataset,</li>
<li>the network condition (e.g., LAN or WAN),</li>
<li>the number of non-colluding servers, and</li>
<li>online versus total performance.</li>
</ul>
<h3 id="oblivious-inference">Oblivious Inference</h3>
<p>We summarize the performance of <span class="math inline">\(23\)</span> frameworks marked with CIFAR-10 in <a href="#summary-fw">our Summary of all Frameworks</a>
(except AdamInPrivate with only training benchmarks).
The tests used CIFAR-10 and LAN.</p>
<ul>
<li><p><span class="math inline">\(7\)</span> pure-HE frameworks:
CHET <span class="citation" data-cites="pldi/DathathriSCLLMM19">(<a href="#ref-pldi/DathathriSCLLMM19" role="doc-biblioref">Dathathri et al. 2019</a>)</span>, HCNN <span class="citation" data-cites="tetc/BadawiJLMJTNAC21">(<a href="#ref-tetc/BadawiJLMJTNAC21" role="doc-biblioref">Badawi et al. 2021</a>)</span>, FasterCryptoNets <span class="citation" data-cites="corr/fastercryptonets">(<a href="#ref-corr/fastercryptonets" role="doc-biblioref">Chou et al. 2018</a>)</span>
(or Faster-Crypt in Tables/Figures),
CryptoDL <span class="citation" data-cites="popets/HesamifardTGW18">(<a href="#ref-popets/HesamifardTGW18" role="doc-biblioref">Hesamifard et al. 2018</a>)</span>, nGraph-HE <span class="citation" data-cites="cf/BoemerLCW19">(<a href="#ref-cf/BoemerLCW19" role="doc-biblioref">Boemer, Lao, et al. 2019</a>)</span>, LoLa <span class="citation" data-cites="icml/BrutzkusGE19">(<a href="#ref-icml/BrutzkusGE19" role="doc-biblioref">Brutzkus, Gilad-Bachrach, and Elisha 2019</a>)</span>, and SHE <span class="citation" data-cites="nips/Lou019">(<a href="#ref-nips/Lou019" role="doc-biblioref">Lou and Jiang 2019</a>)</span>;</p></li>
<li><p><span class="math inline">\(13\)</span> server/client (S/C) frameworks:
CrypTFlow2 <span class="citation" data-cites="ccs/RatheeR0CGR020">(<a href="#ref-ccs/RatheeR0CGR020" role="doc-biblioref">Rathee et al. 2020</a>)</span>, XONN <span class="citation" data-cites="uss/RiaziS0LLK19">(<a href="#ref-uss/RiaziS0LLK19" role="doc-biblioref">Riazi et al. 2019</a>)</span>, Delphi <span class="citation" data-cites="uss/MishraLSZP20">(<a href="#ref-uss/MishraLSZP20" role="doc-biblioref">Mishra et al. 2020</a>)</span>, GForce <span class="citation" data-cites="uss/NgC21">(<a href="#ref-uss/NgC21" role="doc-biblioref">Ng and Chow 2021</a>)</span>, FalconI <span class="citation" data-cites="cvpr/LiXZDGWW20">(<a href="#ref-cvpr/LiXZDGWW20" role="doc-biblioref">Li et al. 2020</a>)</span>, DeepSecure <span class="citation" data-cites="dac/RouhaniRK18">(<a href="#ref-dac/RouhaniRK18" role="doc-biblioref">Rouhani, Riazi, and Koushanfar 2018</a>)</span>, EzPC <span class="citation" data-cites="eurosp/ChandranGRST19">(<a href="#ref-eurosp/ChandranGRST19" role="doc-biblioref">Chandran et al. 2019</a>)</span>, Gazelle <span class="citation" data-cites="uss/JuvekarVC18">(<a href="#ref-uss/JuvekarVC18" role="doc-biblioref">Juvekar, Vaikuntanathan, and Chandrakasan 2018</a>)</span>, MiniONN <span class="citation" data-cites="ccs/LiuJLA17">(<a href="#ref-ccs/LiuJLA17" role="doc-biblioref">Liu et al. 2017</a>)</span>, GarbledNN <span class="citation" data-cites="eprint:GarbledNN">(<a href="#ref-eprint:GarbledNN" role="doc-biblioref">Ball et al. 2019</a>)</span>, COINN <span class="citation" data-cites="ccs/HussainJSK21">(<a href="#ref-ccs/HussainJSK21" role="doc-biblioref">Hussain et al. 2021</a>)</span>, SafeNet <span class="citation" data-cites="iclr/LouSJ021">(<a href="#ref-iclr/LouSJ021" role="doc-biblioref">Lou et al. 2021</a>)</span>, and CryptoNAS <span class="citation" data-cites="nips/GhodsiVRG20">(<a href="#ref-nips/GhodsiVRG20" role="doc-biblioref">Ghodsi et al. 2020</a>)</span>;</p></li>
<li><p><span class="math inline">\(3\)</span> non-colluding frameworks:
FalconN! <span class="citation" data-cites="popets/WaghTBKMR21">(<a href="#ref-popets/WaghTBKMR21" role="doc-biblioref">Wagh et al. 2021</a>)</span>, CryptGPU <span class="citation" data-cites="sp/TanKTW21">(<a href="#ref-sp/TanKTW21" role="doc-biblioref">Tan et al. 2021</a>)</span>,
Piranha <span class="citation" data-cites="uss/WatsonWP22">(<a href="#ref-uss/WatsonWP22" role="doc-biblioref">Watson, Wagh, and Popa 2022</a>)</span>’s GPU-instantiation of
FalconN and Fantastic 4.</p></li>
</ul>
<p>We first provide an integrated report of their accuracy, latency, and throughput.
Our figures aim to give readers a sense of differences among different paradigms (marked with different colors) and the significance of GPU-friendly protocols.
We also aim to identify the state of the art in each paradigm.
Thus, the figures only name the Pareto fronts of each paradigm, which also avoids overcrowded name tags.</p>
<p>Multiple <em>data points</em> in each figure can be contributed by a single framework over different NN architectures (all tested with CIFAR-10).</p>
<p>Most existing tests are on LAN, which may not be realistic.
To better understand the identified state of the art,
we re-evaluate them on WAN with various bandwidths to unveil which framework performs the best under a non-ideal network.
It gives us clues on deployment requirements, particularly the minimal bandwidth needed for their peak performance.</p>
<p>For readability,
we omitted <span class="math inline">\(8\)</span> outliers, from the following <span class="math inline">\(3\)</span> frameworks, due to their low accuracy:
nGraph-HE (<span class="math inline">\(62\%\)</span>),
COINN (<span class="math inline">\(68.1\%\)</span> – but it has <em>another</em> data point on the Pareto frontier for throughput),
and Piranha (<span class="math inline">\(40\%\)</span> and <span class="math inline">\(55\%\)</span> – two data points nevertheless on the Pareto frontier for <span class="math inline">\(3.25\times 10^6\)</span> Images/hr throughput and <span class="math inline">\(131\)</span>ms latency).</p>
<strong>1. Online Latency.</strong>
<div class="layout-chunk" data-layout="l-body">
<div class="figure"><span style="display:block;" id="fig:cifar-acc-on-lat"></span>
<img src="images/acc_on_lat_cifar10.png" alt="Online Latency of Inference on CIFAR-10 (*top-left* ones are better)" width="816" />
<p class="caption">
Figure 1: Online Latency of Inference on CIFAR-10 (<em>top-left</em> ones are better)
</p>
</div>
</div>
<p>Figure <a href="#fig:cifar-acc-on-lat">1</a> shows the inference accuracy with respect to the online latency, which is more crucial than total latency for user experiences.</p>
<p>GForce <span class="citation" data-cites="uss/NgC21">(<a href="#ref-uss/NgC21" role="doc-biblioref">Ng and Chow 2021</a>)</span> strikes the best balance
and surprisingly outperforms non-colluding MPC frameworks like CryptGPU <span class="citation" data-cites="sp/TanKTW21">(<a href="#ref-sp/TanKTW21" role="doc-biblioref">Tan et al. 2021</a>)</span> and FalconN <span class="citation" data-cites="popets/WaghTBKMR21">(<a href="#ref-popets/WaghTBKMR21" role="doc-biblioref">Wagh et al. 2021</a>)</span> for inference (albeit supporting training).
It may be attributed to its adoption of QNN and GPU-friendly comparison protocol.
Pure-HE frameworks perform the worst in this setting.</p>
<p>For CIFAR-10, deep NNs are usually needed for reasonable accuracy, leading to huge HE parameters and worse performance.</p>
<strong>2. Online Throughput.</strong>
<div class="layout-chunk" data-layout="l-body">
<div class="figure"><span style="display:block;" id="fig:cifar-acc-on-tp"></span>
<img src="images/acc_on_tp_cifar10.png" alt="Online Throughput of Inference on CIFAR-10 (*top-right* ones are better)" width="816" />
<p class="caption">
Figure 2: Online Throughput of Inference on CIFAR-10 (<em>top-right</em> ones are better)
</p>
</div>
</div>
<p>Figure <a href="#fig:cifar-acc-on-tp">2</a>
shows that CryptGPU defeats GForce in throughput, probably for amortizing the communication cost between the <span class="math inline">\(3\)</span> servers over its batched queries.
CryptoDL <span class="citation" data-cites="popets/HesamifardTGW18">(<a href="#ref-popets/HesamifardTGW18" role="doc-biblioref">Hesamifard et al. 2018</a>)</span>, using only LHE, achieves a throughput close to GForce <span class="citation" data-cites="uss/NgC21">(<a href="#ref-uss/NgC21" role="doc-biblioref">Ng and Chow 2021</a>)</span> and CryptGPU, the representative of the other approaches.
It encodes all batched queries in an HE ciphertext to avoid expensive rotations.</p>
<p>For accuracy, circuit-based SHE <span class="citation" data-cites="nips/Lou019">(<a href="#ref-nips/Lou019" role="doc-biblioref">Lou and Jiang 2019</a>)</span> is the best, albeit the worst in throughput.
Other pure-HE frameworks, e.g., CryptoDL,
attain accuracy inferior to GForce and CryptGPU,
the representative of the other <span class="math inline">\(2\)</span> approaches.
It shows that polynomial approximation bound to HE frameworks harms accuracy.</p>
<p><strong>3. GPU Trumps.</strong>
Figures for <a href="#fig:cifar-acc-on-lat">online latency</a> and <a href="#fig:cifar-acc-on-tp">online throughput</a> over CIFAR-10 show the important role of GPU.
CryptGPU, GForce, and HCNN <span class="citation" data-cites="tetc/BadawiJLMJTNAC21">(<a href="#ref-tetc/BadawiJLMJTNAC21" role="doc-biblioref">Badawi et al. 2021</a>)</span> all utilize GPUs
and achieve the shortest latency or the highest throughput among the frameworks of their respective major approach.
We expect to see future frameworks will continue to involve GPUs.</p>
<p><strong>Open Problem 11.</strong>
<em>What can other cryptographic primitives (for non-linear operations) be made GPU/TPU-friendly?</em></p>
<p><strong>4. Re-evaluation over WAN.</strong></p>
<p>The representative S/C and MPC frameworks
are evaluated on LAN.</p>
<p>We re-evaluate GForce <span class="citation" data-cites="uss/NgC21">(<a href="#ref-uss/NgC21" role="doc-biblioref">Ng and Chow 2021</a>)</span> (for S/C),
CryptGPU <span class="citation" data-cites="sp/TanKTW21">(<a href="#ref-sp/TanKTW21" role="doc-biblioref">Tan et al. 2021</a>)</span>,
FalconN <span class="citation" data-cites="popets/WaghTBKMR21">(<a href="#ref-popets/WaghTBKMR21" role="doc-biblioref">Wagh et al. 2021</a>)</span>, and
the Piranha <span class="citation" data-cites="uss/WatsonWP22">(<a href="#ref-uss/WatsonWP22" role="doc-biblioref">Watson, Wagh, and Popa 2022</a>)</span> version of FalconN on GPU (P-FalconN).
(for MPC) to understand their performance under non-ideal network conditions.
For LAN, they are state-of-the-art –
GForce has the lowest latency,
and all of them have (almost) the highest throughput.
For Piranha, a GPU platform for MPC frameworks,
we pick its instantiation for FalconN as it demonstrates the best performance
among all existing instantiations.</p>
<p>The evaluation is done on a Google Cloud machine with <span class="math inline">\(4\)</span> Nvidia V100 GPUs,
an Intel Xeon 2.2GHz CPU with 32 cores, and <span class="math inline">\(64\)</span>GB RAM.
We set the communication latency to <span class="math inline">\(50\)</span>ms
because it is the latency for data centers communicating within
a continent.</p>
<aside>
<a href="https://www.cloudping.co/grid">https://www.cloudping.co/grid</a>
</aside>
<div class="layout-chunk" data-layout="l-body">
<div class="figure"><span style="display:block;" id="fig:re-eval-wan-lat"></span>
<img src="images/re-eval-wan-lat.png" alt="Our Re-evaluation for Latency over WAN (The *lower* the better)" width="800" />
<p class="caption">
Figure 3: Our Re-evaluation for Latency over WAN (The <em>lower</em> the better)
</p>
</div>
</div>
<p>Figure <a href="#fig:re-eval-wan-lat">3</a> shows their online latency in a WAN setting with
<span class="math inline">\(50\)</span>ms communication latency
using the Linux <span class="math inline">\(\mathsf{tc}\)</span> command for VGG-16 over CIFAR-10.
GForce demonstrates the lowest latency, an order of magnitude lower than others.
Its latency is <span class="math inline">\({\sim} 10\)</span>s for <span class="math inline">\({\geq}100\)</span>Mbps bandwidth and remains <span class="math inline">\({&lt;}1\)</span> minute even under
<span class="math inline">\({\sim}10\)</span>Mbps,
which seems good enough for non-real-time usage.
Notably, these frameworks still have shorter latency than the pure-HE frameworks in WAN.
With only <span class="math inline">\(10{\sim}100\)</span>Mbps, the re-evaluated MPC and S/C frameworks attain <span class="math inline">\(10^2{\sim}10^3\)</span>s latency,
which outperforms Faster-Crypt (with the shortest latency of <span class="math inline">\({\sim}10^5\)</span>s among pure-HE frameworks
in Figure <a href="#fig:cifar-acc-on-lat">1</a>.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="figure"><span style="display:block;" id="fig:re-eval-wan-tp"></span>
<img src="images/re-eval-wan-tp.png" alt="Our Re-evaluation for Throughput over WAN (The *higher* the better)" width="800" />
<p class="caption">
Figure 4: Our Re-evaluation for Throughput over WAN (The <em>higher</em> the better)
</p>
</div>
</div>
<p>Figure <a href="#fig:re-eval-wan-tp">4</a> shows their online throughput under the same experimental setting.
They show a decrease from the order of <span class="math inline">\(10^4\)</span> images/hr in LAN
to <span class="math inline">\(10^2{\sim}10^3\)</span> in WAN.
Similarly, their inference latency is increased by <span class="math inline">\(100\times\)</span>.
When the bandwidth is high (<span class="math inline">\({\geq}100\)</span>Mbps),
their relative order remains similar to Figure <a href="#fig:mnist-acc-on-tp">5</a>.
Compared to pure-HE frameworks, these MPC and S/C frameworks have lower throughput (<span class="math inline">\(10^2{\sim}10^3\)</span> images/hr)
than HCNN (<span class="math inline">\({\sim}10^4\)</span> images/hr) and CryptoDL (<span class="math inline">\({&gt;}10^3\)</span> images/hr).
It is expected because pure-HE is insensitive to network conditions and can leverage SIMD for higher throughput.</p>
<p>All frameworks appear to need <span class="math inline">\({\sim}1000\)</span>Mbps to reach their peak performance.
Notably, FalconN is less sensitive to low bandwidth
as it starts to outperform Piranha and CryptGPU in extremely low bandwidth of <span class="math inline">\(10 \sim 100\)</span>Mbps.</p>
<p><strong>5. MNIST over WAN.</strong></p>
<p>Pure-HE frameworks often perform the best in WAN settings
for not requiring interaction and hence less affected by the
communication latency.
Yet, they support a limited depth of NNs and limited choices of non-linear layers, so they usually
only handle MNIST, an easier dataset.
We did not perform re-evaluations for MNIST being an unrealistically easy dataset to reflect performance for real tasks.</p>
<p>The definition of WAN varies across papers.
Their network traffic latency ranges from <span class="math inline">\(40\)</span> to <span class="math inline">\(150\)</span>ms, and <span class="math inline">\(40\)</span> to <span class="math inline">\(320\)</span>Mbps for their bandwidth.</p>
<p>Figure <a href="#fig:mnist-acc-on-tp">5</a> and <a href="#fig:mnist-acc-on-lat">6</a> includes <span class="math inline">\(21\)</span> frameworks:</p>
<ul>
<li><p><span class="math inline">\(13\)</span> pure-HE frameworks:
CryptoNets <span class="citation" data-cites="icml/Gilad-BachrachD16">(<a href="#ref-icml/Gilad-BachrachD16" role="doc-biblioref">Gilad-Bachrach et al. 2016</a>)</span>,
CryptoDL <span class="citation" data-cites="popets/HesamifardTGW18">(<a href="#ref-popets/HesamifardTGW18" role="doc-biblioref">Hesamifard et al. 2018</a>)</span>,
E2DM <span class="citation" data-cites="ccs/JiangKLS18">(<a href="#ref-ccs/JiangKLS18" role="doc-biblioref">Jiang et al. 2018</a>)</span>,
Faster-Crypt <span class="citation" data-cites="corr/fastercryptonets">(<a href="#ref-corr/fastercryptonets" role="doc-biblioref">Chou et al. 2018</a>)</span>,
FHE-DiNN <span class="citation" data-cites="crypto/BourseMMP18">(<a href="#ref-crypto/BourseMMP18" role="doc-biblioref">Bourse et al. 2018</a>)</span>,
TAPAS <span class="citation" data-cites="icml/SanyalKGK18">(<a href="#ref-icml/SanyalKGK18" role="doc-biblioref">Sanyal et al. 2018</a>)</span>,
CHET <span class="citation" data-cites="pldi/DathathriSCLLMM19">(<a href="#ref-pldi/DathathriSCLLMM19" role="doc-biblioref">Dathathri et al. 2019</a>)</span>,
nGraph-HE <span class="citation" data-cites="cf/BoemerLCW19">(<a href="#ref-cf/BoemerLCW19" role="doc-biblioref">Boemer, Lao, et al. 2019</a>)</span>,
SEALion <span class="citation" data-cites="van2019sealion">(<a href="#ref-van2019sealion" role="doc-biblioref">Elsloo, Patrini, and Ivey-Law 2019</a>)</span>,
SHE <span class="citation" data-cites="nips/Lou019">(<a href="#ref-nips/Lou019" role="doc-biblioref">Lou and Jiang 2019</a>)</span>
LoLa <span class="citation" data-cites="icml/BrutzkusGE19">(<a href="#ref-icml/BrutzkusGE19" role="doc-biblioref">Brutzkus, Gilad-Bachrach, and Elisha 2019</a>)</span>,
Glyph <span class="citation" data-cites="nips/LouFF020">(<a href="#ref-nips/LouFF020" role="doc-biblioref">Lou et al. 2020</a>)</span>,
and HCNN <span class="citation" data-cites="tetc/BadawiJLMJTNAC21">(<a href="#ref-tetc/BadawiJLMJTNAC21" role="doc-biblioref">Badawi et al. 2021</a>)</span>.</p></li>
<li><p><span class="math inline">\(7\)</span> MPC frameworks:
FalconN <span class="citation" data-cites="popets/WaghTBKMR21">(<a href="#ref-popets/WaghTBKMR21" role="doc-biblioref">Wagh et al. 2021</a>)</span>, Quotient <span class="citation" data-cites="ccs/AgrawalSKG19">(<a href="#ref-ccs/AgrawalSKG19" role="doc-biblioref">Agrawal et al. 2019</a>)</span>, Blaze <span class="citation" data-cites="ndss/PatraS20">(<a href="#ref-ndss/PatraS20" role="doc-biblioref">Patra and Suresh 2020</a>)</span>,
SecureNN <span class="citation" data-cites="popets/WaghGC19">(<a href="#ref-popets/WaghGC19" role="doc-biblioref">Wagh, Gupta, and Chandran 2019</a>)</span>,
Trident <span class="citation" data-cites="ndss/RachuriS20">(<a href="#ref-ndss/RachuriS20" role="doc-biblioref">Rachuri and Suresh 2020</a>)</span>, SecureML <span class="citation" data-cites="sp/MohasselZ17">(<a href="#ref-sp/MohasselZ17" role="doc-biblioref">Mohassel and Zhang 2017</a>)</span>, and ABY3 <span class="citation" data-cites="ccs/MohasselR18">(<a href="#ref-ccs/MohasselR18" role="doc-biblioref">Mohassel and Rindal 2018</a>)</span>.</p></li>
<li><p><span class="math inline">\(5\)</span> outliers, namely,
ABY3,
FHE-DiNN,
SecureML,
Trident,
and SecureNN,
are omitted
as their accuracy is only <span class="math inline">\({\sim}93\%\)</span>.</p></li>
</ul>
<p>EzPC <span class="citation" data-cites="eurosp/ChandranGRST19">(<a href="#ref-eurosp/ChandranGRST19" role="doc-biblioref">Chandran et al. 2019</a>)</span>
is the only S/C framework included
since most S/C frameworks skip their WAN evaluation.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="figure"><span style="display:block;" id="fig:mnist-acc-on-tp"></span>
<img src="images/acc_on_tp_mnist.png" alt="Online Throughput of Inference on MNIST (*top-right* ones are better" width="842" />
<p class="caption">
Figure 5: Online Throughput of Inference on MNIST (<em>top-right</em> ones are better
</p>
</div>
</div>
<p>Figure <a href="#fig:mnist-acc-on-tp">5</a> shows their online throughput in WAN.
HCNN (using <span class="math inline">\(4\)</span> GPUs) has the highest throughput.
CryptoDL and nGraph-HE also strike a good balance between accuracy and throughput.
These three frameworks use HE’s SIMD for batch processing.
SecureNN, FalconN, Trident, and Blaze</p>
<p>are the Pareto frontiers of the MPC frameworks.
Yet, they are all inferior to the frontiers of the pure-HE frameworks as they require more interactions.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="figure"><span style="display:block;" id="fig:mnist-acc-on-lat"></span>
<img src="images/acc_on_lat_mnist.png" alt="Online Latency of Inference on MNIST (*top-left* ones are better)" width="830" />
<p class="caption">
Figure 6: Online Latency of Inference on MNIST (<em>top-left</em> ones are better)
</p>
</div>
</div>
<p>For online latency, LoLa reduces the rotation cost of SIMD, and Figure <a href="#fig:mnist-acc-on-lat">6</a> shows its good balance with accuracy.</p>
<p>In general, pure-HE remains better than MPC, but their gap is much narrower since SIMD does not help as much.
(SecureNN is also on the Pareto frontier for <span class="math inline">\(130\)</span>ms latency.
Yet, it is omitted from Figure <a href="#fig:mnist-acc-on-lat">6</a> due to its low accuracy (<span class="math inline">\(93.4\%\)</span>).)</p>
<h3 id="private-training">Private Training</h3>
<p>Among the <span class="math inline">\(9\)</span> frameworks for private (and outsourced) training,
CryptGPU <span class="citation" data-cites="sp/TanKTW21">(<a href="#ref-sp/TanKTW21" role="doc-biblioref">Tan et al. 2021</a>)</span>, FalconN <span class="citation" data-cites="popets/WaghTBKMR21">(<a href="#ref-popets/WaghTBKMR21" role="doc-biblioref">Wagh et al. 2021</a>)</span>, Piranha <span class="citation" data-cites="uss/WatsonWP22">(<a href="#ref-uss/WatsonWP22" role="doc-biblioref">Watson, Wagh, and Popa 2022</a>)</span>, and AdamInPrivate <span class="citation" data-cites="popets/AttrapadungHIKM22">(<a href="#ref-popets/AttrapadungHIKM22" role="doc-biblioref">Attrapadung et al. 2022</a>)</span> are the only fours tested on CIFAR-10 or any harder datasets using deep NNs (VGG-16).</p>
<div class="layout-chunk" data-layout="l-body">
<table>
<caption><span id="tab:reported-training">Table 3: </span>REPORTED ACCURACY AND THROUGHPUT ON LAN</caption>
<thead>
<tr class="header">
<th style="text-align: left;">Training Framework</th>
<th style="text-align: right;">Accuracy</th>
<th style="text-align: right;">Throughput (Image/hr)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">FalconN</td>
<td style="text-align: right;">76.8%</td>
<td style="text-align: right;">1482</td>
</tr>
<tr class="even">
<td style="text-align: left;">Accuracy</td>
<td style="text-align: right;">82.3%</td>
<td style="text-align: right;">9489</td>
</tr>
<tr class="odd">
<td style="text-align: left;">P-FalconN</td>
<td style="text-align: right;">55.1%</td>
<td style="text-align: right;">15152</td>
</tr>
<tr class="even">
<td style="text-align: left;">AdamInPrivate</td>
<td style="text-align: right;">75%</td>
<td style="text-align: right;">4171</td>
</tr>
</tbody>
</table>
</div>
<p>As shown in Table <a href="#tab:reported-training">3</a>,
CryptGPU has the highest accuracy of <span class="math inline">\(82.3\%\)</span>,
while P-FalconN (Piranha’s instantiation on FalconN) has the highest throughput.
Yet, CryptGPU may still be the winner since accuracy is usually more important in NN training,
when CryptGPU’s throughput (<span class="math inline">\(9489\)</span> images/hr) is only slightly lower than P-FalconN’s (<span class="math inline">\(15152\)</span> images/hr).</p>
<p>We note that frameworks for oblivious inference can attain <span class="math inline">\({&gt;}90\%\)</span> accuracy with the same architecture (VGG-16) adopted by CryptGPU.
It implies the potential for improvements.</p>
<p><strong>Challenge 15.</strong>
<em>How to attain a higher level of accuracy in private training? For instance, what other (normalization) layers can we use with what other approximation approaches?</em></p>
<h2 id="sec:discussion">Discussion</h2>
<p>We give some brief guidance on what PPNN frameworks to use.
The threat models and performance are the main concerns.
If the users trust some non-colluding servers, they can outsource the NN computation for the best-in-the-class performance
since non-colluding MPC frameworks often feature efficient outsourced training and inference.</p>
<p>If the users do not trust non-colluding servers, they have two choices left:
1. For inference expecting nearly real-time results, e.g., facial recognition, they can run S/C frameworks, e.g., GForce <span class="citation" data-cites="uss/NgC21">(<a href="#ref-uss/NgC21" role="doc-biblioref">Ng and Chow 2021</a>)</span>, with a reasonably good network (<span class="math inline">\(\leq 50\)</span>ms network latency and <span class="math inline">\(\geq 100\)</span>Mbps) and using pre-computation to save online inference latency.
1. For applications that can wait or involve many sub-queries, e.g., medical diagnosis and financial data analysis. These applications can use pure-HE frameworks, e.g., CryptoDL, to achieve high throughput (but high inference latency) using SIMD, i.e., running the queries in batches.</p>
<p>Without non-colluding servers,
E2DM and GarbledNN support outsourced inference, and Glyph supports outsourced training.
Yet, their performance is far from the S/C and pure-HE frameworks.
For detailed discussions about the state-of-the-art frameworks in different settings,
see <a href="#sec:sota-diff-setting">the Section on SoTA</a> and Fig. <a href="#fig:soa-perf">7</a>.</p>
<p>We end with remarks on two popular application areas.
Natural language processing often runs on RNNs,
which require heavy computation due to recurrent calls to some gigantic NN
that contains complex non-linear functions.
SiRnn <span class="citation" data-cites="sp/RatheeRGGSCR21">(<a href="#ref-sp/RatheeRGGSCR21" role="doc-biblioref">Rathee et al. 2021</a>)</span> is the only efficient (non-colluding) framework for oblivious inference.</p>
<p>CryptGPU, FalconN, and AdamInPrivate,
being non-colluding frameworks,
also support those complex non-linear functions
and thus should be able to train and infer over RNNs.</p>
<p>Graph NNs, which apply NNs for graphs,
can use the techniques in this SoK in oblivious settings.
Yet, it poses new challenges on how to hide the topology of the graphs.</p>
<h2 id="sec:sota-diff-setting">The State of the Art for Various Setups/Features</h2>
<div class="layout-chunk" data-layout="l-body">
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:soa-perf"></span>
<img src="images/manager.png" alt="The State of the Art with Different Features/Setups" width="65%" />
<p class="caption">
Figure 7: The State of the Art with Different Features/Setups
</p>
</div>
</div>
<p>This section shows which framework best fits a given scenario.
Figure <a href="#fig:soa-perf">7</a> serves as a decision tree guiding the choice.</p>
<h3 id="oblivious-inference-without-non-colluding-servers">Oblivious Inference without Non-colluding Servers</h3>
<p>Figure <a href="#fig:cifar-acc-on-lat">1</a> shows the accuracy and latency in a LAN setting of <span class="math inline">\({&gt;}1\)</span>Gbps and <span class="math inline">\({&lt;}5\)</span>ms communication latency.</p>
<h4 id="low-latency-inference.">Low-Latency Inference.</h4>
<p>COINN <span class="citation" data-cites="ccs/HussainJSK21">(<a href="#ref-ccs/HussainJSK21" role="doc-biblioref">Hussain et al. 2021</a>)</span> and
GForce <span class="citation" data-cites="uss/NgC21">(<a href="#ref-uss/NgC21" role="doc-biblioref">Ng and Chow 2021</a>)</span> are on the <em>Pareto frontier</em>,
i.e., not superseded by any other in both accuracy and latency.
Also, their latency is low (<span class="math inline">\(0.01{\sim}1\)</span>s) for real-time usage.</p>
<p>Note that the servers and the clients may need offline preparation before the queries arrive.</p>
<h4 id="high-throughput-inference.">High-Throughput Inference.</h4>
<p>Online throughput shows the volume of queries answered in a given time.
GForce still demonstrates high throughput,
and COINN has its superiority in its high accuracy.
HCNN <span class="citation" data-cites="tetc/BadawiJLMJTNAC21">(<a href="#ref-tetc/BadawiJLMJTNAC21" role="doc-biblioref">Badawi et al. 2021</a>)</span> exploits parallelism to achieve the highest throughput
but attains relatively low accuracy (<span class="math inline">\(80\%\)</span>).</p>
<h4 id="oblivious-inference-under-nonideal-network-conditions.">Oblivious Inference under Nonideal Network Conditions.</h4>
<p>Oblivious inference may go under a WAN (vs. the “ideal” LAN) setting,
i.e., <span class="math inline">\({&lt;}320\)</span> Mbps and <span class="math inline">\({&gt;}40\)</span>ms latency.
If large NNs for complex tasks and accuracy are paramount,
S/C frameworks still seem to be the best, e.g., GForce <span class="citation" data-cites="uss/NgC21">(<a href="#ref-uss/NgC21" role="doc-biblioref">Ng and Chow 2021</a>)</span> still performs an inference within one minute, as shown in Figure <a href="#fig:re-eval-wan-lat">3</a>.</p>
<p>For smaller NNs and easier tasks, e.g., MNIST,
pure-HE frameworks perform better for their constant communication round.
For throughput, CryptoDL <span class="citation" data-cites="popets/HesamifardTGW18">(<a href="#ref-popets/HesamifardTGW18" role="doc-biblioref">Hesamifard et al. 2018</a>)</span>, nGraph-HE <span class="citation" data-cites="cf/BoemerLCW19">(<a href="#ref-cf/BoemerLCW19" role="doc-biblioref">Boemer, Lao, et al. 2019</a>)</span>, and HCNN <span class="citation" data-cites="tetc/BadawiJLMJTNAC21">(<a href="#ref-tetc/BadawiJLMJTNAC21" role="doc-biblioref">Badawi et al. 2021</a>)</span> haVE the highest throughput and accuracy, as shown in Figure <a href="#fig:mnist-acc-on-tp">5</a>.
For latency, LoLa <span class="citation" data-cites="icml/BrutzkusGE19">(<a href="#ref-icml/BrutzkusGE19" role="doc-biblioref">Brutzkus, Gilad-Bachrach, and Elisha 2019</a>)</span>, E2DM <span class="citation" data-cites="ccs/JiangKLS18">(<a href="#ref-ccs/JiangKLS18" role="doc-biblioref">Jiang et al. 2018</a>)</span>, and SEALion <span class="citation" data-cites="van2019sealion">(<a href="#ref-van2019sealion" role="doc-biblioref">Elsloo, Patrini, and Ivey-Law 2019</a>)</span>
are on the Pareto frontier in Figure <a href="#fig:mnist-acc-on-lat">6</a>.</p>
<h4 id="outsourced-traininginference.">Outsourced Training/Inference.</h4>
<p>Glyph <span class="citation" data-cites="nips/LouFF020">(<a href="#ref-nips/LouFF020" role="doc-biblioref">Lou et al. 2020</a>)</span> is the only option for outsourced training (but not private training).
It also supports outsourced inference, but E2DM <span class="citation" data-cites="ccs/JiangKLS18">(<a href="#ref-ccs/JiangKLS18" role="doc-biblioref">Jiang et al. 2018</a>)</span> has better performance for using LHE
and does not leak the weights of the first few layers to the client like Glyph.</p>
<h3 id="do-non-colluding-based-frameworks-always-win">Do Non-colluding-based Frameworks Always Win?</h3>
<p>For oblivious inference,
Figure <a href="#fig:cifar-acc-on-lat">1</a> shows that GForce has the lowest latency, outperforming non-colluding frameworks.
CryptGPU strikes the best balance of throughput and accuracy.
Only Glyph can support secure training without non-colluding servers.
Note that it requires public datasets for transfer training.</p>
<h3 id="non-colluding-mpc-settings">Non-colluding MPC Settings</h3>
<h4 id="non-colluding-servers."><span class="math inline">\(2\)</span> Non-colluding Servers.</h4>
<p>SecureML and Quotient <span class="citation" data-cites="ccs/AgrawalSKG19">(<a href="#ref-ccs/AgrawalSKG19" role="doc-biblioref">Agrawal et al. 2019</a>)</span>
are the only two.
(ABY2.0 does not provide accuracy.)
Quotient is reported to produce NNs with <span class="math inline">\(6\)</span>pp higher accuracy on MNIST and
<span class="math inline">\({&gt;}5\times\)</span>/<span class="math inline">\(50\times\)</span> training throughput in the LAN/WAN setting.</p>
<h4 id="non-colluding-servers.-1"><span class="math inline">\(3\)</span> Non-colluding Servers.</h4>
<p>CryptGPU <span class="citation" data-cites="sp/TanKTW21">(<a href="#ref-sp/TanKTW21" role="doc-biblioref">Tan et al. 2021</a>)</span>
reports the highest accuracy, lower latency, and higher throughput than
FalconN, SecureNN, and CrypTen in CIFAR-10.
Yet, its throughput and latency are slightly worse than Piranha <span class="citation" data-cites="uss/WatsonWP22">(<a href="#ref-uss/WatsonWP22" role="doc-biblioref">Watson, Wagh, and Popa 2022</a>)</span>,
as discussed in <a href="#sec:benchmark">the Section on benchmark</a>.</p>
<p>Other <span class="math inline">\(3\)</span>PC solutions (e.g., Blaze <span class="citation" data-cites="ndss/PatraS20">(<a href="#ref-ndss/PatraS20" role="doc-biblioref">Patra and Suresh 2020</a>)</span> and Swift <span class="citation" data-cites="uss/KotiPPS21">(<a href="#ref-uss/KotiPPS21" role="doc-biblioref">Koti et al. 2021</a>)</span>) only support inference and are tested on MNIST but not harder datasets.
We remark that some of them can defend against malicious adversaries.</p>
<h4 id="non-colluding-servers.-2"><span class="math inline">\(4\)</span> Non-colluding Servers.</h4>
<p>Fantastic Four <span class="citation" data-cites="uss/Dalskov0K21">(<a href="#ref-uss/Dalskov0K21" role="doc-biblioref">Dalskov, Escudero, and Keller 2021</a>)</span> reports that it has slightly better performance
than Swift <span class="citation" data-cites="uss/KotiPPS21">(<a href="#ref-uss/KotiPPS21" role="doc-biblioref">Koti et al. 2021</a>)</span> and Flash <span class="citation" data-cites="popets/ByaliCPS20">(<a href="#ref-popets/ByaliCPS20" role="doc-biblioref">Byali et al. 2020</a>)</span>.
How it <span class="citation" data-cites="uss/Dalskov0K21">(<a href="#ref-uss/Dalskov0K21" role="doc-biblioref">Dalskov, Escudero, and Keller 2021</a>)</span> compared to Trident <span class="citation" data-cites="ndss/RachuriS20">(<a href="#ref-ndss/RachuriS20" role="doc-biblioref">Rachuri and Suresh 2020</a>)</span> is not clear
because they have not compared themselves with each other but just emphasized their speed-up over ABY3 <span class="citation" data-cites="ccs/MohasselR18">(<a href="#ref-ccs/MohasselR18" role="doc-biblioref">Mohassel and Rindal 2018</a>)</span> as a reference point.
Notably, they do not provide their training or inference accuracy over CIFAR-10.</p>
<h3 id="sec:malicious">Defending against Malicious Adversaries</h3>
<p>A recent pursuit is to ensure security even when one server deviates from the protocol (but it still cannot collude with other servers) by fully exploiting non-colluding assumptions.</p>
<p>In the basic malicious setting (ABY3 <span class="citation" data-cites="ccs/MohasselR18">(<a href="#ref-ccs/MohasselR18" role="doc-biblioref">Mohassel and Rindal 2018</a>)</span>, SecureNN <span class="citation" data-cites="popets/WaghGC19">(<a href="#ref-popets/WaghGC19" role="doc-biblioref">Wagh, Gupta, and Chandran 2019</a>)</span>, and FalconN <span class="citation" data-cites="popets/WaghTBKMR21">(<a href="#ref-popets/WaghTBKMR21" role="doc-biblioref">Wagh et al. 2021</a>)</span>), honest servers would <em>abort</em></p>
<p>if a corrupt server deviates from the protocol.
For <em>fairness</em> (<em>Malicious Security</em> in <a href="#summary-fw">our Summary of all Frameworks</a>), all honest parties can get the output (of internal protocols) if the corrupted can receive it (Blaze <span class="citation" data-cites="ndss/PatraS20">(<a href="#ref-ndss/PatraS20" role="doc-biblioref">Patra and Suresh 2020</a>)</span> and Trident <span class="citation" data-cites="ndss/RachuriS20">(<a href="#ref-ndss/RachuriS20" role="doc-biblioref">Rachuri and Suresh 2020</a>)</span>), even if the corrupted is malicious (i.e., <em>guarantee of delivery</em> (G.O.D.) of Flash <span class="citation" data-cites="popets/ByaliCPS20">(<a href="#ref-popets/ByaliCPS20" role="doc-biblioref">Byali et al. 2020</a>)</span> and Swift <span class="citation" data-cites="uss/KotiPPS21">(<a href="#ref-uss/KotiPPS21" role="doc-biblioref">Koti et al. 2021</a>)</span>).</p>
<p>Muse <span class="citation" data-cites="uss/LehmkuhlMSP21">(<a href="#ref-uss/LehmkuhlMSP21" role="doc-biblioref">Lehmkuhl et al. 2021</a>)</span> features “abort security” against clients with higher efficiency than pure-GC approaches <span class="citation" data-cites="uss/RiaziS0LLK19 eprint:GarbledNN">(<a href="#ref-uss/RiaziS0LLK19" role="doc-biblioref">Riazi et al. 2019</a>; <a href="#ref-eprint:GarbledNN" role="doc-biblioref">Ball et al. 2019</a>)</span>.</p>
<h2 id="genealogy">Genealogy</h2>
<div class="layout-chunk" data-layout="l-body">
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:genealogy"></span>
<img src="images/genealogy.png" alt="The Genealogy of Frameworks (Names we came up with: BNormCrypt, GarbledNN, and FalconI/N for Inference/Training)" width="100%" />
<p class="caption">
Figure 8: The Genealogy of Frameworks (Names we came up with: BNormCrypt, GarbledNN, and FalconI/N for Inference/Training)
</p>
</div>
</div>
<h2 id="summary-fw">Summary of Frameworks</h2>
<iframe class="l-page" src="./tables/tables.html" height="800px" frameBorder="0">
</iframe>
<h2 id="interactive-chart">Interactive Chart</h2>
<p>The following chart shows the accuracy and throughput/latency under various setting.
You can scroll to zoom in or out the chart.
You may use the drop-down lists to change the setting.</p>
<ul>
<li>For throughput, the closer to the <em>top-right</em> the better.</li>
<li>For latency, the closer to the <em>top-left</em> the better.</li>
</ul>
<iframe class="l-body-outset" src="./charts/charts.html" height="800px" frameBorder="0" style="border:0;">
</iframe>
<div class="sourceCode" id="cb2"><pre class="sourceCode r distill-force-highlighting-css"><code class="sourceCode r"></code></pre></div>
<div id="refs" class="references csl-bib-body hanging-indent" role="list">
<div id="ref-osdi/AbadiBCCDDDGIIK16" class="csl-entry" role="listitem">
Abadi, Martı́n, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, et al. 2016. <span>“<span>TensorFlow</span>: <span>A</span> System for Large-Scale Machine Learning.”</span> In <em><span>OSDI</span></em>.
</div>
<div id="ref-ccs/AgrawalSKG19" class="csl-entry" role="listitem">
Agrawal, Nitin, Ali Shahin Shamsabadi, Matt J. Kusner, and Adrià Gascón. 2019. <span>“QUOTIENT: Two-Party Secure Neural Network Training and Prediction.”</span> In <em>CCS</em>, 1231–47.
</div>
<div id="ref-popets/AttrapadungHIKM22" class="csl-entry" role="listitem">
Attrapadung, Nuttapong, Koki Hamada, Dai Ikarashi, Ryo Kikuchi, Takahiro Matsuda, Ibuki Mishina, Hiraku Morita, and Jacob C. N. Schuldt. 2022. <span>“Adam in Private: Secure and Fast Training of Deep Neural Networks with Adaptive Moment Estimation.”</span> <em>PoPETs</em>, no. 4: 746–67.
</div>
<div id="ref-primelife/AzraouiBBCCEMMO19" class="csl-entry" role="listitem">
Azraoui, Monir, Muhammad Barham, Beyza Bozdemir, Sébastien Canard, Eleonora Ciceri, Orhan Ermis, Ramy Masalha, et al. 2019. <span>“<span>SoK: C</span>ryptography for Neural Networks.”</span> In <em>Summer Sch. On Pri. &amp; Id. Mgt. (Data for Better Living)</em>, 63–81.
</div>
<div id="ref-tetc/BadawiJLMJTNAC21" class="csl-entry" role="listitem">
Badawi, Ahmad Al, Chao Jin, Jie Lin, Chan Fook Mun, Sim Jun Jie, Benjamin Hong Meng Tan, Xiao Nan, Khin Mi Mi Aung, and Vijay Ramaseshan Chandrasekhar. 2021. <span>“Towards the <span>AlexNet</span> Moment for Homomorphic Encryption: <span>HCNN</span>, the First Homomorphic <span>CNN</span> on Encrypted Data with <span>GPUs</span>.”</span> <em><span>IEEE</span> Trans. Emerg. Top. Comput.</em> 9 (3): 1330–43.
</div>
<div id="ref-tches/BadawiVMA18" class="csl-entry" role="listitem">
Badawi, Ahmad Al, Bharadwaj Veeravalli, Chan Fook Mun, and Khin Mi Mi Aung. 2018. <span>“High-Performance <span>FV</span> Somewhat Homomorphic Encryption on <span>GPUs</span>: An Implementation Using <span>CUDA</span>.”</span> <em>TCHES</em>, no. 2: 70–95.
</div>
<div id="ref-eprint:GarbledNN" class="csl-entry" role="listitem">
Ball, Marshall, Brent Carmer, Tal Malkin, Mike Rosulek, and Nichole Schimanski. 2019. <span>“Garbled Neural Networks Are Practical.”</span> PPML.
</div>
<div id="ref-mmsec/BarniOP06" class="csl-entry" role="listitem">
Barni, Mauro, Claudio Orlandi, and Alessandro Piva. 2006. <span>“A Privacy-Preserving Protocol for Neural-Network-Based Computation.”</span> In <em>MM&amp;sec</em>.
</div>
<div id="ref-crypto/Beaver91a" class="csl-entry" role="listitem">
Beaver, Donald. 1991. <span>“Efficient Multiparty Protocols Using Circuit Randomization.”</span> In <em><span>CRYPTO</span></em>.
</div>
<div id="ref-ecai/BianJLSS20" class="csl-entry" role="listitem">
Bian, Song, Weiwen Jiang, Qing Lu, Yiyu Shi, and Takashi Sato. 2020. <span>“<span>NASS:</span> Optimizing Secure Inference via Neural Architecture Search.”</span> In <em>ECAI</em>, 1746–53.
</div>
<div id="ref-wahc/BoemerCCW19" class="csl-entry" role="listitem">
Boemer, Fabian, Anamaria Costache, Rosario Cammarota, and Casimir Wierzynski. 2019. <span>“<span class="nocase">nGraph-HE2</span>: <span>A</span> High-Throughput Framework for Neural Network Inference on Encrypted Data.”</span> In <em>Enc. Comp. &amp; App. Homo. Crypt. (WAHC)</em>, 45–56.
</div>
<div id="ref-cf/BoemerLCW19" class="csl-entry" role="listitem">
Boemer, Fabian, Yixing Lao, Rosario Cammarota, and Casimir Wierzynski. 2019. <span>“<span class="nocase">nGraph-HE</span>: A Graph Compiler for Deep Learning on Homomorphically Encrypted Data.”</span> In <em>Computing Frontiers</em>, 3–13.
</div>
<div id="ref-nutmic/BouraGGJ19" class="csl-entry" role="listitem">
Boura, Christina, Nicolas Gama, Mariya Georgieva, and Dimitar Jetchev. 2019. <span>“CHIMERA: Combining Ring-<span>LWE</span>-Based Fully Homomorphic Encryption Schemes.”</span> In <em>Number-Theoretic Methods in Cryptology (NuTMiC)</em>.
</div>
<div id="ref-crypto/BourseMMP18" class="csl-entry" role="listitem">
Bourse, Florian, Michele Minelli, Matthias Minihold, and Pascal Paillier. 2018. <span>“Fast Homomorphic Evaluation of Deep Discretized Neural Networks.”</span> In <em>CRYPTO <span>III</span></em>, 483–512.
</div>
<div id="ref-crypto/Brakerski12" class="csl-entry" role="listitem">
Brakerski, Zvika. 2012. <span>“Fully Homomorphic Encryption Without Modulus Switching from Classical <span>GapSVP</span>.”</span> In <em>CRYPTO</em>, 868–86. <a href="https://doi.org/10.1007/978-3-642-32009-5_50">https://doi.org/10.1007/978-3-642-32009-5_50</a>.
</div>
<div id="ref-toct/BrakerskiGV14" class="csl-entry" role="listitem">
Brakerski, Zvika, Craig Gentry, and Vinod Vaikuntanathan. 2014. <span>“<span>(Leveled) Fully</span> Homomorphic Encryption Without Bootstrapping.”</span> <em><span>ACM</span> Trans. Comput. Theory</em> 6 (3): 13:1–36.
</div>
<div id="ref-icml/BrutzkusGE19" class="csl-entry" role="listitem">
Brutzkus, Alon, Ran Gilad-Bachrach, and Oren Elisha. 2019. <span>“Low Latency Privacy Preserving Inference.”</span> In <em>ICML</em>, 812–21.
</div>
<div id="ref-popets/ByaliCPS20" class="csl-entry" role="listitem">
Byali, Megha, Harsh Chaudhari, Arpita Patra, and Ajith Suresh. 2020. <span>“FLASH: Fast and Robust Framework for Privacy-Preserving Machine Learning.”</span> <em>PoPETs</em>, no. 2: 459–80.
</div>
<div id="ref-popets/Cabrero-Holgueras21" class="csl-entry" role="listitem">
Cabrero-Holgueras, José, and Sergio Pastrana. 2021. <span>“<span>SoK: P</span>rivacy-Preserving Computation Techniques for Deep Learning.”</span> <em>PoPETs</em>, no. 4: 139–62.
</div>
<div id="ref-fc/CatrinaS10" class="csl-entry" role="listitem">
Catrina, Octavian, and Amitabh Saxena. 2010. <span>“Secure Computation with Fixed-Point Numbers.”</span> In <em>Financial Cryptography and Data Security</em>, 35–50.
</div>
<div id="ref-eprint/2017:035" class="csl-entry" role="listitem">
Chabanne, Hervé, Amaury de Wargny, Jonathan Milgram, Constance Morel, and Emmanuel Prouff. 2017. <span>“Privacy-Preserving Classification on Deep Neural Network.”</span> Cryptology ePrint 2017/035.
</div>
<div id="ref-eurosp/ChandranGRST19" class="csl-entry" role="listitem">
Chandran, Nishanth, Divya Gupta, Aseem Rastogi, Rahul Sharma, and Shardul Tripathi. 2019. <span>“<span>EzPC</span>: Programmable and Efficient Secure Two-Party Computation for Machine Learning.”</span> In <em>Euro<span>S&amp;P</span></em>, 496–511.
</div>
<div id="ref-iccd/ChenCVR19" class="csl-entry" role="listitem">
Chen, Huili, Rosario Cammarota, Felipe Valencia, and Francesco Regazzoni. 2019. <span>“<span>PlaidML-HE</span>: Acceleration of Deep Learning Kernels to Compute on Encrypted Data.”</span> In <em>Intl’ Conf. On Comp. Design (ICCD)</em>, 333–36.
</div>
<div id="ref-asiacrypt/CheonKKS17" class="csl-entry" role="listitem">
Cheon, Jung Hee, Andrey Kim, Miran Kim, and Yong Soo Song. 2017. <span>“Homomorphic Encryption for Arithmetic of Approximate Numbers.”</span> In <em>AsiaCrypt Part <span>I</span></em>, 409–37. <a href="https://doi.org/10.1007/978-3-319-70694-8\_15">https://doi.org/10.1007/978-3-319-70694-8\_15</a>.
</div>
<div id="ref-joc/ChillottiGGI20" class="csl-entry" role="listitem">
Chillotti, Ilaria, Nicolas Gama, Mariya Georgieva, and Malika Izabachène. 2020. <span>“<span>TFHE:</span> Fast Fully Homomorphic Encryption over the Torus.”</span> <em>J. Crypt.</em> 33: 34–91.
</div>
<div id="ref-iccv/ChoH19" class="csl-entry" role="listitem">
Cho, Jang Hyun, and Bharath Hariharan. 2019. <span>“On the Efficacy of Knowledge Distillation.”</span> In <em>ICCV</em>, 4793–4801.
</div>
<div id="ref-ssst/ChoMBB14" class="csl-entry" role="listitem">
Cho, Kyunghyun, Bart van Merrienboer, Dzmitry Bahdanau, and Yoshua Bengio. 2014. <span>“On the Properties of Neural Machine Translation: Encoder-Decoder Approaches.”</span> In <em>SSST@EMNLP</em>.
</div>
<div id="ref-corr/fastercryptonets" class="csl-entry" role="listitem">
Chou, Edward, Josh Beal, Daniel Levy, Serena Yeung, Albert Haque, and Fei-Fei Li. 2018. <span>“Faster <span>CryptoNets</span>: Leveraging Sparsity for Real-World Encrypted Inference.”</span> arXiv:1811.09953.
</div>
<div id="ref-ndss/ChowLS09" class="csl-entry" role="listitem">
Chow, Sherman S. M., Jie-Han Lee, and Lakshminarayanan Subramanian. 2009. <span>“Two-Party Computation Model for Privacy-Preserving Queries over Distributed Databases.”</span> In <em>NDSS</em>.
</div>
<div id="ref-popets/Dalskov0K20" class="csl-entry" role="listitem">
Dalskov, Anders P. K., Daniel Escudero, and Marcel Keller. 2020. <span>“Secure Evaluation of Quantized Neural Networks.”</span> <em>PoPETS</em>, no. 4: 355–75.
</div>
<div id="ref-uss/Dalskov0K21" class="csl-entry" role="listitem">
———. 2021. <span>“Fantastic Four: Honest-Majority Four-Party Secure Computation with Malicious Security.”</span> In <em>USENIX Security Symposium</em>, 2183–2200.
</div>
<div id="ref-acisp/DamgardGK07" class="csl-entry" role="listitem">
Damgård, Ivan, Martin Geisler, and Mikkel Krøigaard. 2007. <span>“Efficient and Secure Comparison for on-Line Auctions.”</span> In <em>ACISP</em>, 416–30.
</div>
<div id="ref-pldi/DathathriSCLLMM19" class="csl-entry" role="listitem">
Dathathri, Roshan, Olli Saarikivi, Hao Chen, Kim Laine, Kristin E. Lauter, Saeed Maleki, Madanlal Musuvathi, and Todd Mytkowicz. 2019. <span>“<span>CHET:</span> An Optimizing Compiler for Fully-Homomorphic Neural-Network Inferencing.”</span> In <em>PLDI</em>, 142–56.
</div>
<div id="ref-ndss/DemmlerSZ15" class="csl-entry" role="listitem">
Demmler, Daniel, Thomas Schneider, and Michael Zohner. 2015. <span>“<span>ABY - A</span> Framework for Efficient Mixed-Protocol Secure Two-Party Computation.”</span> In <em>NDSS</em>.
</div>
<div id="ref-ndss/DessoukyKS0ZZ17" class="csl-entry" role="listitem">
Dessouky, Ghada, Farinaz Koushanfar, Ahmad-Reza Sadeghi, Thomas Schneider, Shaza Zeitouni, and Michael Zohner. 2017. <span>“Pushing the Communication Barrier in Secure Computation Using Lookup Tables.”</span> In <em><span>NDSS</span></em>.
</div>
<div id="ref-www/DuYCS23" class="csl-entry" role="listitem">
Du, Minxin, Xiang Yue, Sherman S. M. Chow, and Huan Sun. 2023. <span>“Sanitizing Sentence Embeddings (and Labels) for Local Differential Privacy.”</span> In <em>The Web (WWW)</em>, 2349–59.
</div>
<div id="ref-ccs/DuYCWHS23" class="csl-entry" role="listitem">
Du, Minxin, Xiang Yue, Sherman S. M. Chow, Tianhao Wang, Chenyu Huang, and Huan Sun. 2023. <span>“<span>DP-Forward</span>: Fine-Tuning and Inference on Language Models with Differential Privacy in Forward Pass.”</span> In <em>CCS</em>.
</div>
<div id="ref-ais/DuA01" class="csl-entry" role="listitem">
Du, Wenliang, and Mikhail J. Atallah. 2001. <span>“Protocols for Secure Remote Database Access with Approximate Matching.”</span> In <em>E-Commerce Security &amp; Privacy</em>, 87–111.
</div>
<div id="ref-tcc/DworkMNS06" class="csl-entry" role="listitem">
Dwork, Cynthia, Frank McSherry, Kobbi Nissim, and Adam D. Smith. 2006. <span>“Calibrating Noise to Sensitivity in Private Data Analysis.”</span> In <em><span>TCC</span></em>, 265–84.
</div>
<div id="ref-van2019sealion" class="csl-entry" role="listitem">
Elsloo, Tim van, Giorgio Patrini, and Hamish Ivey-Law. 2019. <span>“<span>SEALion</span>: A Framework for Neural Network Inference on Encrypted Data.”</span> arXiv:1904.12840.
</div>
<div id="ref-eprint/FanV12" class="csl-entry" role="listitem">
Fan, Junfeng, and Frederik Vercauteren. 2012. <span>“Somewhat Practical Fully Homomorphic Encryption.”</span> Cryptology ePrint, 2012/144.
</div>
<div id="ref-ccs/FredriksonJR15" class="csl-entry" role="listitem">
Fredrikson, Matt, Somesh Jha, and Thomas Ristenpart. 2015. <span>“Model Inversion Attacks That Exploit Confidence Information &amp; Basic Countermeasures.”</span> In <em>CCS</em>.
</div>
<div id="ref-stoc/Gentry09" class="csl-entry" role="listitem">
Gentry, Craig. 2009. <span>“Fully Homomorphic Encryption Using Ideal Lattices.”</span> In <em>STOC</em>, 169–78.
</div>
<div id="ref-nips/GhodsiJ21" class="csl-entry" role="listitem">
Ghodsi, Zahra, Nandan Kumar Jha, Brandon Reagen, and Siddharth Garg. 2021. <span>“Circa: Stochastic <span>ReLUs</span> for Private Deep Learning.”</span> In <em>NeurIPS</em>.
</div>
<div id="ref-nips/GhodsiVRG20" class="csl-entry" role="listitem">
Ghodsi, Zahra, Akshaj Kumar Veldanda, Brandon Reagen, and Siddharth Garg. 2020. <span>“<span>CryptoNAS</span>: Private Inference on a <span>ReLU</span> Budget.”</span> In <em>NeurIPS</em>.
</div>
<div id="ref-icml/Gilad-BachrachD16" class="csl-entry" role="listitem">
Gilad-Bachrach, Ran, Nathan Dowlin, Kim Laine, Kristin E. Lauter, Michael Naehrig, and John Wernsing. 2016. <span>“<span>CryptoNets</span>: Applying Neural Networks to Encrypted Data with High Throughput and Accuracy.”</span> In <em><span>ICML</span></em>.
</div>
<div id="ref-stoc/GoldreichMW87" class="csl-entry" role="listitem">
Goldreich, Oded, Silvio Micali, and Avi Wigderson. 1987. <span>“How to Play Any Mental Game or <span>A</span> Completeness Theorem for Protocols with Honest Majority.”</span> In <em>STOC</em>.
</div>
<div id="ref-iclr/GoodfellowSS15" class="csl-entry" role="listitem">
Goodfellow, Ian J., Jonathon Shlens, and Christian Szegedy. 2015. <span>“Explaining and Harnessing Adversarial Examples.”</span> In <em>ICLR</em>.
</div>
<div id="ref-cvpr/HeZRS16" class="csl-entry" role="listitem">
He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. <span>“Deep Residual Learning for Image Recognition.”</span> In <em>CVPR</em>.
</div>
<div id="ref-ccs/HeathK21" class="csl-entry" role="listitem">
Heath, David, and Vladimir Kolesnikov. 2021. <span>“One Hot Garbling.”</span> In <em>CCS</em>.
</div>
<div id="ref-popets/HesamifardTGW18" class="csl-entry" role="listitem">
Hesamifard, Ehsan, Hassan Takabi, Mehdi Ghasemi, and Rebecca N. Wright. 2018. <span>“Privacy-Preserving Machine Learning as a Service.”</span> <em>PoPETs</em>, no. 3: 123–42.
</div>
<div id="ref-sci/HieCB18" class="csl-entry" role="listitem">
Hie, Brian, Hyunghoon Cho, and Bonnie Berger. 2018. <span>“Realizing Private and Practical Pharmacological Collaboration.”</span> <em>Science</em> 362 (6412): 347–50.
</div>
<div id="ref-neco/HochreiterS97" class="csl-entry" role="listitem">
Hochreiter, Sepp, and Jürgen Schmidhuber. 1997. <span>“Long Short-Term Memory.”</span> <em>Neural Comput.</em> 9 (8): 1735–80.
</div>
<div id="ref-nips/HubaraCSEB16" class="csl-entry" role="listitem">
Hubara, Itay, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. 2016. <span>“Binarized Neural Networks.”</span> In <em>NIPS</em>, 4107–15.
</div>
<div id="ref-jmlr/HubaraCSEB17" class="csl-entry" role="listitem">
———. 2017. <span>“Quantized Neural Networks: Training Neural Networks with Low Precision Weights and Activations.”</span> <em>JMLR</em> 18: 187:1–30.
</div>
<div id="ref-ccs/HussainJSK21" class="csl-entry" role="listitem">
Hussain, Siam Umar, Mojan Javaheripi, Mohammad Samragh, and Farinaz Koushanfar. 2021. <span>“<span>COINN:</span> <span>Crypto/ML</span> Codesign for Oblivious Inference via Neural Networks.”</span> In <em><span>CCS</span></em>.
</div>
<div id="ref-icml/IoffeS15" class="csl-entry" role="listitem">
Ioffe, Sergey, and Christian Szegedy. 2015. <span>“Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift.”</span> In <em>ICML</em>.
</div>
<div id="ref-crypto/IshaiKNP03" class="csl-entry" role="listitem">
Ishai, Yuval, Joe Kilian, Kobbi Nissim, and Erez Petrank. 2003. <span>“Extending Oblivious Transfers Efficiently.”</span> In <em>CRYPTO</em>, 145–61.
</div>
<div id="ref-cvpr/JacobKCZTHAK18" class="csl-entry" role="listitem">
Jacob, Benoit, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew G. Howard, Hartwig Adam, and Dmitry Kalenichenko. 2018. <span>“Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference.”</span> In <em>CVPR</em>, 2704–13.
</div>
<div id="ref-icml/JhaGGR21" class="csl-entry" role="listitem">
Jha, Nandan Kumar, Zahra Ghodsi, Siddharth Garg, and Brandon Reagen. 2021. <span>“<span>DeepReDuce: ReLU</span> Reduction for Fast Private Inference.”</span> In <em><span>ICML</span></em>, 4839–49.
</div>
<div id="ref-mm/JiaSDKLGGD14" class="csl-entry" role="listitem">
Jia, Yangqing, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross B. Girshick, Sergio Guadarrama, and Trevor Darrell. 2014. <span>“Caffe: Convolutional Architecture for Fast Feature Embedding.”</span> In <em>ACM MM</em>.
</div>
<div id="ref-ccs/JiangKLS18" class="csl-entry" role="listitem">
Jiang, Xiaoqian, Miran Kim, Kristin E. Lauter, and Yongsoo Song. 2018. <span>“Secure Outsourced Matrix Computation and Application to Neural Networks.”</span> In <em>CCS</em>, 1209–22.
</div>
<div id="ref-uss/JuvekarVC18" class="csl-entry" role="listitem">
Juvekar, Chiraag, Vinod Vaikuntanathan, and Anantha Chandrakasan. 2018. <span>“GAZELLE: A Low Latency Framework for Secure Neural Network Inference.”</span> In <em>USENIX Security Symposium</em>.
</div>
<div id="ref-nips/KnottVHSIM21" class="csl-entry" role="listitem">
Knott, Brian, Shobha Venkataraman, Awni Y. Hannun, Shubho Sengupta, Mark Ibrahim, and Laurens van der Maaten. 2021. <span>“<span>CrypTen</span>: Secure Multi-Party Computation Meets Machine Learning.”</span> In <em>NeurIPS</em>.
</div>
<div id="ref-uss/KotiPPS21" class="csl-entry" role="listitem">
Koti, Nishat, Mahak Pancholi, Arpita Patra, and Ajith Suresh. 2021. <span>“SWIFT: Super-Fast and Robust Privacy-Preserving Machine Learning.”</span> In <em>USENIX Security Symposium</em>, 2651–68.
</div>
<div id="ref-sp/KumarRCGR020" class="csl-entry" role="listitem">
Kumar, Nishant, Mayank Rathee, Nishanth Chandran, Divya Gupta, Aseem Rastogi, and Rahul Sharma. 2020. <span>“<span>CrypTFlow</span>: <span>Secure TensorFlow Inference</span>.”</span> In <em><span>S&amp;P</span></em>, 336–53.
</div>
<div id="ref-uss/LehmkuhlMSP21" class="csl-entry" role="listitem">
Lehmkuhl, Ryan, Pratyush Mishra, Akshayaram Srinivasan, and Raluca Ada Popa. 2021. <span>“Muse: Secure Inference Resilient to Malicious Clients.”</span> In <em>USENIX Security Symposium</em>, 2201–18.
</div>
<div id="ref-cvpr/LiXZDGWW20" class="csl-entry" role="listitem">
Li, Shaohua, Kaiping Xue, Bin Zhu, Chenkai Ding, Xindi Gao, David S. L. Wei, and Tao Wan. 2020. <span>“FALCON: <span>A F</span>ourier Transform Based Approach for Fast and Secure Convolutional Neural Network Predictions.”</span> In <em>CVPR</em>, 8702–11.
</div>
<div id="ref-ccs/LiuJLA17" class="csl-entry" role="listitem">
Liu, Jian, Mika Juuti, Yao Lu, and N. Asokan. 2017. <span>“Oblivious Neural Network Predictions via <span>MiniONN</span> Transformations.”</span> In <em>CCS</em>.
</div>
<div id="ref-nips/LivniSS14" class="csl-entry" role="listitem">
Livni, Roi, Shai Shalev-Shwartz, and Ohad Shamir. 2014. <span>“On the Computational Efficiency of Training Neural Networks.”</span> In <em>NIPS</em>, 855–63.
</div>
<div id="ref-nips/LouFF020" class="csl-entry" role="listitem">
Lou, Qian, Bo Feng, Geoffrey Charles Fox, and Lei Jiang. 2020. <span>“Glyph: Fast and Accurately Training Deep Neural Networks on Encrypted Data.”</span> In <em>NeurIPS</em>.
</div>
<div id="ref-nips/Lou019" class="csl-entry" role="listitem">
Lou, Qian, and Lei Jiang. 2019. <span>“<span>SHE</span>: <span>A</span> Fast and Accurate Deep Neural Network for Encrypted Data.”</span> In <em>NeurIPS</em>, 10035–43.
</div>
<div id="ref-iclr/LouSJ021" class="csl-entry" role="listitem">
Lou, Qian, Yilin Shen, Hongxia Jin, and Lei Jiang. 2021. <span>“<span>SafeNet</span>: <span>A</span> Secure, Accurate and Fast Neural Network Inference.”</span> In <em><span>ICLR</span></em>.
</div>
<div id="ref-corr/privacyInDeepLearning" class="csl-entry" role="listitem">
Mireshghallah, Fatemehsadat, Mohammadkazem Taram, Praneeth Vepakomma, Abhishek Singh, Ramesh Raskar, and Hadi Esmaeilzadeh. 2020. <span>“Privacy in Deep Learning: <span>A</span> Survey.”</span> arXiv.
</div>
<div id="ref-uss/MishraLSZP20" class="csl-entry" role="listitem">
Mishra, Pratyush, Ryan Lehmkuhl, Akshayaram Srinivasan, Wenting Zheng, and Raluca Ada Popa. 2020. <span>“Delphi: A Cryptographic Inference Service for Neural Networks.”</span> In <em>USENIX Security Symposium</em>.
</div>
<div id="ref-ccs/MohasselR18" class="csl-entry" role="listitem">
Mohassel, Payman, and Peter Rindal. 2018. <span>“<span>ABY<span class="math inline">\(^3\)</span></span>: <span>A</span> Mixed Protocol Framework for Machine Learning.”</span> In <em>CCS</em>, 35–52.
</div>
<div id="ref-sp/MohasselZ17" class="csl-entry" role="listitem">
Mohassel, Payman, and Yupeng Zhang. 2017. <span>“<span>SecureML</span>: <span>A</span> System for Scalable Privacy-Preserving Machine Learning.”</span> In <em><span>S&amp;P</span></em>.
</div>
<div id="ref-uss/NgC21" class="csl-entry" role="listitem">
Ng, Lucien K. L., and Sherman S. M. Chow. 2021. <span>“<span>GForce: GPU</span>-Friendly Oblivious and Rapid Neural Network Inference.”</span> In <em>USENIX Security Symposium</em>, 2147–64.
</div>
<div id="ref-aaai/NgCWW021" class="csl-entry" role="listitem">
Ng, Lucien K. L., Sherman S. M. Chow, Anna P. Y. Woo, Donald P. H. Wong, and Yongjun Zhao. 2021. <span>“Goten: <span>GPU</span>-Outsourcing Trusted Execution of Neural Network Training.”</span> In <em><span>AAAI</span></em>, 14876–83.
</div>
<div id="ref-ejisec/OrlandiPB07" class="csl-entry" role="listitem">
Orlandi, Claudio, Alessandro Piva, and Mauro Barni. 2007. <span>“Oblivious Neural Network Computing via Homomorphic Encryption.”</span> <em><span>EURASIP</span> J. Info. Sec.</em>
</div>
<div id="ref-eurosp/PapernotMSW18" class="csl-entry" role="listitem">
Papernot, Nicolas, Patrick D. McDaniel, Arunesh Sinha, and Michael P. Wellman. 2018. <span>“<span>SoK</span>: Security and Privacy in Machine Learning.”</span> In <em>Euro<span>S&amp;P</span></em>.
</div>
<div id="ref-uss/Patra0SY21" class="csl-entry" role="listitem">
Patra, Arpita, Thomas Schneider, Ajith Suresh, and Hossein Yalame. 2021. <span>“<span>ABY2.0:</span> Improved Mixed-Protocol Secure Two-Party Computation.”</span> In <em>USENIX Security Symposium</em>.
</div>
<div id="ref-ndss/PatraS20" class="csl-entry" role="listitem">
Patra, Arpita, and Ajith Suresh. 2020. <span>“BLAZE: Blazing Fast Privacy-Preserving Machine Learning.”</span> In <em>NDSS</em>.
</div>
<div id="ref-ndss/RachuriS20" class="csl-entry" role="listitem">
Rachuri, Rahul, and Ajith Suresh. 2020. <span>“Trident: Efficient 4<span>PC</span> Framework for Privacy Preserving Machine Learning.”</span> In <em><span>NDSS</span></em>.
</div>
<div id="ref-sp/RatheeRGGSCR21" class="csl-entry" role="listitem">
Rathee, Deevashwer, Mayank Rathee, Rahul-Kranti-Kiran Goli, Divya Gupta, Rahul Sharma, Nishanth Chandran, and Aseem Rastogi. 2021. <span>“<span>SiRnn</span>: <span>A</span> Math Library for Secure <span>RNN</span> Inference.”</span> In <em><span>S&amp;P</span></em>, 1003–20.
</div>
<div id="ref-ccs/RatheeR0CGR020" class="csl-entry" role="listitem">
Rathee, Deevashwer, Mayank Rathee, Nishant Kumar, Nishanth Chandran, Divya Gupta, Aseem Rastogi, and Rahul Sharma. 2020. <span>“<span>CrypTFlow2</span>: Practical 2-Party Secure Inference.”</span> In <em>CCS</em>, 325–42.
</div>
<div id="ref-ieeesp/RiaziRK19" class="csl-entry" role="listitem">
Riazi, M. Sadegh, Bita Darvish Rouhani, and Farinaz Koushanfar. 2019. <span>“Deep Learning on Private Data.”</span> <em><span>IEEE</span> Secur. Priv.</em> 17 (6): 54–63.
</div>
<div id="ref-uss/RiaziS0LLK19" class="csl-entry" role="listitem">
Riazi, M. Sadegh, Mohammad Samragh, Hao Chen, Kim Laine, Kristin E. Lauter, and Farinaz Koushanfar. 2019. <span>“<span class="smallcaps">XONN</span>: <span class="smallcaps">XNOR</span>-Based Oblivious Deep Neural Network Inference.”</span> In <em>USENIX Security Symposium</em>.
</div>
<div id="ref-asiaccs/RiaziWTS0K18" class="csl-entry" role="listitem">
Riazi, M. Sadegh, Christian Weinert, Oleksandr Tkachenko, Ebrahim M. Songhori, Thomas Schneider, and Farinaz Koushanfar. 2018. <span>“Chameleon: <span>A</span> Hybrid Secure Computation Framework for Machine Learning Applications.”</span> In <em>AsiaCCS</em>, 707–21.
</div>
<div id="ref-dac/RouhaniRK18" class="csl-entry" role="listitem">
Rouhani, Bita Darvish, M. Sadegh Riazi, and Farinaz Koushanfar. 2018. <span>“<span>DeepSecure</span>: Scalable Provably-Secure Deep Learning.”</span> In <em>Design Auto. Conf. (DAC)</em>, 2:1–6.
</div>
<div id="ref-icisc/SadeghiS08" class="csl-entry" role="listitem">
Sadeghi, Ahmad-Reza, and Thomas Schneider. 2008. <span>“Generalized Universal Circuits for Secure Evaluation of Private Functions with Application to Data Classification.”</span> In <em>ICISC</em>, 336–53.
</div>
<div id="ref-icml/SanyalKGK18" class="csl-entry" role="listitem">
Sanyal, Amartya, Matt J. Kusner, Adrià Gascón, and Varun Kanade. 2018. <span>“<span>TAPAS</span>: Tricks to Accelerate (Encrypted) Prediction as a Service.”</span> In <em>ICML</em>, 4497–4506.
</div>
<div id="ref-sp/ShokriSSS17" class="csl-entry" role="listitem">
Shokri, Reza, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. 2017. <span>“Membership Inference Attacks Against Machine Learning Models.”</span> In <em><span>S&amp;P</span></em>.
</div>
<div id="ref-iclr/SimonyanZ15" class="csl-entry" role="listitem">
Simonyan, Karen, and Andrew Zisserman. 2015. <span>“Very Deep Convolutional Networks for Large-Scale Image Recognition.”</span> In <em>ICLR</em>.
</div>
<div id="ref-sp/TanKTW21" class="csl-entry" role="listitem">
Tan, Sijun, Brian Knott, Yuan Tian, and David J. Wu. 2021. <span>“<span>CryptGPU</span>: Fast Privacy-Preserving Machine Learning on the <span>GPU</span>.”</span> In <em><span>S&amp;P</span></em>, 1021–38.
</div>
<div id="ref-ml4cs/TanuwidjajaCK19" class="csl-entry" role="listitem">
Tanuwidjaja, Harry Chandra, Rakyong Choi, and Kwangjo Kim. 2019. <span>“A Survey on Deep Learning Techniques for Privacy-Preserving.”</span> In <em>ML4CS</em>, 29–46.
</div>
<div id="ref-iclr/TramerB19" class="csl-entry" role="listitem">
Tramèr, Florian, and Dan Boneh. 2019. <span>“Slalom: Fast, Verifiable and Private Execution of Neural Networks in Trusted Hardware.”</span> In <em>ICLR</em>.
</div>
<div id="ref-uss/TramerZJRR16" class="csl-entry" role="listitem">
Tramèr, Florian, Fan Zhang, Ari Juels, Michael K. Reiter, and Thomas Ristenpart. 2016. <span>“Stealing Machine Learning Models via Prediction <span>APIs</span>.”</span> In <em>USENIX Security Symposium</em>, 601–18.
</div>
<div id="ref-wifs/Veugen12" class="csl-entry" role="listitem">
Veugen, Thijs. 2012. <span>“Improving the <span>DGK</span> Comparison Protocol.”</span> In <em><span>WIFS</span></em>.
</div>
<div id="ref-popets/WaghGC19" class="csl-entry" role="listitem">
Wagh, Sameer, Divya Gupta, and Nishanth Chandran. 2019. <span>“<span>SecureNN</span>: 3-Party Secure Computation for Neural Network Training.”</span> <em>PoPETs</em>, no. 3: 26–49.
</div>
<div id="ref-popets/WaghTBKMR21" class="csl-entry" role="listitem">
Wagh, Sameer, Shruti Tople, Fabrice Benhamouda, Eyal Kushilevitz, Prateek Mittal, and Tal Rabin. 2021. <span>“FALCON: Honest-Majority Maliciously Secure Framework for Private Deep Learning.”</span> <em>PoPETs</em>, no. 1: 188–208.
</div>
<div id="ref-uss/WatsonWP22" class="csl-entry" role="listitem">
Watson, Jean-Luc, Sameer Wagh, and Raluca Popa. 2022. <span>“Piranha: A <span>GPU</span> Platform for Secure Computation.”</span> In <em>USENIX Security Symposium</em>.
</div>
<div id="ref-ijcai/WongMWNC20" class="csl-entry" role="listitem">
Wong, Harry W. H., Jack P. K. Ma, Donald P. H. Wong, Lucien K. L. Ng, and Sherman S. M. Chow. 2020. <span>“Learning Model with Error - Exposing the Hidden Model of <span>BAYHENN</span>.”</span> In <em>IJCAI</em>, 3529–35.
</div>
<div id="ref-tr/he-stat" class="csl-entry" role="listitem">
Wu, David J., and Jacob Haven. 2012. <span>“Using Homomorphic Encryption for Large Scale Statistical Analysis.”</span> Stanford.
</div>
<div id="ref-iclr/WuLCS18" class="csl-entry" role="listitem">
Wu, Shuang, Guoqi Li, Feng Chen, and Luping Shi. 2018. <span>“Training and Inference with Integers in Deep Neural Networks.”</span> In <em><span>ICLR</span></em>.
</div>
<div id="ref-icml/YangZKBWS19" class="csl-entry" role="listitem">
Yang, Guandao, Tianyi Zhang, Polina Kirichenko, Junwen Bai, Andrew Gordon Wilson, and Christopher De Sa. 2019. <span>“<span>SWALP</span> : Stochastic Weight Averaging in Low Precision Training.”</span> In <em>ICML</em>, 7015–24.
</div>
<div id="ref-tist/YangLCT19" class="csl-entry" role="listitem">
Yang, Qiang, Yang Liu, Tianjian Chen, and Yongxin Tong. 2019. <span>“Federated Machine Learning: Concept and Applications.”</span> <em>Trans. Intell. Sys. Tech.</em> 10 (2): 12:1–19.
</div>
<div id="ref-focs/Yao86" class="csl-entry" role="listitem">
Yao, Andrew Chi-Chih. 1986. <span>“How to Generate and Exchange Secrets.”</span> In <em>FOCS</em>, 162–67.
</div>
<div id="ref-cacm/ZhangBHRV21" class="csl-entry" role="listitem">
Zhang, Chiyuan, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. 2021. <span>“Understanding Deep Learning (Still) Requires Rethinking Generalization.”</span> <em>Commun. <span>ACM</span></em> 64 (3): 107–15.
</div>
<div id="ref-ndss/ZhangXW21" class="csl-entry" role="listitem">
Zhang, Qiao, Chunsheng Xin, and Hongyi Wu. 2021. <span>“<span>GALA</span>: Greedy ComputAtion for Linear Algebra in Privacy-Preserved Neural Networks.”</span> In <em>NDSS</em>.
</div>
<div id="ref-sp/ZhengPGS19" class="csl-entry" role="listitem">
Zheng, Wenting, Raluca Ada Popa, Joseph E. Gonzalez, and Ion Stoica. 2019. <span>“Helen: Maliciously Secure Coopetitive Learning for Linear Models.”</span> In <em><span>S&amp;P</span></em>, 724–38.
</div>
</div>
<!--radix_placeholder_article_footer-->
<!--/radix_placeholder_article_footer-->
</div>

<div class="d-appendix">
</div>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

<!--radix_placeholder_site_after_body-->
<!--/radix_placeholder_site_after_body-->
<!--radix_placeholder_appendices-->
<div class="appendix-bottom">
<h3 id="references">References</h3>
<div id="references-listing"></div>
<h3 id="citation">Citation</h3>

<p>BibTeX citation</p>
<pre class="citation-appendix long">@inproceedings{sp/NgC23,
  author = {Ng, Lucien K. L. and Chow, Sherman S. M},
  title = {{SoK}: {Cryptographic} Neural-Network Computation},
  booktitle = {44th {IEEE} Symposium on Security and Privacy, {SP} 2023, San Francisco,
               CA, USA, 22-25 May 2023},
  year = {2023},
  month = {May},
  pages = {497--514},
  notes = {full version available at \url{https://sokcryptonn.github.io/}},
  doi = {10.1109/SP46215.2023.00198},
  publisher = {IEEE Computer Society}
}</pre>

</div>
<!--/radix_placeholder_appendices-->
<!--radix_placeholder_navigation_after_body-->
<!--/radix_placeholder_navigation_after_body-->

</body>

</html>
